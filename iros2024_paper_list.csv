Binary Amplitude-Only Hologram Design for Acoustic End-Effector Construction by Physics-based deep learning,"Liu, Qing; su, hu; LIU, Song",,
Learning Agile Locomotion on Risky Terrains,"Zhang, Chong; Rudin, Nikita; Hoeller, David; Hutter, Marco",https://arxiv.org/abs/2311.10484,"Quadruped robots have shown remarkable mobility on various terrains through reinforcement learning. Yet, in the presence of sparse footholds and risky terrains such as stepping stones and balance beams, which require precise foot placement to avoid falls, model-based approaches are often used. In this paper, we show that end-to-end reinforcement learning can also enable the robot to traverse risky terrains with dynamic motions. To this end, our approach involves training a generalist policy for agile locomotion on disorderly and sparse stepping stones before transferring its reusable knowledge to various more challenging terrains by finetuning specialist policies from it. Given that the robot needs to rapidly adapt its velocity on these terrains, we formulate the task as a navigation task instead of the commonly used velocity tracking which constrains the robot's behavior and propose an exploration strategy to overcome sparse rewards and achieve high robustness. We validate our proposed method through simulation and real-world experiments on an ANYmal-D robot achieving peak forward velocity of >= 2.5 m/s on sparse stepping stones and narrow balance beams. Video: youtu.be/Z5X0J8OH6z4"
Kinetic-Energy-Optimal and Safety-Guaranteed Trajectory Planning for Bridge Inspection Robot Manipulator,"Zhang, Tianyu; Chang, Yong; Wang, Hongguang; Wang, Tianlong",,
HGP-RL: Distributed Hierarchical Gaussian Processes for Wi-Fi-based Relative Localization in Multi-Robot Systems,"Latif, Ehsan; Parasuraman, Ramviyas",https://arxiv.org/abs/2307.10614,"Relative localization is crucial for multi-robot systems to perform cooperative tasks, especially in GPS-denied environments. Current techniques for multi-robot relative localization rely on expensive or short-range sensors such as cameras and LIDARs. As a result, these algorithms face challenges such as high computational complexity (e.g., map merging), dependencies on well-structured environments, etc. To remedy this gap, we propose a new distributed approach to perform relative localization (RL) using a common Access Point (AP). To achieve this efficiently, we propose a novel Hierarchical Gaussian Processes (HGP) mapping of the Radio Signal Strength Indicator (RSSI) values from a Wi-Fi AP to which the robots are connected. Each robot performs hierarchical inference using the HGP map to locate the AP in its reference frame, and the robots obtain relative locations of the neighboring robots leveraging AP-oriented algebraic transformations. The approach readily applies to resource-constrained devices and relies only on the ubiquitously-available WiFi RSSI measurement. We extensively validate the performance of the proposed HGR-PL in Robotarium simulations against several state-of-the-art methods. The results indicate superior performance of HGP-RL regarding localization accuracy, computation, and communication overheads. Finally, we showcase the utility of HGP-RL through a multi-robot cooperative experiment to achieve a rendezvous task in a team of three mobile robots."
Object-Oriented Material Classification and 3D Clustering for Improved Semantic Perception and Mapping in Mobile Robots,"Ravipati, Siva Krishna; Latif, Ehsan; Bhandarkar, Suchendra; Parasuraman, Ramviyas",https://arxiv.org/abs/2407.06077,"Classification of different object surface material types can play a significant role in the decision-making algorithms for mobile robots and autonomous vehicles. RGB-based scene-level semantic segmentation has been well-addressed in the literature. However, improving material recognition using the depth modality and its integration with SLAM algorithms for 3D semantic mapping could unlock new potential benefits in the robotics perception pipeline. To this end, we propose a complementarity-aware deep learning approach for RGB-D-based material classification built on top of an object-oriented pipeline. The approach further integrates the ORB-SLAM2 method for 3D scene mapping with multiscale clustering of the detected material semantics in the point cloud map generated by the visual SLAM algorithm. Extensive experimental results with existing public datasets and newly contributed real-world robot datasets demonstrate a significant improvement in material classification and 3D clustering accuracy compared to state-of-the-art approaches for 3D semantic scene mapping."
PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation,"Meyer, Lukas; Erich, Floris Marc Arden; Yoshiyasu, Yusuke; Stamminger, Marc; Ando, Noriaki; Domae, Yukiyasu",https://arxiv.org/abs/2401.02281,"We introduce Physically Enhanced Gaussian Splatting Simulation System (PEGASUS) for 6DOF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting.  Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting. <i>PEGASUS</i> allows the composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects. Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment. Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects. By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted. Our study demonstrates that training on data generated by PEGASUS enables pose estimation networks to successfully transfer from synthetic data to real-world data. Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This dataset includes spherical scans that captures images from both object hemisphere and the Gaussian Splatting reconstruction, making them compatible with PEGASUS."
Multi-target Tracking with Occlusion Resistance for Mobile Robots in Dynamic Environments,"Liu, Zhongyan; Lu, Biao; Xing, Xinghai; Mao, Dun; Fang, Yongchun",,
Flight Structure Optimization of Modular Reconfigurable UAVs,"Su, Yao; Jiao, Ziyuan; Zhang, Zeyu; Zhang, Jingwen; Li, Hang; Wang, Meng; Liu, Hangxin",https://arxiv.org/abs/2407.03724,"This paper presents a Genetic Algorithm (GA) designed to reconfigure a large group of modular Unmanned Aerial Vehicles (UAVs), each with different weights and inertia parameters, into an over-actuated flight structure with improved dynamic properties. Previous research efforts either utilized expert knowledge to design flight structures for a specific task or relied on enumeration-based algorithms that required extensive computation to find an optimal one. However, both approaches encounter challenges in accommodating the heterogeneity among modules. Our GA addresses these challenges by incorporating the complexities of over-actuation and dynamic properties into its formulation. Additionally, we employ a tree representation and a vector representation to describe flight structures, facilitating efficient crossover operations and fitness evaluations within the GA framework, respectively. Using cubic modular quadcopters capable of functioning as omni-directional thrust generators, we validate that the proposed approach can (i) adeptly identify suboptimal configurations ensuring over-actuation while ensuring trajectory tracking accuracy and (ii) significantly reduce computational costs compared to traditional enumeration-based methods."
CBGL: Fast Monte Carlo Passive Global Localisation of 2D LIDAR Sensor,"Filotheou, Alexandros",https://arxiv.org/abs/2307.14247,"Navigation of a mobile robot is conditioned on the knowledge of its pose. In observer-based localisation configurations its initial pose may not be knowable in advance, leading to the need of its estimation. Solutions to the problem of global localisation are either robust against noise and environment arbitrariness but require motion and time, which may (need to) be economised on, or require minimal estimation time but assume environmental structure, may be sensitive to noise, and demand preprocessing and tuning. This article proposes a method that retains the strengths and avoids the weaknesses of the two approaches. The method leverages properties of the Cumulative Absolute Error per Ray (CAER) metric with respect to the errors of pose hypotheses of a 2D LIDAR sensor, and utilises scan--to--map-scan matching for fine(r) pose estimations. A large number of tests, in real and simulated conditions, involving disparate environments and sensor properties, illustrate that the proposed method outperforms state-of-the-art methods of both classes of solutions in terms of pose discovery rate and execution time. The source code is available for download."
Understanding Strain Wave Gear Directional Efficiency in the Context of Robotic Actuation and Overcoming the Corresponding Performance Limitations Through Direct Torque Control,"Georgiev, Nikola",,
Decentralized Trajectory Planning for Formation Flight in Unknown and Dense Environments,"Zeng, Jianxin; Wang, Yaonan; Miao, Zhiqiang; He, Wei; Wang, Hesheng",,
Large-scale Deployment of Vision-based Tactile Sensors on Multi-fingered Grippers,"Wang, Meng; Li, Wanlin; Liang, Hao; Li, Boren; Althoefer, Kaspar; Su, Yao; Liu, Hangxin",https://arxiv.org/abs/2408.02206,"Vision-based Tactile Sensors (VBTSs) show significant promise in that they can leverage image measurements to provide high-spatial-resolution human-like performance. However, current VBTS designs, typically confined to the fingertips of robotic grippers, prove somewhat inadequate, as many grasping and manipulation tasks require multiple contact points with the object. With an end goal of enabling large-scale, multi-surface tactile sensing via VBTSs, our research (i) develops a synchronized image acquisition system with minimal latency,(ii) proposes a modularized VBTS design for easy integration into finger phalanges, and (iii) devises a zero-shot calibration approach to improve data efficiency in the simultaneous calibration of multiple VBTSs. In validating the system within a miniature 3-fingered robotic gripper equipped with 7 VBTSs we demonstrate improved tactile perception performance by covering the contact surfaces of both gripper fingers and palm. Additionally, we show that our VBTS design can be seamlessly integrated into various end-effector morphologies significantly reducing the data requirements for calibration."
Beyond Success: Quantifying Quality of Task Execution for Improved Learning from Demonstration,"Bilal, Muhammad; Lipovetzky, Nir; Oetomo, Denny; Johal, Wafa",,
Local Linearity is All You Need (in Data Driven Teleoperation),"Przystupa, Michael; Gidel, Gauthier; Taylor, Matthew; Jagersand, Martin; Piater, Justus; Tosatto, Samuele",,
FBG-based Shape-Sensing to Enable Lateral Deflection Methods of Autonomous Needle Insertion,"Lezcano, Dimitri A.; Iordachita, Ioan Iulian; Kim, Jin Seob",,
Archie Jnr: A Robotic Platform for Autonomous Cane Pruning of Grapevines,"Williams, Henry; Smith, David Anthony James; Shahabi, Jalil; Gee, Trevor; Qureshi, Ans; McGuinness, Benjamin John; Harvey, Scott; Downes, Catherine; Jangali, Rahul; Black, Kale; Lim, Shen Hin; Duke, Mike; MacDonald, Bruce",,
Swiss DINO: Efficient and Versatile Vision Framework for On-Device Personal Object Search,"Paramonov, Kirill; Zhong, Jia-Xing; Michieli, Umberto; Moon, Jijoong; Ozay, Mete",https://arxiv.org/abs/2407.07541,"In this paper, we address a recent trend in robotic home appliances to include vision systems on personal devices, capable of personalizing the appliances on the fly. In particular, we formulate and address an important technical task of personal object search, which involves localization and identification of personal items of interest on images captured by robotic appliances, with each item referenced only by a few annotated images. The task is crucial for robotic home appliances and mobile systems, which need to process personal visual scenes or to operate with particular personal objects (e.g., for grasping or navigation). In practice, personal object search presents two main technical challenges. First, a robot vision system needs to be able to distinguish between many fine-grained classes, in the presence of occlusions and clutter. Second, the strict resource requirements for the on-device system restrict the usage of most state-of-the-art methods for few-shot learning and often prevent on-device adaptation. In this work, we propose Swiss DINO: a simple yet effective framework for one-shot personal object search based on the recent DINOv2 transformer model, which was shown to have strong zero-shot generalization properties. Swiss DINO handles challenging on-device personalized scene understanding requirements and does not require any adaptation training. We show significant improvement (up to 55%) in segmentation and recognition accuracy compared to the common lightweight solutions, and significant footprint reduction of backbone inference time (up to 100x) and GPU consumption (up to 10x) compared to the heavy transformer-based solutions."
Cross-Architecture Auxiliary Feature Space Translation for Efficient Few-Shot Personalized Object Detection,"Barbato, Francesco; Michieli, Umberto; Moon, Jijoong; Zanuttigh, Pietro; Ozay, Mete",https://arxiv.org/abs/2407.01193,"Recent years have seen object detection robotic systems deployed in several personal devices (e.g., home robots and appliances). This has highlighted a challenge in their design, i.e., they cannot efficiently update their knowledge to distinguish between general classes and user-specific instances (e.g., a dog vs. user's dog). We refer to this challenging task as Instance-level Personalized Object Detection (IPOD). The personalization task requires many samples for model tuning and optimization in a centralized server, raising privacy concerns. An alternative is provided by approaches based on recent large-scale Foundation Models, but their compute costs preclude on-device applications. In our work we tackle both problems at the same time, designing a Few-Shot IPOD strategy called AuXFT. We introduce a conditional coarse-to-fine few-shot learner to refine the coarse predictions made by an efficient object detector, showing that using an off-the-shelf model leads to poor personalization due to neural collapse. Therefore, we introduce a Translator block that generates an auxiliary feature space where features generated by a self-supervised model (e.g., DINOv2) are distilled without impacting the performance of the detector. We validate AuXFT on three publicly available datasets and one in-house benchmark designed for the IPOD task, achieving remarkable gains in all considered scenarios with excellent time-complexity trade-off: AuXFT reaches a performance of 80% its upper bound at just 32% of the inference time, 13% of VRAM and 19% of the model size."
Enhanced Model Robustness to Input Corruptions by Per-Corruption Adaptation of Normalization Statistics,"Camuffo, Elena; Michieli, Umberto; Milani, Simone; Moon, Jijoong; Ozay, Mete",https://arxiv.org/abs/2407.06450,"Developing a reliable vision system is a fundamental challenge for robotic technologies (e.g., indoor service robots and outdoor autonomous robots) which can ensure reliable navigation even in challenging environments such as adverse weather conditions (e.g., fog, rain), poor lighting conditions (e.g., over/under exposure), or sensor degradation (e.g., blurring, noise), and can guarantee high performance in safety-critical functions. Current solutions proposed to improve model robustness usually rely on generic data augmentation techniques or employ costly test-time adaptation methods. In addition, most approaches focus on addressing a single vision task (typically, image recognition) utilising synthetic data. In this paper, we introduce Per-corruption Adaptation of Normalization statistics (PAN) to enhance the model robustness of vision systems. Our approach entails three key components: (i) a corruption type identification module, (ii) dynamic adjustment of normalization layer statistics based on identified corruption type, and (iii) real-time update of these statistics according to input data. PAN can integrate seamlessly with any convolutional model for enhanced accuracy in several robot vision tasks. In our experiments, PAN obtains robust performance improvement on challenging real-world corrupted image datasets (e.g., OpenLoris, ExDark, ACDC), where most of the current solutions tend to fail. Moreover, PAN outperforms the baseline models by 20-30% on synthetic benchmarks in object recognition tasks."
Language-Guided Pattern Formation for Swarm Robotics with Multi-Agent Reinforcement Learning,"Liu, Hsu-Shen; Kuroki, So; Kozuno, Tadashi; Sun, Wei-Fang; Lee, Chun-Yi",,
RaNDT SLAM: Radar SLAM Based on Intensity-Augmented Normal Distributions Transform,"Hilger, Maximilian; Mandischer, Nils; Corves, Burkhard",https://arxiv.org/abs/2408.11576,"Rescue robotics sets high requirements to perception algorithms due to the unstructured and potentially vision-denied environments. Pivoting Frequency-Modulated Continuous Wave radars are an emerging sensing modality for SLAM in this kind of environment. However, the complex noise characteristics of radar SLAM makes, particularly indoor, applications computationally demanding and slow. In this work, we introduce a novel radar SLAM framework, RaNDT SLAM, that operates fast and generates accurate robot trajectories. The method is based on the Normal Distributions Transform augmented by radar intensity measures. Motion estimation is based on fusion of motion model, IMU data, and registration of the intensity-augmented Normal Distributions Transform. We evaluate RaNDT SLAM in a new benchmark dataset and the Oxford Radar RobotCar dataset. The new dataset contains indoor and outdoor environments besides multiple sensing modalities (LiDAR, radar, and IMU)."
Online Multi-Agent Pickup and Delivery with Task Deadlines,"Makino, Hiroya; Ito, Seigo",https://arxiv.org/abs/2403.12377,"Managing delivery deadlines in automated warehouses and factories is crucial for maintaining customer satisfaction and ensuring seamless production. This study introduces the problem of online multi-agent pickup and delivery with task deadlines (MAPD-D), an advanced variant of the online MAPD problem incorporating delivery deadlines. In the MAPD problem, agents must manage a continuous stream of delivery tasks online. Tasks are added at any time. Agents must complete their tasks while avoiding collisions with each other. MAPD-D introduces a dynamic, deadline-driven approach that incorporates task deadlines, challenging the conventional MAPD frameworks. To tackle MAPD-D, we propose a novel algorithm named deadline-aware token passing (D-TP). The D-TP algorithm calculates pickup deadlines and assigns tasks while balancing execution cost and deadline proximity. Additionally, we introduce the D-TP with task swaps (D-TPTS) method to further reduce task tardiness, enhancing flexibility and efficiency through task-swapping strategies. Numerical experiments were conducted in simulated warehouse environments to showcase the effectiveness of the proposed methods. Both D-TP and D-TPTS demonstrated significant reductions in task tardiness compared to existing methods. Our methods contribute to efficient operations in automated warehouses and factories with delivery deadlines."
MARPF: Multi-Agent and Multi-Rack Path Finding,"Makino, Hiroya; Ohama, Yoshihiro; Ito, Seigo",https://arxiv.org/abs/2403.12376,"In environments where many automated guided vehicles (AGVs) operate, planning efficient, collision-free paths is essential. Related research has mainly focused on environments with pre-defined passages, resulting in space inefficiency. We attempt to relax this assumption. In this study, we define multi-agent and multi-rack path finding (MARPF) as the problem of planning paths for AGVs to convey target racks to their designated locations in environments without passages. In such environments, an AGV without a rack can pass under racks, whereas one with a rack cannot pass under racks to avoid collisions. MARPF entails conveying the target racks without collisions, while the obstacle racks are relocated to prevent any interference with the target racks. We formulated MARPF as an integer linear programming problem in a network flow. To distinguish situations in which an AGV is or is not loading a rack, the proposed method introduces two virtual layers into the network. We optimized the AGVs' movements to move obstacle racks and convey the target racks. The formulation and applicability of the algorithm were validated through numerical experiments. The results indicated that the proposed algorithm addressed issues in environments with dense racks."
Optimal Sensing in Soft Pneumatic Actuators via Stretchable Optical Waveguides,"ALJaber, Faisal; Hassan, Ahmed; Vitanov, Ivan; Almeadadi, Noora; ALHAJRI, HIND; AlEnazi, Sara; Al-Marri, Rashid; Choe, Pilsung",,
A Fast Heuristic Scheduling Search for Robotic Cellular Manufacturing Systems with Generalized and Timed Petri Nets,"Xiao, YuanZheng; Gao, YangQing; Wu, Haoran; Huang, Bo; Lv, Jianyong",,
Continuous Rapid Learning by Human Imitation using Audio Prompts and One-Shot Learning,"Duque Domingo, Jaime; García-Gómez, Miguel; Zalama, Eduardo; Gomez Garcia Bermejo, Jaime",,
ASY-VRNet: Waterway Panoptic Driving Perception Model based on Asymmetric Fair Fusion of Vision and 4D mmWave Radar,"Guan, Runwei; Yao, Shanliang; Man, Ka Lok; Zhu, Xiaohui; Yue, Yong; Smith, Jeremy; Yue, Yutao; Lim, Eng Gee",https://arxiv.org/abs/2308.10287,"Panoptic Driving Perception (PDP) is critical for the autonomous navigation of Unmanned Surface Vehicles (USVs). A PDP model typically integrates multiple tasks, necessitating the simultaneous and robust execution of various perception tasks to facilitate downstream path planning. The fusion of visual and radar sensors is currently acknowledged as a robust and cost-effective approach. However, most existing research has primarily focused on fusing visual and radar features dedicated to object detection or utilizing a shared feature space for multiple tasks, neglecting the individual representation differences between various tasks. To address this gap, we propose a pair of Asymmetric Fair Fusion (AFF) modules with favorable explainability designed to efficiently interact with independent features from both visual and radar modalities, tailored to the specific requirements of object detection and semantic segmentation tasks. The AFF modules treat image and radar maps as irregular point sets and transform these features into a crossed-shared feature space for multitasking, ensuring equitable treatment of vision and radar point cloud features. Leveraging AFF modules, we propose a novel and efficient PDP model, ASY-VRNet, which processes image and radar features based on irregular super-pixel point sets. Additionally, we propose an effective multitask learning method specifically designed for PDP models. Compared to other lightweight models, ASY-VRNet achieves state-of-the-art performance in object detection, semantic segmentation, and drivable-area segmentation on the WaterScenes benchmark. Our project is publicly available at https://github.com/GuanRunwei/ASY-VRNet."
Robust-Adaptive Two-Loop Control for Robots with Mixed Rigid-Elastic Joints,"Hua, Minh Tuan; Sveen, Emil Mühlbradt; Schlanbusch, Siri Marte; Sanfilippo, Filippo",,
Archie Snr: A Robotic Platform for Autonomous Apple Fruitlet Thinning,"Williams, Henry; Qureshi, Ans; Smith, David Anthony James; Gee, Trevor; McGuinness, Benjamin John; Jangali, Rahul; Black, Kale; Harvey, Scott; Downes, Catherine; Lim, Shen Hin; Oliver, Richard; Duke, Mike; MacDonald, Bruce",,
Development of a Bendable and Extendable Soft Gripper Driven by Differential Worm Gear Mechanism,"Selvamuthu, Moses Gladson; Tadakuma, Riichiro",,
3D Object Visibility Prediction in Autonomous Driving,"Luo, Chuanyu; Cheng, Nuo; Zhong, Ren; Jiang, Haipeng; Chen, Wenyu; Wang, Aoli; Li, Pu",https://arxiv.org/abs/2403.03681,"With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth. The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control. At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation. In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility. By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model's effectiveness and efficiency. Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios."
PGA: Personalizing Grasping Agents with Single Human-Robot Interaction,"Kim, Junghyun; Kang, Gi-Cheon; Kim, Jaein; Yang, Seoyun; Jung, Minjoon; Zhang, Byoung-Tak",https://arxiv.org/abs/2310.12547,"Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that comprehend and grasp objects based on natural language instructions. While the ability to understand personal objects like my wallet facilitates more natural interaction with human users, current LCRG systems only allow generic language instructions, e.g., the black-colored wallet next to the laptop. To this end, we introduce a task scenario GraspMine alongside a novel dataset aimed at pinpointing and grasping personal objects given personal indicators via learning from a single human-robot interaction, rather than a large labeled dataset. Our proposed method, Personalized Grasping Agent (PGA), addresses GraspMine by leveraging the unlabeled image data of the user's environment, called Reminiscence. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. This results in significant efficiency while previous LCRG systems rely on resource-intensive human annotations -- necessitating hundreds of labeled data to learn my wallet. Moreover, PGA outperforms baseline methods across all metrics and even shows comparable performance compared to the fully-supervised method, which learns from 9k annotated data samples. We further validate PGA's real-world applicability by employing a physical robot to execute GrsapMine. Code and data are publicly available at https://github.com/JHKim-snu/PGA."
Volumetric Semantically Consistent 3D Panoptic Mapping,"Miao, Yang; Armeni, Iro; Pollefeys, Marc; Barath, Daniel",https://arxiv.org/abs/2309.14737,"We introduce an online 2D-to-3D semantic instance mapping algorithm aimed at generating comprehensive, accurate, and efficient semantic 3D maps suitable for autonomous agents in unstructured environments. The proposed approach is based on a Voxel-TSDF representation used in recent algorithms. It introduces novel ways of integrating semantic prediction confidence during mapping, producing semantic and instance-consistent 3D regions. Further improvements are achieved by graph optimization-based semantic labeling and instance refinement. The proposed method achieves accuracy superior to the state of the art on public large-scale datasets, improving on a number of widely used metrics. We also highlight a downfall in the evaluation of recent studies: using the ground truth trajectory as input instead of a SLAM-estimated one substantially affects the accuracy, creating a large gap between the reported results and the actual performance on real-world data."
Precise Pick-and-Place using Score-Based Diffusion Networks,"Guo, Shih-Wei; Hsiao, Tsu-Ching; Liu, Yu-Lun; Lee, Chun-Yi",,
Visual Quality Inspection Planning: A Model-Based Framework for Generating Optimal and Feasible Inspection Poses,"Staderini, Vanessa; Glück, Tobias; Schneider, Philipp; Kugi, Andreas",,
Depth Helps: Improving Pre-trained RGB-based Policy with Depth Information Injection,"Pang, Xincheng; Xia, Wenke; Wang, Zhigang; Zhao, Bin; Hu, Di; Wang, Dong; Li, Xuelong",https://arxiv.org/abs/2408.05107,"3D perception ability is crucial for generalizable robotic manipulation. While recent foundation models have made significant strides in perception and decision-making with RGB-based input, their lack of 3D perception limits their effectiveness in fine-grained robotic manipulation tasks. To address these limitations, we propose a Depth Information Injection ($\bold{DI}^{\bold{2}}$) framework that leverages the RGB-Depth modality for policy fine-tuning, while relying solely on RGB images for robust and efficient deployment. Concretely, we introduce the Depth Completion Module (DCM) to extract the spatial prior knowledge related to depth information and generate virtual depth information from RGB inputs to aid policy deployment. Further, we propose the Depth-Aware Codebook (DAC) to eliminate noise and reduce the cumulative error from the depth prediction. In the inference phase, this framework employs RGB inputs and accurately predicted depth data to generate the manipulation action. We conduct experiments on simulated LIBERO environments and real-world scenarios, and the experiment results prove that our method could effectively enhance the pre-trained RGB-based policy with 3D perception ability for robotic manipulation. The website is released at https://gewu-lab.github.io/DepthHelps-IROS2024."
An Online RCM Adjusting System for Robot-Assisted Retinal Surgeries,"Xia, Jun; Wang, Ting; Ni, Huanqi; Li, Yanlin; Chen, Ruoxi; Nasseri, M. Ali; Lin, Haotian; Huang, Kai",,
A Deep Signed Directional Distance Function for Shape Representation,"Zobeidi, Ehsan; Atanasov, Nikolay",https://arxiv.org/abs/2107.11024,"Neural networks that map 3D coordinates to signed distance function (SDF) or occupancy values have enabled high-fidelity implicit representations of object shape. This paper develops a new shape model that allows synthesizing novel distance views by optimizing a continuous signed directional distance function (SDDF). Similar to deep SDF models, our SDDF formulation can represent whole categories of shapes and complete or interpolate across shapes from partial input data. Unlike an SDF, which measures distance to the nearest surface in any direction, an SDDF measures distance in a given direction. This allows training an SDDF model without 3D shape supervision, using only distance measurements, readily available from depth camera or Lidar sensors. Our model also removes post-processing steps like surface extraction or rendering by directly predicting distance at arbitrary locations and viewing directions. Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which train high-capacity black-box models, our model encodes by construction the property that SDDF values decrease linearly along the viewing direction. This structure constraint not only results in dimensionality reduction but also provides analytical confidence about the accuracy of SDDF predictions, regardless of the distance to the object surface."
Progressive Query Refinement Framework for Bird's-Eye-View Semantic Segmentation from Surrounding Images,"Choi, Dooseop; Kang, Jungyu; An, Taeg-Hyun; An, Kyounghwan; KyoungWook, MIN",https://arxiv.org/abs/2407.17003,"Expressing images with Multi-Resolution (MR) features has been widely adopted in many computer vision tasks. In this paper, we introduce the MR concept into Bird's-Eye-View (BEV) semantic segmentation for autonomous driving. This introduction enhances our model's ability to capture both global and local characteristics of driving scenes through our proposed residual learning. Specifically, given a set of MR BEV query maps, the lowest resolution query map is initially updated using a View Transformation (VT) encoder. This updated query map is then upscaled and merged with a higher resolution query map to undergo further updates in a subsequent VT encoder. This process is repeated until the resolution of the updated query map reaches the target. Finally, the lowest resolution map is added to the target resolution to generate the final query map. During training, we enforce both the lowest and final query maps to align with the ground-truth BEV semantic map to help our model effectively capture the global and local characteristics. We also propose a visual feature interaction network that promotes interactions between features across images and across feature levels, thus highly contributing to the performance improvement. We evaluate our model on a large-scale real-world dataset. The experimental results show that our model outperforms the SOTA models in terms of IoU metric. Codes are available at https://github.com/d1024choi/ProgressiveQueryRefineNet"
Robust Imitation Learning for Mobile Manipulator Focusing on Task-Related Viewpoints and Regions,"Ishida, Yutaro; Noguchi, Yuki; Kanai, Takayuki; Shintani, Kazuhiro; Bitoh, Hiroshi",,
OPG-Policy: Occluded Push-Grasp Policy Learning with Amodal Segmentation,"Ding, Hao; Zeng, Yiming; Wan, Zhaoliang; Cheng, Hui",,
"Demonstrating a Robust Walking Algorithm for Underactuated Bipedal Robots in Non-flat, Non-stationary Environments","Dosunmu-Ogunbi, Oluwami; Shrivastava, Aayushi; Grizzle, J.W",https://arxiv.org/abs/2403.02486,"This work explores an innovative algorithm designed to enhance the mobility of underactuated bipedal robots across challenging terrains, especially when navigating through spaces with constrained opportunities for foot support, like steps or stairs. By combining ankle torque with a refined angular momentum-based linear inverted pendulum model (ALIP), our method allows variability in the robot's center of mass height. We employ a dual-strategy controller that merges virtual constraints for precise motion regulation across essential degrees of freedom with an ALIP-centric model predictive control (MPC) framework, aimed at enforcing gait stability. The effectiveness of our feedback design is demonstrated through its application on the Cassie bipedal robot, which features 20 degrees of freedom. Key to our implementation is the development of tailored nominal trajectories and an optimized MPC that reduces the execution time to under 500 microseconds--and, hence, is compatible with Cassie's controller update frequency. This paper not only showcases the successful hardware deployment but also demonstrates a new capability, a bipedal robot using a moving walkway."
IC-FPS: Instance-Centroid Faster Point Sampling Framework for 3D Point-based Object Detection,"Hu, Haotian; Wang, Fanyi; Zhang, Zhiwang",,
DNS-SLAM: Dense Neural Semantic-Informed SLAM,"Li, Kunyi; Niemeyer, Michael; Navab, Nassir; Tombari, Federico",https://arxiv.org/abs/2312.00204,"In recent years, coordinate-based neural implicit representations have shown promising results for the task of Simultaneous Localization and Mapping (SLAM). While achieving impressive performance on small synthetic scenes, these methods often suffer from oversmoothed reconstructions, especially for complex real-world scenes. In this work, we introduce DNS SLAM, a novel neural RGB-D semantic SLAM approach featuring a hybrid representation. Relying only on 2D semantic priors, we propose the first semantic neural SLAM method that trains class-wise scene representations while providing stable camera tracking at the same time. Our method integrates multi-view geometry constraints with image-based feature extraction to improve appearance details and to output color, density, and semantic class information, enabling many downstream applications. To further enable real-time tracking, we introduce a lightweight coarse scene representation which is trained in a self-supervised manner in latent space. Our experimental results achieve state-of-the-art performance on both synthetic data and real-world data tracking while maintaining a commendable operational speed on off-the-shelf hardware. Further, our method outputs class-wise decomposed reconstructions with better texture capturing appearance and geometric details."
PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking,"Yang, Yifan; Cui, Zhihao; Zhang, Qianyi; Liu, Jingtai",https://arxiv.org/abs/2405.11257,"6D object pose estimation holds essential roles in various fields, particularly in the grasping of industrial workpieces. Given challenges like rust, high reflectivity, and absent textures, this paper introduces a point cloud based pose estimation framework (PS6D). PS6D centers on slender and multi-symmetric objects. It extracts multi-scale features through an attention-guided feature extraction module, designs a symmetry-aware rotation loss and a center distance sensitive translation loss to regress the pose of each point to the centroid of the instance, and then uses a two-stage clustering method to complete instance segmentation and pose estimation. Objects from the Siléane and IPA datasets and typical workpieces from industrial practice are used to generate data and evaluate the algorithm. In comparison to the state-of-the-art approach, PS6D demonstrates an 11.5\% improvement in F$_{1_{inst}}$ and a 14.8\% improvement in Recall. The main part of PS6D has been deployed to the software of Mech-Mind, and achieves a 91.7\% success rate in bin-picking experiments, marking its application in industrial pose estimation tasks."
Integrated Electronic Circuitry for Soft Robots using Multi-Material FDM Printing,"Aygul, Cem; Pandey, Ritwik; Kothimbakam, Krishram; Yilmaz Akkaya, Ceren; Rao, Pratap; Nemitz, Markus",,
SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes,"Liu, Min; Yang, Gang; Luo, Siyuan; Shao, Lin",https://arxiv.org/abs/2312.03297,"Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework that couples soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a novel forecast-based contact model for MPM, which effectively reduces penetration without introducing other artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Diverging from previous works, SoftMAC simulates the complete dynamics of each modality and incorporates them into a cohesive system with an explicit and differentiable coupling mechanism. The feature empowers SoftMAC to handle a broader spectrum of interactions, such as soft bodies serving as manipulators and engaging with underactuated systems. We conducted comprehensive experiments to validate the effectiveness and accuracy of the proposed differentiable pipeline in downstream robotic manipulation applications. Supplementary materials and videos are available on our project website at https://damianliumin.github.io/SoftMAC."
Compliance Optimization Control for Rigid-Soft Hybrid System and its Application in Humanoid Robot Motion Control,"He, Zewen; Ishigaki, Taiki; Yamamoto, Ko",,
CoDe: A Cooperative and Decentralized Collision Avoidance Algorithm for Small-Scale UAV Swarms Considering Energy Efficiency,"Huang, Shuangyao; Zhang, Haibo; Huang, Zhiyi",,
Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks,"Valencia Redrovan, David Patricio; Williams, Henry; Xing, Yuning; Gee, Trevor; Liarokapis, Minas; MacDonald, Bruce",https://arxiv.org/abs/2407.21338,"Reinforcement Learning (RL) has been widely used to solve tasks where the environment consistently provides a dense reward value. However, in real-world scenarios, rewards can often be poorly defined or sparse. Auxiliary signals are indispensable for discovering efficient exploration strategies and aiding the learning process. In this work, inspired by intrinsic motivation theory, we postulate that the intrinsic stimuli of novelty and surprise can assist in improving exploration in complex, sparsely rewarded environments. We introduce a novel sample-efficient method able to learn directly from pixels, an image-based extension of TD3 with an autoencoder called \textit{NaSA-TD3}. The experiments demonstrate that NaSA-TD3 is easy to train and an efficient method for tackling complex continuous-control robotic tasks, both in simulated environments and real-world settings. NaSA-TD3 outperforms existing state-of-the-art RL image-based methods in terms of final performance without requiring pre-trained models or human demonstrations."
Multi-task real-robot data with gaze attention for dual-arm fine manipulation,"Kim, Heecheol; Ohmura, Yoshiyuki; Kuniyoshi, Yasuo",https://arxiv.org/abs/2401.07603,"In the field of robotic manipulation, deep imitation learning is recognized as a promising approach for acquiring manipulation skills. Additionally, learning from diverse robot datasets is considered a viable method to achieve versatility and adaptability. In such research, by learning various tasks, robots achieved generality across multiple objects. However, such multi-task robot datasets have mainly focused on single-arm tasks that are relatively imprecise, not addressing the fine-grained object manipulation that robots are expected to perform in the real world. This paper introduces a dataset of diverse object manipulations that includes dual-arm tasks and/or tasks requiring fine manipulation. To this end, we have generated dataset with 224k episodes (150 hours, 1,104 language instructions) which includes dual-arm fine tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data is publicly available. Additionally, this dataset includes visual attention signals as well as dual-action labels, a signal that separates actions into a robust reaching trajectory and precise interaction with objects, and language instructions to achieve robust and precise object manipulation. We applied the dataset to our Dual-Action and Attention (DAA), a model designed for fine-grained dual arm manipulation tasks and robust against covariate shifts. The model was tested with over 7k total trials in real robot manipulation tasks, demonstrating its capability in fine manipulation."
Det-Recon-Reg: An Intelligent Framework Towards Automated Large-Scale Infrastructure Inspection,"YANG, Guidong; Zhang, Jihan; ZHAO, Benyun; GAO, CHUANXIANG; HUANG, Yijun; Wen, Junjie; Li, Qingxiang; Chen, Xi; Chen, Ben M.",,
Perception for Connected Autonomous Vehicles under Adverse Weather Conditions,"Tsakmakopoulou, Dimitra; Moustakas, Konstantinos",https://arxiv.org/abs/2110.07206,"Visual perception in autonomous driving is a crucial part of a vehicle to navigate safely and sustainably in different traffic conditions. However, in bad weather such as heavy rain and haze, the performance of visual perception is greatly affected by several degrading effects. Recently, deep learning-based perception methods have addressed multiple degrading effects to reflect real-world bad weather cases but have shown limited success due to 1) high computational costs for deployment on mobile devices and 2) poor relevance between image enhancement and visual perception in terms of the model ability. To solve these issues, we propose a task-driven image enhancement network connected to the high-level vision task, which takes in an image corrupted by bad weather as input. Specifically, we introduce a novel low memory network to reduce most of the layer connections of dense blocks for less memory and computational cost while maintaining high performance. We also introduce a new task-driven training strategy to robustly guide the high-level task model suitable for both high-quality restoration of images and highly accurate perception. Experiment results demonstrate that the proposed method improves the performance among lane and 2D object detection, and depth estimation largely under adverse weather in terms of both low memory and accuracy."
Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning,"Flögel, Daniel; Fischer, Lars; Rudolf, Thomas; Schürmann, Tobias; Hohmann, Sören",https://arxiv.org/abs/2403.09793,"Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, which states that social acting is oriented toward the acting of others. The DRL policy is trained in an environment where other agents interact socially integrated and reward the robot's behavior individually. The simulation results indicate that the proposed socially integrated navigation approach outperforms a socially aware approach in terms of ego navigation performance while significantly reducing the negative impact on all agents within the environment."
mini-PointNetPlus: A Local Feature Descriptor in Deep Learning Model for Real-time 3D Environment Perception,"Luo, Chuanyu; Cheng, Nuo; Ma, Sikun; Xiang, Jun; Li, Xiaohan; Lei, Shengguang; Li, Pu",,
Passive Underwater Robot Hand Utilizing Water Resistance,"NATE, Issei; Hirai, Shinichi",,
Response Improvement of Hydraulic Robotic Joints via New Force Servo and Inverted Pendulum Demo,"Arai, Ryo; Sakai, Satoru; Ono, Kazuki",,
Grasping Trajectory Optimization with Point Clouds,"Xiang, Yu; Allu, Sai Haneesh; Peddi, Rohith; Summers, Tyler; GOGATE, VIBHAV",https://arxiv.org/abs/2403.05466,"We introduce a new trajectory optimization method for robotic grasping based on a point-cloud representation of robots and task spaces. In our method, robots are represented by 3D points on their link surfaces. The task space of a robot is represented by a point cloud that can be obtained from depth sensors. Using the point-cloud representation, goal reaching in grasping can be formulated as point matching, while collision avoidance can be efficiently achieved by querying the signed distance values of the robot points in the signed distance field of the scene points. Consequently, a constrained nonlinear optimization problem is formulated to solve the joint motion and grasp planning problem. The advantage of our method is that the point-cloud representation is general to be used with any robot in any environment. We demonstrate the effectiveness of our method by performing experiments on a tabletop scene and a shelf scene for grasping with a Fetch mobile manipulator and a Franka Panda arm. The project page is available at \url{https://irvlutd.github.io/GraspTrajOpt}"
V3D-SLAM: Robust RGB-D SLAM in Dynamic Environments with 3D Semantic Geometry Voting,"Dang, Tuan; Nguyen, Khang; Huber, Manfred",,
KLILO: Kalman Filter based LiDAR-Inertial-Leg Odometry for Legged Robots,"Xu, Shaohang; Zhang, Wentao; Zhu, Lijun",,
Image to Patterning: Density-specified Patterning of Micro-structured Surfaces with a Mobile Robot,"Taylor, Annalisa; Landis, Malachi; Wang, Yaoke; Murphey, Todd; Guo, Ping",,
Lightweight Fisheye Object Detection Network with Transformer-based Feature Enhancement for Autonomous Driving,"Cao, Hu; li, Yanpeng; Liu, Yinlong; Li, Xinyi; Chen, Guang; Knoll, Alois",,
Stable Wheel Gait Generation for Planar X-shaped Walker with Telescopic Legs Based on Asymmetric Impact Posture,"Asano, Fumihiko; Komori, Mikito; Sedoguchi, Taiki; Zheng, Yanqiu",,
Behavior-Actor: Behavioral Decomposition and Efficient-Training for Robotic Manipulation,"Jiang, Wenyi; Xv, Baowei; Cui, Zhihao",,
Extended Tree Search for Robot Task and Motion Planning,"REN, Tianyu; Chalvatzaki, Georgia; Peters, Jan",https://arxiv.org/abs/2103.05456,"Integrated task and motion planning (TAMP) is desirable for generalized autonomy robots but it is challenging at the same time. TAMP requires the planner to not only search in both the large symbolic task space and the high-dimension motion space but also deal with the infeasible task actions due to its intrinsic hierarchical process. We propose a novel decision-making framework for TAMP by constructing an extended decision tree for both symbolic task planning and high-dimension motion variable binding. We integrate top-k planning for generating explicitly a skeleton space where a variety of candidate skeleton plans are at disposal. Moreover, we effectively combine this skeleton space with the resultant motion variable spaces into a single extended decision space. Accordingly, we use Monte-Carlo Tree Search (MCTS) to ensure an exploration-exploitation balance at each decision node and optimize globally to produce optimal solutions. The proposed seamless combination of symbolic top-k planning with streams, with the proved optimality of MCTS, leads to a powerful planning algorithm that can handle the combinatorial complexity of long-horizon manipulation tasks. We empirically evaluate our proposed algorithm in challenging robot tasks with different domains that require multi-stage decisions and show how our method can overcome the large task space and motion space through its effective tree search compared to its most competitive baseline method."
Visual Timing For Sound Source Depth Estimation in the Wild,"Sun, Wei; Qiu, Lili",,
A Novel Framework for Structure Descriptors-Guided Hand-drawn Floor Plan Reconstruction,"zhang, zhentong; liu, juan; li, xinde; Hu, Chuanfei; Dunkin, Fir; zhang, shaokun",,
Language-driven Grasp Detection with Mask-guided Attention,"Van Vo, Tuan; Vu, Minh Nhat; Huang, Baoru; Vuong, An Dinh; Le, Ngan; Vo, Thieu; Nguyen, Anh",https://arxiv.org/abs/2407.19877,"Grasp detection is an essential task in robotics with various industrial applications. However, traditional methods often struggle with occlusions and do not utilize language for grasping. Incorporating natural language into grasp detection remains a challenging task and largely unexplored. To address this gap, we propose a new method for language-driven grasp detection with mask-guided attention by utilizing the transformer attention mechanism with semantic segmentation features. Our approach integrates visual data, segmentation mask features, and natural language instructions, significantly improving grasp detection accuracy. Our work introduces a new framework for language-driven grasp detection, paving the way for language-driven robotic applications. Intensive experiments show that our method outperforms other recent baselines by a clear margin, with a 10.0% success score improvement. We further validate our method in real-world robotic experiments, confirming the effectiveness of our approach."
TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural Radiance Field Optimization,"Tan, Zhen; Zhou, Zongtan; Ge, Yangbing; Wang, Zi; Chen, Xieyuanli; Hu, Dewen",https://arxiv.org/abs/2405.07027,"The reliance on accurate camera poses is a significant barrier to the widespread deployment of Neural Radiance Fields (NeRF) models for 3D reconstruction and SLAM tasks. The existing method introduces monocular depth priors to jointly optimize the camera poses and NeRF, which fails to fully exploit the depth priors and neglects the impact of their inherent noise. In this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that enables training NeRF from unknown camera poses - by jointly optimizing learnable parameters of the radiance field and camera poses. Our approach explicitly utilizes monocular depth priors through three key advancements: 1) we propose a novel depth-based ray sampling strategy based on the truncated normal distribution, which improves the convergence speed and accuracy of pose estimation; 2) to circumvent local minima and refine depth geometry, we introduce a coarse-to-fine training strategy that progressively improves the depth precision; 3) we propose a more robust inter-frame point constraint that enhances robustness against depth noise during training. The experimental results on three datasets demonstrate that TD-NeRF achieves superior performance in the joint optimization of camera pose and NeRF, surpassing prior works, and generates more accurate depth geometry. The implementation of our method has been released at https://github.com/nubot-nudt/TD-NeRF."
"An Autonomous, 3D Printed, Waterjet-Powered, Open-Source Robotic Trimaran for Environmental Inspection and Monitoring","OBrien, Reuben; Lambrechtse-Reid, Martin; Liarokapis, Minas",,
Anchor-Oriented Localized Voronoi Partitioning for GPS-denied Multi-Robot Coverage,"Munir, Aiman; Latif, Ehsan; Parasuraman, Ramviyas",https://arxiv.org/abs/2407.06296,"Multi-robot coverage is crucial in numerous applications, including environmental monitoring, search and rescue operations, and precision agriculture. In modern applications, a multi-robot team must collaboratively explore unknown spatial fields in GPS-denied and extreme environments where global localization is unavailable. Coverage algorithms typically assume that the robot positions and the coverage environment are defined in a global reference frame. However, coordinating robot motion and ensuring coverage of the shared convex workspace without global localization is challenging. This paper proposes a novel anchor-oriented coverage (AOC) approach to generate dynamic localized Voronoi partitions based around a common anchor position. We further propose a consensus-based coordination algorithm that achieves agreement on the coverage workspace around the anchor in the robots' relative frames of reference. Through extensive simulations and real-world experiments, we demonstrate that the proposed anchor-oriented approach using localized Voronoi partitioning performs as well as the state-of-the-art coverage controller using GPS."
Development of Bidirectional Series Elastic Actuator with Torsion Coil Spring and Implementation to the Legged Robot,"Koda, Yuta; Osawa, Hiroshi; Nagatsuka, Norio; Kariya, Shinichi; Inagawa, Taeko; Ishizuka, Kensaku",,
Similarity Distance-Based Label Assignment for Tiny Object Detection,"Shi, Shuohao; fang, qiang; Zhao, Tong; Xu, Xin",https://arxiv.org/abs/2407.02394,"Tiny object detection is becoming one of the most challenging tasks in computer vision because of the limited object size and lack of information. The label assignment strategy is a key factor affecting the accuracy of object detection. Although there are some effective label assignment strategies for tiny objects, most of them focus on reducing the sensitivity to the bounding boxes to increase the number of positive samples and have some fixed hyperparameters need to set. However, more positive samples may not necessarily lead to better detection results, in fact, excessive positive samples may lead to more false positives. In this paper, we introduce a simple but effective strategy named the Similarity Distance (SimD) to evaluate the similarity between bounding boxes. This proposed strategy not only considers both location and shape similarity but also learns hyperparameters adaptively, ensuring that it can adapt to different datasets and various object sizes in a dataset. Our approach can be simply applied in common anchor-based detectors in place of the IoU for label assignment and Non Maximum Suppression (NMS). Extensive experiments on four mainstream tiny object detection datasets demonstrate superior performance of our method, especially, 1.8 AP points and 4.1 AP points of very tiny higher than the state-of-the-art competitors on AI-TOD. Code is available at: \url{https://github.com/cszzshi/SimD}."
An Efficient Position Reconfiguration Approach for Maximizing Lifetime of Fixed-wing Swarm Drones,"Liu, Han; Liu, Tian; Cui, Mingyue; Shan, Yunxiao; Zhao, Shuai; Huang, Kai",,
Lightweight Language-driven Grasp Detection using Conditional Consistency Model,"Nguyen, Nghia; Vu, Minh Nhat; Huang, Baoru; Vuong, An Dinh; Le, Ngan; Vo, Thieu; Nguyen, Anh",https://arxiv.org/abs/2407.17967,"Language-driven grasp detection is a fundamental yet challenging task in robotics with various industrial applications. In this work, we present a new approach for language-driven grasp detection that leverages the concept of lightweight diffusion models to achieve fast inference time. By integrating diffusion processes with grasping prompts in natural language, our method can effectively encode visual and textual information, enabling more accurate and versatile grasp positioning that aligns well with the text query. To overcome the long inference time problem in diffusion models, we leverage the image and text features as the condition in the consistency model to reduce the number of denoising timesteps during inference. The intensive experimental results show that our method outperforms other recent grasp detection methods and lightweight diffusion models by a clear margin. We further validate our method in real-world robotic experiments to demonstrate its fast inference time capability."
Disentangled Acoustic Fields For Multimodal Physical Scene Understanding,"Yin, Jie; Luo, Andrew; Du, Yilun; Cherian, Anoop; Marks, Tim K.; Le Roux, Jonathan; Gan, Chuang",https://arxiv.org/abs/2407.11333,"We study the problem of multimodal physical scene understanding, where an embodied agent needs to find fallen objects by inferring object properties, direction, and distance of an impact sound source. Previous works adopt feed-forward neural networks to directly regress the variables from sound, leading to poor generalization and domain adaptation issues. In this paper, we illustrate that learning a disentangled model of acoustic formation, referred to as disentangled acoustic field (DAF), to capture the sound generation and propagation process, enables the embodied agent to construct a spatial uncertainty map over where the objects may have fallen. We demonstrate that our analysis-by-synthesis framework can jointly infer sound properties by explicitly decomposing and factorizing the latent space of the disentangled model. We further show that the spatial uncertainty map can significantly improve the success rate for the localization of fallen objects by proposing multiple plausible exploration locations."
SGNet: Salient Geometric Network for Point Cloud Registration,"Wu, Qianliang; Ding, Yaqing; Luo, Lei; Jiang, Haobo; Gu, Shuo; Zhou, Chuanwei; Xie, Jin; YANG, JIAN",https://arxiv.org/abs/2309.06207,"Point Cloud Registration (PCR) is a critical and challenging task in computer vision. One of the primary difficulties in PCR is identifying salient and meaningful points that exhibit consistent semantic and geometric properties across different scans. Previous methods have encountered challenges with ambiguous matching due to the similarity among patch blocks throughout the entire point cloud and the lack of consideration for efficient global geometric consistency. To address these issues, we propose a new framework that includes several novel techniques. Firstly, we introduce a semantic-aware geometric encoder that combines object-level and patch-level semantic information. This encoder significantly improves registration recall by reducing ambiguity in patch-level superpoint matching. Additionally, we incorporate a prior knowledge approach that utilizes an intrinsic shape signature to identify salient points. This enables us to extract the most salient super points and meaningful dense points in the scene. Secondly, we introduce an innovative transformer that encodes High-Order (HO) geometric features. These features are crucial for identifying salient points within initial overlap regions while considering global high-order geometric consistency. To optimize this high-order transformer further, we introduce an anchor node selection strategy. By encoding inter-frame triangle or polyhedron consistency features based on these anchor nodes, we can effectively learn high-order geometric features of salient super points. These high-order features are then propagated to dense points and utilized by a Sinkhorn matching module to identify key correspondences for successful registration. In our experiments conducted on well-known datasets such as 3DMatch/3DLoMatch and KITTI, our approach has shown promising results, highlighting the effectiveness of our novel method."
Adv3D: Generating 3D Adversarial Examples for 3D Object Detection in Driving Scenarios with NeRF,"Li, Leheng; Lian, Qing; Chen, Yingcong",https://arxiv.org/abs/2309.01351,"Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To generate physically realizable adversarial examples, we propose primitive-aware sampling and semantic-guided regularization that enable 3D patch attacks with camouflage adversarial texture. Experimental results demonstrate that the trained adversarial NeRF generalizes well to different poses, scenes, and 3D detectors. Finally, we provide a defense method to our attacks that involves adversarial training through data augmentation. Project page: https://len-li.github.io/adv3d-web"
Gravity-aware Grasp Generation with Implicit Grasp Mode Selection for Underactuated Hands,"Ko, Tianyi; Ikeda, Takuya; Stewart, Thomas; Lee, Robert; Nishiwaki, Koichi",https://arxiv.org/abs/2312.11804,"Learning-based grasp detectors typically assume a precision grasp, where each finger only has one contact point, and estimate the grasp probability. In this work, we propose a data generation and learning pipeline that can leverage power grasping, which has more contact points with an enveloping configuration and is robust against both positioning error and force disturbance. To train a grasp detector to prioritize power grasping while still keeping precision grasping as the secondary choice, we propose to train the network against the magnitude of disturbance in the gravity direction a grasp can resist (gravity-rejection score) rather than the binary classification of success. We also provide an efficient data generation pipeline for a dataset with gravity-rejection score annotation. In addition to thorough ablation studies, quantitative evaluation in both simulation and real-robot clarifies the significant improvement in our approach, especially when the objects are heavy."
Efficient Global Trajectory Planning for Multi-robot System with Affinely Deformable Formation,"Sha, Hao; Cui, Yuxiang; Lu, Wangtao; Zhang, Dongkun; Wang, Chaoqun; Wu, Jun; Xiong, Rong; Wang, Yue",,
Rocket Landing Control with Random Annealing Jump Start Reinforcement Learning,"Jiang, Yuxuan; Yang, Yujie; Lan, Zhiqian; Zhan, Guojian; Li, Shengbo Eben; Sun, Qi; Ma, Jian; Yu, Tianwen; Zhang, Changwu",https://arxiv.org/abs/2407.15083,"Rocket recycling is a crucial pursuit in aerospace technology, aimed at reducing costs and environmental impact in space exploration. The primary focus centers on rocket landing control, involving the guidance of a nonlinear underactuated rocket with limited fuel in real-time. This challenging task prompts the application of reinforcement learning (RL), yet goal-oriented nature of the problem poses difficulties for standard RL algorithms due to the absence of intermediate reward signals. This paper, for the first time, significantly elevates the success rate of rocket landing control from 8% with a baseline controller to 97% on a high-fidelity rocket model using RL. Our approach, called Random Annealing Jump Start (RAJS), is tailored for real-world goal-oriented problems by leveraging prior feedback controllers as guide policy to facilitate environmental exploration and policy learning in RL. In each episode, the guide policy navigates the environment for the guide horizon, followed by the exploration policy taking charge to complete remaining steps. This jump-start strategy prunes exploration space, rendering the problem more tractable to RL algorithms. The guide horizon is sampled from a uniform distribution, with its upper bound annealing to zero based on performance metrics, mitigating distribution shift and mismatch issues in existing methods. Additional enhancements, including cascading jump start, refined reward and terminal condition, and action smoothness regulation, further improve policy performance and practical applicability. The proposed method is validated through extensive evaluation and Hardware-in-the-Loop testing, affirming the effectiveness, real-time feasibility, and smoothness of the proposed controller."
Mitigating Adversarial Perturbations for Deep Reinforcement Learning via Vector Quantization,"Luu, Tung; Nguyen, Thanh; Tee, Joshua Tian Jin; Kim, Sungwoong; Yoo, Chang D.",,
S-BUN: Soft Bifunctional Utility Module for Robot Sensing and Signaling,"Mahuttanatan, Suksakaow; Asawalertsak, Naris; Paripurana, Jinjuta; Tarapongnivat, Kanut; Chuthong, Thirawat; Manoonpong, Poramate",,
NDT-Map-Code: A 3D global descriptor for real-time loop closure detection in lidar SLAM,"Liao, Lizhou; Yan, Wenlei; Sun, Li; Bai, Xinhui; You, Zhenxing; Yuan, Hongyuan; Fu, Chunyun",https://arxiv.org/abs/2307.08221,"Loop-closure detection, also known as place recognition, aiming to identify previously visited locations, is an essential component of a SLAM system. Existing research on lidar-based loop closure heavily relies on dense point cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal Distribution Transform) based global descriptor, NDT-Map-Code, designed for both on-road driving and underground valet parking scenarios. NDT-Map-Code can be directly extracted from the NDT map without the need for a dense point cloud, resulting in excellent scalability and low maintenance cost. The NDT representation is leveraged to identify representative patterns, which are further encoded according to their spatial location (bearing, range, and height). Experimental results on the NIO underground parking lot dataset and the KITTI dataset demonstrate that our method achieves significantly better performance compared to the state-of-the-art."
Bayesian Floor Field: Transferring people flow predictions across environments,"Verdoja, Francesco; Kucner, Tomasz Piotr; Kyrki, Ville",https://arxiv.org/abs/2208.10851,"Mapping people dynamics is a crucial skill for robots, because it enables them to coexist in human-inhabited environments. However, learning a model of people dynamics is a time consuming process which requires observation of large amount of people moving in an environment. Moreover, approaches for mapping dynamics are unable to transfer the learned models across environments: each model is only able to describe the dynamics of the environment it has been built in. However, the impact of architectural geometry on people's movement can be used to anticipate their patterns of dynamics, and recent work has looked into learning maps of dynamics from occupancy. So far however, approaches based on trajectories and those based on geometry have not been combined. In this work we propose a novel Bayesian approach to learn people dynamics able to combine knowledge about the environment geometry with observations from human trajectories. An occupancy-based deep prior is used to build an initial transition model without requiring any observations of pedestrian; the model is then updated when observations become available using Bayesian inference. We demonstrate the ability of our model to increase data efficiency and to generalize across real large-scale environments, which is unprecedented for maps of dynamics."
Resource-Aware Collaborative Monte Carlo Localization with Distribution Compression,"Zimmerman, Nicky; Giusti, Alessandro; Guzzi, Jerome",https://arxiv.org/abs/2404.02010,"Global localization is essential in enabling robot autonomy, and collaborative localization is key for multi-robot systems. In this paper, we address the task of collaborative global localization under computational and communication constraints. We propose a method which reduces the amount of information exchanged and the computational cost. We also analyze, implement and open-source seminal approaches, which we believe to be a valuable contribution to the community. We exploit techniques for distribution compression in near-linear time, with error guarantees. We evaluate our approach and the implemented baselines on multiple challenging scenarios, simulated and real-world. Our approach can run online on an onboard computer. We release an open-source C++/ROS2 implementation of our approach, as well as the baselines"
Online Efficient Safety-Critical Control for Mobile Robots in Unknown Dynamic Multi-Obstacle Environments,"Zhang, Yu; Tian, Guangyao; Wen, Long; Yao, Xiangtong; Zhang, Liding; Bing, Zhenshan; He, Wei; Knoll, Alois",https://arxiv.org/abs/2402.16449,"This paper proposes a LiDAR-based goal-seeking and exploration framework, addressing the efficiency of online obstacle avoidance in unstructured environments populated with static and moving obstacles. This framework addresses two significant challenges associated with traditional dynamic control barrier functions (D-CBFs): their online construction and the diminished real-time performance caused by utilizing multiple D-CBFs. To tackle the first challenge, the framework's perception component begins with clustering point clouds via the DBSCAN algorithm, followed by encapsulating these clusters with the minimum bounding ellipses (MBEs) algorithm to create elliptical representations. By comparing the current state of MBEs with those stored from previous moments, the differentiation between static and dynamic obstacles is realized, and the Kalman filter is utilized to predict the movements of the latter. Such analysis facilitates the D-CBF's online construction for each MBE. To tackle the second challenge, we introduce buffer zones, generating Type-II D-CBFs online for each identified obstacle. Utilizing these buffer zones as activation areas substantially reduces the number of D-CBFs that need to be activated. Upon entering these buffer zones, the system prioritizes safety, autonomously navigating safe paths, and hence referred to as the exploration mode. Exiting these buffer zones triggers the system's transition to goal-seeking mode. We demonstrate that the system's states under this framework achieve safety and asymptotic stabilization. Experimental results in simulated and real-world environments have validated our framework's capability, allowing a LiDAR-equipped mobile robot to efficiently and safely reach the desired location within dynamic environments containing multiple obstacles."
Using Augmented Reality in Human-Robot Assembly: A Comparative Study of Eye-Gaze and Hand-Ray Pointing Methods,"Tadeja, Slawomir Konrad; Zhou, Tianye; Capponi, Matteo; Walas, Krzysztof, Tadeusz; Bohné, Thomas; Forni, Fulvio",,
Deep Ad-hoc Sub-Team Partition Learning for Multi-Agent Air Combat Cooperation,"Fan, Songyuan; Piao, Haiyin; Hu, Yi; Jiang, Feng; Yang, Roushu",,
Robustness Study of Optimal Geometries for Cooperative Multi-Robot Localization,"Theunissen, Mathilde; Fantoni, Isabelle; Malis, Ezio; Martinet, Philippe",,
Offline Meta-Reinforcement Learning with Evolving Gradient Agreement,"Chen, Jiaxing; Yuan, Weilin; Chen, Shaofei; liu, furong; MA, AO; Hu, Zhenzhen; Li, Peng",,
Gradient-based Regularization for Action Smoothness in Robotic Control with Reinforcement Learning,"Li, Yi; Cao, Hoang-Giang; Dao, Cong-Tinh; Chen, Yu-Cheng; Wu, I-Chen",https://arxiv.org/abs/2407.04315,"Deep Reinforcement Learning (DRL) has achieved remarkable success, ranging from complex computer games to real-world applications, showing the potential for intelligent agents capable of learning in dynamic environments. However, its application in real-world scenarios presents challenges, including the jerky problem, in which jerky trajectories not only compromise system safety but also increase power consumption and shorten the service life of robotic and autonomous systems. To address jerky actions, a method called conditioning for action policy smoothness (CAPS) was proposed by adding regularization terms to reduce the action changes. This paper further proposes a novel method, named Gradient-based CAPS (Grad-CAPS), that modifies CAPS by reducing the difference in the gradient of action and then uses displacement normalization to enable the agent to adapt to invariant action scales. Consequently, our method effectively reduces zigzagging action sequences while enhancing policy expressiveness and the adaptability of our method across diverse scenarios and environments. In the experiments, we integrated Grad-CAPS with different reinforcement learning algorithms and evaluated its performance on various robotic-related tasks in DeepMind Control Suite and OpenAI Gym environments. The results demonstrate that Grad-CAPS effectively improves performance while maintaining a comparable level of smoothness compared to CAPS and Vanilla agents."
Whole-body Humanoid Robot Locomotion with Human Reference,"Zhang, Qiang; Cui, Peter; Yan, David; SUN, Jingkai; DUAN, YIQUN; Han, Gang; Zhao, Wen; ZHANG, Weining; Guo, Yijie; Zhang, Arthur; Xu, Renjing",https://arxiv.org/abs/2402.18294,"Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, ""Adam"", whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot."
Text3DAug  Prompted Instance Augmentation for LiDAR Perception,"Reichardt, Laurenz; Uhr, Luca; Wasenmüller, Oliver",https://arxiv.org/abs/2408.14253,"LiDAR data of urban scenarios poses unique challenges, such as heterogeneous characteristics and inherent class imbalance. Therefore, large-scale datasets are necessary to apply deep learning methods. Instance augmentation has emerged as an efficient method to increase dataset diversity. However, current methods require the time-consuming curation of 3D models or costly manual data annotation. To overcome these limitations, we propose Text3DAug, a novel approach leveraging generative models for instance augmentation. Text3DAug does not depend on labeled data and is the first of its kind to generate instances and annotations from text. This allows for a fully automated pipeline, eliminating the need for manual effort in practical applications. Additionally, Text3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor used. Comprehensive experimental analysis on LiDAR segmentation, detection and novel class discovery demonstrates that Text3DAug is effective in supplementing existing methods or as a standalone method, performing on par or better than established methods, however while overcoming their specific drawbacks. The code is publicly available."
PhotoBot: Reference-Guided Interactive Photography via Natural Language,"Limoyo, Oliver; Li, Jimmy; Rivkin, Dmitriy; Kelly, Jonathan; Dudek, Gregory",https://arxiv.org/abs/2401.11061,"We introduce PhotoBot, a framework for fully automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via reference images that are selected from a curated gallery. We leverage a visual language model (VLM) and an object detector to characterize the reference images via textual descriptions and then use a large language model (LLM) to retrieve relevant reference images based on a user's language query through text-based reasoning. To correspond the reference image and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across marked appearance variations. Using these features, we compute suggested pose adjustments for an RGB-D camera by solving a perspective-n-point (PnP) problem. We demonstrate our approach using a manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBot are often more aesthetically pleasing than those taken by users themselves, as measured by human feedback. We also show that PhotoBot can generalize to other reference sources such as paintings."
Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical Flow Estimation,"Negi, Shubham; Sharma, Deepika; Kosta, Adarsh Kumar; Roy, Kaushik",https://arxiv.org/abs/2306.02960,"In the field of robotics, event-based cameras are emerging as a promising low-power alternative to traditional frame-based cameras for capturing high-speed motion and high dynamic range scenes. This is due to their sparse and asynchronous event outputs. Spiking Neural Networks (SNNs) with their asynchronous event-driven compute, show great potential for extracting the spatio-temporal features from these event streams. In contrast, the standard Analog Neural Networks (ANNs) fail to process event data effectively. However, training SNNs is difficult due to additional trainable parameters (thresholds and leaks), vanishing spikes at deeper layers, and a non-differentiable binary activation function. Furthermore, an additional data structure, membrane potential, responsible for keeping track of temporal information, must be fetched and updated at every timestep in SNNs. To overcome these challenges, we propose a novel SNN-ANN hybrid architecture that combines the strengths of both. Specifically, we leverage the asynchronous compute capabilities of SNN layers to effectively extract the input temporal information. Concurrently, the ANN layers facilitate training and efficient hardware deployment on traditional machine learning hardware such as GPUs. We provide extensive experimental analysis for assigning each layer to be spiking or analog, leading to a network configuration optimized for performance and ease of training. We evaluate our hybrid architecture for optical flow estimation on DSEC-flow and Multi-Vehicle Stereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybrid SNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE) with 22% lower energy consumption compared to Full-SNN, and 48% lower AEE compared to Full-ANN, while maintaining comparable energy usage."
On the Modularity of Elementary Dynamic Actions,"Nah, Moses; Lachner, Johannes; Tessari, Federico; Hogan, Neville",https://arxiv.org/abs/2309.15271,"In this paper, a kinematically modular approach to robot control is presented. The method involves structures called Elementary Dynamic Actions and a network model combining these elements. With this control framework, a rich repertoire of movements can be generated by combination of basic modules. The problems of solving inverse kinematics, managing kinematic singularity and kinematic redundancy are avoided. The modular approach is robust against contact and physical interaction, which makes it particularly effective for contact-rich manipulation. Each kinematic module can be learned by Imitation Learning, thereby resulting in a modular learning strategy for robot control. The theoretical foundations and their real robot implementation are presented. Using a KUKA LBR iiwa14 robot, three tasks were considered: (1) generating a sequence of discrete movements, (2) generating a combination of discrete and rhythmic movements, and (3) a drawing and erasing task. The results obtained indicate that this modular approach has the potential to simplify the generation of a diverse range of robot actions."
DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation,"Ikeda, Takuya; Zakharov, Sergey; Ko, Tianyi; Irshad, Muhammad Zubair; Lee, Robert; Liu, Katherine; Ambrus, Rares; Nishiwaki, Koichi",https://arxiv.org/abs/2402.12647,"This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain."
P4: Pruning and Prediction-Based Priority Planning,"Yang, Rui; Gupta, Rajiv",,
A Point-Line Features Fusion Method for Fast and Robust Monocular Visual-Inertial Initialization,"Xie, Guoqiang; Chen, Jie; Tang, Tianhang; Chen, Zeyu; Lei, Ling; Liu, Yiguang",,
Adaptive Passivation of Admittance Controllers by Bypassing Power to Null Space on Redundant Manipulators,"Yun, Yeoil; Oh, DongJun; Song, Eun Jeong; Choi, Hyouk Ryeol; Moon, Hyungpil; Koo, Ja Choon",,
EasyHeC++: Fully Automatic Hand-Eye Calibration with Pretrained Image Models,"Chen, Linghao; Zheng, Kangfu; Hong, Zhengdong; Zhou, Xiaowei; Su, Hao",,
Enhanced Language-guided Robot Navigation with Panoramic Semantic Depth Perception and Cross-modal Fusion,"Wang, Liuyi; Tang, Jiagui; He, Zongtao; Dang, Ronghao; Liu, Chengju; Chen, Qijun",,
Active Information Gathering for Long-Horizon Navigation Under Uncertainty by Predicting the Value of Information,"Arnob, Raihan Islam; Stein, Gregory",,
Experience-Learning Inspired Two-Step Reward Method for Efficient Legged Locomotion Learning Towards Natural and Robust Gaits,"Li, Yinghui; Wu, Jinze; Liu, Xin; Guo, Weizhong; Xue, Yufei",https://arxiv.org/abs/2401.12389,"Multi-legged robots offer enhanced stability in complex terrains, yet autonomously learning natural and robust motions in such environments remains challenging. Drawing inspiration from animals' progressive learning patterns, from simple to complex tasks, we introduce a universal two-stage learning framework with two-step reward setting based on self-acquired experience, which efficiently enables legged robots to incrementally learn natural and robust movements. In the first stage, robots learn through gait-related rewards to track velocity on flat terrain, acquiring natural, robust movements and generating effective motion experience data. In the second stage, mirroring animal learning from existing experiences, robots learn to navigate challenging terrains with natural and robust movements using adversarial imitation learning. To demonstrate our method's efficacy, we trained both quadruped robots and a hexapod robot, and the policy were successfully transferred to a physical quadruped robot GO1, which exhibited natural gait patterns and remarkable robustness in various terrains."
Multi-Agent Teamwise Cooperative Path Finding and Traffic Intersection Coordination,"Ren, Zhongqiang; Cai, Yilin; Wang, Hesheng",,
Can Reasons Help Improve Pedestrian Intent Estimation? A Cross-Modal Approach,"Khindkar, Vaishnavi; Balasubramanian, Vineeth; Arora, Chetan; Subramanian, Anbumani; Jawahar, C.V.",,
Representing 3D sparse map points and lines for camera relocalization,"Bui, Bach-Thuan; Bui, Huy Hoang; Tran, Dinh Tuan; Lee, Joo-Ho",https://arxiv.org/abs/2402.18011,"Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features. However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks. In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings. Specifically, we utilize a single transformer block to encode line features, effectively transforming them into distinctive point-like descriptors. Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets. Through the integration of self- and cross-attention within several graph layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs. In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations. Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies. The source code and demo videos of this work are publicly available at: https://thpjp.github.io/pl2map/"
Active propulsion noise shaping for multi-rotor aircraft localization,"Serussi, Gabriele; Shor, Tamir; Hirshberg, Tom; Baskin, Chaim; Bronstein, Alexander",https://arxiv.org/abs/2402.17289,"Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes. However, visual localization and odometry techniques suffer from poor performance in low or direct sunlight, a limited field of view, and vulnerability to occlusions. Acoustic sensing can serve as a complementary or even alternative modality for vision in many situations, and it also has the added benefits of lower system cost and energy footprint, which is especially important for micro aircraft. This paper proposes actively controlling and shaping the aircraft propulsion noise generated by the rotors to benefit localization tasks, rather than considering it a harmful nuisance. We present a neural network architecture for selfnoise-based localization in a known environment. We show that training it simultaneously with learning time-varying rotor phase modulation achieves accurate and robust localization. The proposed methods are evaluated using a computationally affordable simulation of MAV rotor noise in 2D acoustic environments that is fitted to real recordings of rotor pressure fields."
HPHS: Hierarchical Planning based on Hybrid Frontier Sampling for Unknown Environments Exploration,"Long, Shijun; Li, Ying; Wu, Chenming; Xu, Bin; Fan, Wei",https://arxiv.org/abs/2407.10660,"Rapid sampling from the environment to acquire available frontier points and timely incorporating them into subsequent planning to reduce fragmented regions are critical to improve the efficiency of autonomous exploration. We propose HPHS, a fast and effective method for the autonomous exploration of unknown environments. In this work, we efficiently sample frontier points directly from the LiDAR data and the local map around the robot, while exploiting a hierarchical planning strategy to provide the robot with a global perspective. The hierarchical planning framework divides the updated environment into multiple subregions and arranges the order of access to them by considering the overall revenue of the global path. The combination of the hybrid frontier sampling method and hierarchical planning strategy reduces the complexity of the planning problem and mitigates the issue of region remnants during the exploration process. Detailed simulation and real-world experiments demonstrate the effectiveness and efficiency of our approach in various aspects. The source code will be released to benefit the further research."
Time-Optimal Path Parameterization for Cooperative Multi-Arm Robotic Systems with Third-Order Constraints,"Dio, Maximilian; Graichen, Knut; Völz, Andreas",,
MERSYS: A Collaborative Estimation and Dense Mapping System for Multi-Agent Generic SLAM,"Lai, Qianhua; Zhao, Enhao; Fan, Shicai; Zou, Jianxiao",,
A Slices Perspective for Incremental Nonparametric Inference in High Dimensional State Spaces,"Shienman, Moshe; Levy-Or, Ohad; Kaess, Michael; Indelman, Vadim",https://arxiv.org/abs/2405.16453,"We introduce an innovative method for incremental nonparametric probabilistic inference in high-dimensional state spaces. Our approach leverages \slices from high-dimensional surfaces to efficiently approximate posterior distributions of any shape. Unlike many existing graph-based methods, our \slices perspective eliminates the need for additional intermediate reconstructions, maintaining a more accurate representation of posterior distributions. Additionally, we propose a novel heuristic to balance between accuracy and efficiency, enabling real-time operation in nonparametric scenarios. In empirical evaluations on synthetic and real-world datasets, our \slices approach consistently outperforms other state-of-the-art methods. It demonstrates superior accuracy and achieves a significant reduction in computational complexity, often by an order of magnitude."
Enhanced Robotic Assistance for Human Activities through Human-Object Interaction Segment Prediction,"Wu, Yuankai; Messaoud, Rayene; Hildebrandt, Arne-Christoph; Baldini, Marco; Salihu, Driton; Patsch, Constantin; Steinbach, Eckehard",,
WidthFormer: Toward Efficient Transformer-based BEV View Transformation,"Yang, Chenhongyi; Lin, Tianwei; Huang, Lichao; Crowley, Elliot J.",https://arxiv.org/abs/2401.03836,"We present WidthFormer, a novel transformer-based module to compute Bird's-Eye-View (BEV) representations from multi-view cameras for real-time autonomous-driving applications. WidthFormer is computationally efficient, robust and does not require any special engineering effort to deploy. We first introduce a novel 3D positional encoding mechanism capable of accurately encapsulating 3D geometric information, which enables our model to compute high-quality BEV representations with only a single transformer decoder layer. This mechanism is also beneficial for existing sparse 3D object detectors. Inspired by the recently proposed works, we further improve our model's efficiency by vertically compressing the image features when serving as attention keys and values, and then we develop two modules to compensate for potential information loss due to feature compression. Experimental evaluation on the widely-used nuScenes 3D object detection benchmark demonstrates that our method outperforms previous approaches across different 3D detection architectures. More importantly, our model is highly efficient. For example, when using $256\times 704$ input images, it achieves 1.5 ms and 2.8 ms latency on NVIDIA 3090 GPU and Horizon Journey-5 computation solutions. Furthermore, WidthFormer also exhibits strong robustness to different degrees of camera perturbations. Our study offers valuable insights into the deployment of BEV transformation methods in real-world, complex road environments. Code is available at https://github.com/ChenhongyiYang/WidthFormer ."
Object Segmentation from Open-Vocabulary Manipulation Instructions Based on Optimal Transport Polygon Matching with Multimodal Foundation Models,"Nishimura, Takayuki; Kuyo, Katsuyuki; Kambara, Motonari; Sugiura, Komei",https://arxiv.org/abs/2407.00985,"We consider the task of generating segmentation masks for the target object from an object manipulation instruction, which allows users to give open vocabulary instructions to domestic service robots. Conventional segmentation generation approaches often fail to account for objects outside the camera's field of view and cases in which the order of vertices differs but still represents the same polygon, which leads to erroneous mask generation. In this study, we propose a novel method that generates segmentation masks from open vocabulary instructions. We implement a novel loss function using optimal transport to prevent significant loss where the order of vertices differs but still represents the same polygon. To evaluate our approach, we constructed a new dataset based on the REVERIE dataset and Matterport3D dataset. The results demonstrated the effectiveness of the proposed method compared with existing mask generation methods. Remarkably, our best model achieved a +16.32% improvement on the dataset compared with a representative polygon-based method."
FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework,"Meyer, Lukas; Gilson, Andreas; Schmid, Ute; Stamminger, Marc",https://arxiv.org/abs/2408.06190,"We introduce FruitNeRF, a unified novel fruit counting framework that leverages state-of-the-art view synthesis methods to count any fruit type directly in 3D. Our framework takes an unordered set of posed images captured by a monocular camera and segments fruit in each image. To make our system independent of the fruit type, we employ a foundation model that generates binary segmentation masks for any fruit. Utilizing both modalities, RGB and semantic, we train a semantic neural radiance field. Through uniform volume sampling of the implicit Fruit Field, we obtain fruit-only point clouds. By applying cascaded clustering on the extracted point cloud, our approach achieves precise fruit count.The use of neural radiance fields provides significant advantages over conventional methods such as object tracking or optical flow, as the counting itself is lifted into 3D. Our method prevents double counting fruit and avoids counting irrelevant fruit.We evaluate our methodology using both real-world and synthetic datasets. The real-world dataset consists of three apple trees with manually counted ground truths, a benchmark apple dataset with one row and ground truth fruit location, while the synthetic dataset comprises various fruit types including apple, plum, lemon, pear, peach, and mango.Additionally, we assess the performance of fruit counting using the foundation model compared to a U-Net."
Vine Robots That Evert through Bending,"Wu, Rui; Mintchev, Stefano",https://arxiv.org/abs/2212.03951,"Everting, soft growing vine robots benefit from reduced friction with their environment, which allows them to navigate challenging terrain. Vine robots can use air pouches attached to their sides for lateral steering. However, when all pouches are serially connected, the whole robot can only perform one constant curvature in free space. It must contact the environment to navigate through obstacles along paths with multiple turns. This work presents a multi-segment vine robot that can navigate complex paths without interacting with its environment. This is achieved by a new steering method that selectively actuates each single pouch at the tip, providing high degrees of freedom with few control inputs. A small magnetic valve connects each pouch to a pressure supply line. A motorized tip mount uses an interlocking mechanism and motorized rollers on the outer material of the vine robot. As each valve passes through the tip mount, a permanent magnet inside the tip mount opens the valve so the corresponding pouch is connected to the pressure supply line at the same moment. Novel cylindrical pneumatic artificial muscles (cPAMs) are integrated into the vine robot and inflate to a cylindrical shape for improved bending characteristics compared to other state-of-the-art vine robots. The motorized tip mount controls a continuous eversion speed and enables controlled retraction. A final prototype was able to repeatably grow into different shapes and hold these shapes. We predict the path using a model that assumes a piecewise constant curvature along the outside of the multi-segment vine robot. The proposed multi-segment steering method can be extended to other soft continuum robot designs."
GenerOcc: Self-supervised Framework of Real-time 3D Occupancy Prediction for Monocular Generic Cameras,"Pan, Xianghui; Du, Jiayuan; Liu, Chengju; Chen, Qijun; Su, Shuai; Zong, Wenhao; Wang, Xiao",,
Contact Stability Control of Stepping Over Partial Footholds Using Plantar Tactile Feedback,"Guadarrama-Olvera, J. Rogelio; Kajita, Shuuji; Kanehiro, Fumio; Cheng, Gordon",,
Navigated Locomotion and Controllable Splitting of a Microswarm in a Complex Environment,"Liu, Yuezhen; Zeng, Guangjun; Du, Xingzhou; Fang, Kaiwen; Yu, Jiangfan",,
Just Flip: Flipped Observation Generation and Optimization for Neural Radiance Fields to Cover Unobserved View,"Lee, Sibaek; Kang, Kyeongsu; Yu, Hyeonwoo",https://arxiv.org/abs/2303.06335,"With the advent of Neural Radiance Field (NeRF), representing 3D scenes through multiple observations has shown remarkable improvements in performance. Since this cutting-edge technique is able to obtain high-resolution renderings by interpolating dense 3D environments, various approaches have been proposed to apply NeRF for the spatial understanding of robot perception. However, previous works are challenging to represent unobserved scenes or views on the unexplored robot trajectory, as these works do not take into account 3D reconstruction without observation information. To overcome this problem, we propose a method to generate flipped observation in order to cover unexisting observation for unexplored robot trajectory. To achieve this, we propose a data augmentation method for 3D reconstruction using NeRF by flipping observed images, and estimating flipped camera 6DOF poses. Our technique exploits the property of objects being geometrically symmetric, making it simple but fast and powerful, thereby making it suitable for robotic applications where real-time performance is important. We demonstrate that our method significantly improves three representative perceptual quality measures on the NeRF synthetic dataset."
Millipede-Inspired Multi-legged Magnetic Soft Robots for Targeted Locomotion in Tortuous Environments,"Wang, Yibin; Xiong, Yiting; Fang, Kaiwen; Yu, Jiangfan",,
Neural Trajectory Model: Implicit Neural Trajectory Representation for Trajectories Generation,"YU, Zihan; Tang, Yuqing",https://arxiv.org/abs/2402.01254,"Trajectory planning is a fundamental problem in robotics. It facilitates a wide range of applications in navigation and motion planning, control, and multi-agent coordination. Trajectory planning is a difficult problem due to its computational complexity and real-world environment complexity with uncertainty, non-linearity, and real-time requirements. The multi-agent trajectory planning problem adds another dimension of difficulty due to inter-agent interaction. Existing solutions are either search-based or optimization-based approaches with simplified assumptions of environment, limited planning speed, and limited scalability in the number of agents. In this work, we make the first attempt to reformulate single agent and multi-agent trajectory planning problem as query problems over an implicit neural representation of trajectories. We formulate such implicit representation as Neural Trajectory Models (NTM) which can be queried to generate nearly optimal trajectory in complex environments. We conduct experiments in simulation environments and demonstrate that NTM can solve single-agent and multi-agent trajectory planning problems. In the experiments, NTMs achieve (1) sub-millisecond panning time using GPUs, (2) almost avoiding all environment collision, (3) almost avoiding all inter-agent collision, and (4) generating almost shortest paths. We also demonstrate that the same NTM framework can also be used for trajectories correction and multi-trajectory conflict resolution refining low quality and conflicting multi-agent trajectories into nearly optimal solutions efficiently. (Open source code will be available at https://github.com/laser2099/neural-trajectory-model)"
Revolutionizing Battery Disassembly: The Design and Implementation of a Battery Disassembly Autonomous Mobile Manipulator Robot(BEAM-1),"Peng, Yanlong; Wang, Zhigang; Zhang, Yisheng; Zhang, Shengmin; cai, nan; Wu, Fan; Chen, Ming",https://arxiv.org/abs/2407.06590,"The efficient disassembly of end-of-life electric vehicle batteries(EOL-EVBs) is crucial for green manufacturing and sustainable development. The current pre-programmed disassembly conducted by the Autonomous Mobile Manipulator Robot(AMMR) struggles to meet the disassembly requirements in dynamic environments, complex scenarios, and unstructured processes. In this paper, we propose a Battery Disassembly AMMR(BEAM-1) system based on NeuralSymbolic AI. It detects the environmental state by leveraging a combination of multi-sensors and neural predicates and then translates this information into a quasi-symbolic space. In real-time, it identifies the optimal sequence of action primitives through LLM-heuristic tree search, ensuring high-precision execution of these primitives. Additionally, it employs positional speculative sampling using intuitive networks and achieves the disassembly of various bolt types with a meticulously designed end-effector. Importantly, BEAM-1 is a continuously learning embodied intelligence system capable of subjective reasoning like a human, and possessing intuition. A large number of real scene experiments have proved that it can autonomously perceive, decide, and execute to complete the continuous disassembly of bolts in multiple, multi-category, and complex situations, with a success rate of 98.78%. This research attempts to use NeuroSymbolic AI to give robots real autonomous reasoning, planning, and learning capabilities. BEAM-1 realizes the revolution of battery disassembly. Its framework can be easily ported to any robotic system to realize different application scenarios, which provides a ground-breaking idea for the design and implementation of future embodied intelligent robotic systems."
Flexible Informed Trees (FIT*): Adaptive Batch-Size Approach in Informed Sampling-Based Path Planning,"Zhang, Liding; Bing, Zhenshan; Chen, Kejia; Chen, Lingyun; Cai, Kuanqi; Zhang, Yu; Wu, Fan; Krumbholz, Peter; Yuan, Zhilin; Haddadin, Sami; Knoll, Alois",https://arxiv.org/abs/2310.12828,"In path planning, anytime almost-surely asymptotically optimal planners dominate the benchmark of sampling-based planners. A notable example is Batch Informed Trees (BIT*), where planners iteratively determine paths to batches of vertices within the exploration area. However, utilizing a consistent batch size is inefficient for initial pathfinding and optimal performance, it relies on effective task allocation. This paper introduces Flexible Informed Trees (FIT*), a sampling-based planner that integrates an adaptive batch-size method to enhance the initial path convergence rate. FIT* employs a flexible approach in adjusting batch sizes dynamically based on the inherent dimension of the configuration spaces and the hypervolume of the n-dimensional hyperellipsoid. By applying dense and sparse sampling strategy, FIT* improves convergence rate while finding successful solutions faster with lower initial solution cost. This method enhances the planner's ability to handle confined, narrow spaces in the initial finding phase and increases batch vertices sampling frequency in the optimization phase. FIT* outperforms existing single-query, sampling-based planners on the tested problems in R^2 to R^8, and was demonstrated on a real-world mobile manipulation task."
Outlier-Robust Geometric Perception: A Novel Thresholding-Based Estimator with Intra-Class Variance Maximization,"Sun, Lei",https://arxiv.org/abs/2204.01324,"Geometric perception problems are fundamental tasks in robotics and computer vision. In real-world applications, they often encounter the inevitable issue of outliers, preventing traditional algorithms from making correct estimates. In this paper, we present a novel general-purpose robust estimator TIVM (Thresholding with Intra-class Variance Maximization) that can collaborate with standard non-minimal solvers to efficiently reject outliers for geometric perception problems. First, we introduce the technique of intra-class variance maximization to design a dynamic 2-group thresholding method on the measurement residuals, aiming to distinctively separate inliers from outliers. Then, we develop an iterative framework that robustly optimizes the model by approaching the pure-inlier group using a multi-layered dynamic thresholding strategy as subroutine, in which a self-adaptive mechanism for layer-number tuning is further employed to minimize the user-defined parameters. We validate the proposed estimator on 3 classic geometric perception problems: rotation averaging, point cloud registration and category-level perception, and experiments show that it is robust against 70--90\% of outliers and can converge typically in only 3--15 iterations, much faster than state-of-the-art robust solvers such as RANSAC, GNC and ADAPT. Furthermore, another highlight is that: our estimator can retain approximately the same level of robustness even when the inlier-noise statistics of the problem are fully unknown."
Kinematics-aware Trajectory Generation and Prediction with Latent Stochastic Differential Modeling,"Jiao, Ruochen; Wang, Yixuan; Liu, Xiangguo; Zhan, Sinong; Huang, Chao; Zhu, Qi",https://arxiv.org/abs/2309.09317,"Trajectory generation and trajectory prediction are two critical tasks in autonomous driving, which generate various trajectories for testing during development and predict the trajectories of surrounding vehicles during operation, respectively. In recent years, emerging data-driven deep learning-based methods have shown great promise for these two tasks in learning various traffic scenarios and improving average performance without assuming physical models. However, it remains a challenging problem for these methods to ensure that the generated/predicted trajectories are physically realistic. This challenge arises because learning-based approaches often function as opaque black boxes and do not adhere to physical laws. Conversely, existing model-based methods provide physically feasible results but are constrained by predefined model structures, limiting their capabilities to address complex scenarios. To address the limitations of these two types of approaches, we propose a new method that integrates kinematic knowledge into neural stochastic differential equations (SDE) and designs a variational autoencoder based on this latent kinematics-aware SDE (LK-SDE) to generate vehicle motions. Experimental results demonstrate that our method significantly outperforms both model-based and learning-based baselines in producing physically realistic and precisely controllable vehicle trajectories. Additionally, it performs well in predicting unobservable physical variables in the latent space."
Two Teachers Are Better Than One: Leveraging Depth In Training Only For Unsupervised Obstacle Segmentation,"Eum, Sungmin; Lee, Hyungtae; Kwon, Heesung; Osteen, Philip; Harrison, Andre",,
Multi-Robot Communication-Aware Cooperative Belief Space Planning with Inconsistent Beliefs: An Action-Consistent Approach,"Kundu, Tanmoy; Rafaeli, Moshe; Indelman, Vadim",https://arxiv.org/abs/2403.05962,"Multi-robot belief space planning (MR-BSP) is essential for reliable and safe autonomy. While planning, each robot maintains a belief over the state of the environment and reasons how the belief would evolve in the future for different candidate actions. Yet, existing MR-BSP works have a common assumption that the beliefs of different robots are consistent at planning time. Such an assumption is often highly unrealistic, as it requires prohibitively extensive and frequent communication capabilities. In practice, each robot may have a different belief about the state of the environment. Crucially, when the beliefs of different robots are inconsistent, state-of-the-art MR-BSP approaches could result in a lack of coordination between the robots, and in general, could yield dangerous, unsafe and sub-optimal decisions. In this paper, we tackle this crucial gap. We develop a novel decentralized algorithm that is guaranteed to find a consistent joint action. For a given robot, our algorithm reasons for action preferences about 1) its local information, 2) what it perceives about the reasoning of the other robot, and 3) what it perceives about the reasoning of itself perceived by the other robot. This algorithm finds a consistent joint action whenever these steps yield the same best joint action obtained by reasoning about action preferences; otherwise, it self-triggers communication between the robots. Experimental results show efficacy of our algorithm in comparison with two baseline algorithms."
Applying Neural Monte Carlo Tree Search to Unsignalized Multi-Intersection Scheduling for Autonomous Vehicles,"Shi, Yucheng; wang, wenlong; Tao, Xiaowen; Dusparic, Ivana; Cahill, Vinny",,
SWIFT: Strategic Weather-informed Image-based Forecasting for Trajectories,"Xia, Youya; Nino, Jose; Han, Yutao; Campbell, Mark",,
Scheduling of Robotic Cellular Manufacturing Systems with Timed Petri Nets and Reinforcement Learning,"Yao, ZhuTao; Huang, Bo; Lv, Jianyong; Lu, Xiaoyu",,
Learning Concept-Based Causal Transition and Symbolic Reasoning for Visual Planning,"Qian, Yilue; Yu, Peiyu; Wu, Ying Nian; Su, Yao; Wang, Wei; Fan, Lifeng",https://arxiv.org/abs/2310.03325,"Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiveness of the proposed model, we collect a large-scale visual planning dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this challenging dataset demonstrate the superior performance of our method in visual task planning. Empirically, we show that our framework can generalize to unseen task trajectories, unseen object categories, and real-world data. Further details of this work are provided at https://fqyqc.github.io/ConTranPlan/."
AnytimeFusion: Parameter-free RGB Camera-Radar Sensor Fusion Algorithm in Complex Maritime Situations,"Shin, Yeongha; Kim, Hanguen; Kim, Jinwhan",,
Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition,"Liu, Zihao; Liu, Xing; Liu, Zhengxiong; Zhang, Yizhai; Huang, Panfeng",https://arxiv.org/abs/2311.11287,"Robotic manipulation holds the potential to replace humans in the execution of tedious or dangerous tasks. However, control-based approaches are not suitable due to the difficulty of formally describing open-world manipulation in reality, and the inefficiency of existing learning methods. Thus, applying manipulation in a wide range of scenarios presents significant challenges. In this study, we propose a novel method for skill learning in robotic manipulation called Tactile Active Inference Reinforcement Learning (Tactile-AIRL), aimed at achieving efficient training. To enhance the performance of reinforcement learning (RL), we introduce active inference, which integrates model-based techniques and intrinsic curiosity into the RL process. This integration improves the algorithm's training efficiency and adaptability to sparse rewards. Additionally, we utilize a vision-based tactile sensor to provide detailed perception for manipulation tasks. Finally, we employ a model-based approach to imagine and plan appropriate actions through free energy minimization. Simulation results demonstrate that our method achieves significantly high training efficiency in non-prehensile objects pushing tasks. It enables agents to excel in both dense and sparse reward tasks with just a few interaction episodes, surpassing the SAC baseline. Furthermore, we conduct physical experiments on a gripper screwing task using our method, which showcases the algorithm's rapid learning capability and its potential for practical applications."
Implicit Neural Fusion of RGB and Far-Infrared 3D Imagery for Invisible Scenes,"Li, Xiangjie; Xie, Shuxiang; Sakurada, Ken; Sagawa, Ryusuke; Oishi, Takeshi",,
3DR-DIFF: Blind Diffusion Inpainting for 3D Point Cloud Reconstruction and Segmentation,"Kariyawasam Thanthrige, Yasas Mahima; Perera, Asanka; Anavatti, Sreenatha; Garratt, Matthew",,
Explosive Legged Robotic Hopping: Energy Accumulation and Power Amplification via Pneumatic Augmentation,"Chen, Yifei; Arturo, Gamboa-Gonzalez; Wehner, Michael; Xiong, Xiaobin",https://arxiv.org/abs/2312.05773,"We present a novel pneumatic augmentation to traditional electric motor-actuated legged robot to increase intermittent power density to perform infrequent explosive hopping behaviors. The pneumatic system is composed of a pneumatic pump, a tank, and a pneumatic actuator. The tank is charged up by the pump during regular hopping motion that is created by the electric motors. At any time after reaching a desired air pressure in the tank, a solenoid valve is utilized to rapidly release the air pressure to the pneumatic actuator (piston) which is used in conjunction with the electric motors to perform explosive hopping, increasing maximum hopping height for one or subsequent cycles. We show that, on a custom-designed one-legged hopping robot, without any additional power source and with this novel pneumatic augmentation system, their associated system identification and optimal control, the robot is able to realize highly explosive hopping with power amplification per cycle by a factor of approximately 5.4 times the power of electric motor actuation alone."
Audio-Visual Traffic Light State Detection for Urban Robots,"Gupta, Sagar; Cosgun, Akansel",https://arxiv.org/abs/2404.19281,"We present a multimodal traffic light state detection using vision and sound, from the viewpoint of a quadruped robot navigating in urban settings. This is a challenging problem because of the visual occlusions and noise from robot locomotion. Our method combines features from raw audio with the ratios of red and green pixels within bounding boxes, identified by established vision-based detectors. The fusion method aggregates features across multiple frames in a given timeframe, increasing robustness and adaptability. Results show that our approach effectively addresses the challenge of visual occlusion and surpasses the performance of single-modality solutions when the robot is in motion. This study serves as a proof of concept, highlighting the significant, yet often overlooked, potential of multi-modal perception in robotics."
Bidirectional Partial-to-Full Non-Rigid Point Set Registration with Non-Overlapping Filtering,"Yu, Hao; Min, Zhe; Liu, Mingyang; Song, Rui; Li, Yibin; Meng, Max Q.-H.",,
Soft Task Planning with Hierarchical Temporal Logic Specifications,"Chen, Ziyang; Zhou, Zhangli; Li, Lin; Kan, Zhen",,
Exploiting Hybrid Policy in Reinforcement Learning for Interpretable Temporal Logic Manipulation,"Zhang, Hao; Wang, Hao; Huang, Xiucai; Chen, Wenrui; Kan, Zhen",,
"GELLO: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators","Wu, Shiyao; Shentu, Yide; Yi, Zhongke; Lin, Xingyu; Abbeel, Pieter",https://arxiv.org/abs/2309.13037,"Humans can teleoperate robots to accomplish complex manipulation tasks. Imitation learning has emerged as a powerful framework that leverages human teleoperated demonstrations to teach robots new skills. However, the performance of the learned policies is bottlenecked by the quality, scale, and variety of the demonstration data. In this paper, we aim to lower the barrier to collecting large and high-quality human demonstration data by proposing a GEneraL framework for building LOw-cost and intuitive teleoperation systems for robotic manipulation (GELLO). Given a target robot arm, we build a GELLO controller device that has the same kinematic structure as the target arm, leveraging 3D-printed parts and economical off-the-shelf motors. GELLO is easy to build and intuitive to use. Through an extensive user study, we show that GELLO enables more reliable and efficient demonstration collection compared to other cost efficient teleoperation devices commonly used in the imitation learning literature such as virtual reality controllers and 3D spacemouses. We further demonstrate the capabilities of GELLO for performing complex bi-manual and contact-rich manipulation tasks. To make GELLO accessible to everyone, we have designed and built GELLO systems for 3 commonly used robotic arms: Franka, UR5, and xArm. All software and hardware are open-sourced and can be found on our website: https://wuphilipp.github.io/gello/."
SR-LIO: LiDAR-Inertial Odometry with Sweep Reconstruction,"Yuan, Zikang; Lang, Fengtian; Xu, Tianle; Yang, Xin",https://arxiv.org/abs/2210.10424,"This paper proposes a novel LiDAR-Inertial odometry (LIO), named SR-LIO, based on an iterated extended Kalman filter (iEKF) framework. We adapt the sweep reconstruction method, which segments and reconstructs raw input sweeps from spinning LiDAR to obtain reconstructed sweeps with higher frequency. We found that such method can effectively reduce the time interval for each iterated state update, improving the state estimation accuracy and enabling the usage of iEKF framework for fusing high-frequency IMU and low-frequency LiDAR. To prevent inaccurate trajectory caused by multiple distortion correction to a particular point, we further propose to perform distortion correction for each segment. Experimental results on four public datasets demonstrate that our SR-LIO outperforms all existing state-of-the-art methods on accuracy, and reducing the time interval of iterated state update via the proposed sweep reconstruction can improve the accuracy and frequency of estimated states. The source code of SR-LIO is publicly available for the development of the community."
RAM-NAS: Resource-aware Multiobjective Neural Architecture Search Method for Robot Vision Tasks,"Mao, Shouren; Dong, Wei; Qin, MingHao; Liu, Huajian; Gao, Yongzhuo",,
Cross-Modal Self-Supervised Learning with Effective Contrastive Units for LiDAR Point Clouds,"Cai, Mu; Luo, Chenxu; Lee, Yong Jae; Yang, Xiaodong",https://arxiv.org/abs/2409.06827,"3D perception in LiDAR point clouds is crucial for a self-driving vehicle to properly act in 3D environment. However, manually labeling point clouds is hard and costly. There has been a growing interest in self-supervised pre-training of 3D perception models. Following the success of contrastive learning in images, current methods mostly conduct contrastive pre-training on point clouds only. Yet an autonomous driving vehicle is typically supplied with multiple sensors including cameras and LiDAR. In this context, we systematically study single modality, cross-modality, and multi-modality for contrastive learning of point clouds, and show that cross-modality wins over other alternatives. In addition, considering the huge difference between the training sources in 2D images and 3D point clouds, it remains unclear how to design more effective contrastive units for LiDAR. We therefore propose the instance-aware and similarity-balanced contrastive units that are tailored for self-driving point clouds. Extensive experiments reveal that our approach achieves remarkable performance gains over various point cloud models across the downstream perception tasks of LiDAR based 3D object detection and 3D semantic segmentation on the four popular benchmarks including Waymo Open Dataset, nuScenes, SemanticKITTI and ONCE."
A Direct Algorithm for Multi-Gyroscope Infield Calibration,"Wang, Tianheng; Roumeliotis, Stergios",https://arxiv.org/abs/2403.08177,"In this paper, we address the problem of estimating the rotational extrinsics, as well as the scale factors of two gyroscopes rigidly mounted on the same device. In particular, we formulate the problem as a least-squares minimization and introduce a direct algorithm that computes the estimated quantities without any iterations, hence avoiding local minima and improving efficiency. Furthermore, we show that the rotational extrinsics are observable while the scale factors can be determined up to global scale for general configurations of the gyroscopes. To this end, we also study special placements of the gyroscopes where a pair, or all, of their axes are parallel and analyze their impact on the scale factors' observability. Lastly, we evaluate our algorithm in simulations and real-world experiments to assess its performance as a function of key motion and sensor characteristics."
Unsupervised Multiple Proactive Behavior Learning of Mobile Robots for Smooth and Safe Navigation,"Srisuchinnawong, Arthicha; Baech, Jonas; Hyzy, Marek Piotr; Kounalakis, Tsampikos; Boukas, Evangelos; Manoonpong, Poramate",,
Advanced Handheld Micro-Surgical System using Hall Sensor and Magnet Trocar for Improved Precision in Retinal Surgery,"Lee, Myung Ho; Im, Jintaek; Prieto Prada, John David; Song, Cheol",,
Channel-wise Motion Features for Efficient Motion Segmentation,"Inoue, Riku; Tsuchiya, Masamitsu; Yasui, Yuji",,
Learning-based Adaptive Admittance Controller for Efficient and Safe pHRI in Contact-Rich Manufacturing Tasks,"Pourakbarian Niaz, Pouya; Erzin, Engin; Basdogan, Cagatay",https://arxiv.org/abs/2407.14161,"This paper proposes an adaptive admittance controller for improving efficiency and safety in physical human-robot interaction (pHRI) tasks in small-batch manufacturing that involve contact with stiff environments, such as drilling, polishing, cutting, etc. We aim to minimize human effort and task completion time while maximizing precision and stability during the contact of the machine tool attached to the robot's end-effector with the workpiece. To this end, a two-layered learning-based human intention recognition mechanism is proposed, utilizing only the kinematic and kinetic data from the robot and two force sensors. A ``subtask detector"" recognizes the human intent by estimating which phase of the task is being performed, e.g., \textit{Idle}, \textit{Tool-Attachment}, \textit{Driving}, and \textit{Contact}. Simultaneously, a ``motion estimator"" continuously quantifies intent more precisely during the \textit{Driving} to predict when \textit{Contact} will begin. The controller is adapted online according to the subtask while allowing early adaptation before the \textit{Contact} to maximize precision and safety and prevent potential instabilities. Three sets of pHRI experiments were performed with multiple subjects under various conditions. Spring compression experiments were performed in virtual environments to train the data-driven models and validate the proposed adaptive system, and drilling experiments were performed in the physical world to test the proposed methods' efficacy in real-life scenarios. Experimental results show subtask classification accuracy of 84\% and motion estimation R\textsuperscript{2} score of 0.96. Furthermore, 57\% lower human effort was achieved during \textit{Driving} as well as 53\% lower oscillation amplitude at \textit{Contact} as a result of the proposed system."
An Optical Interferometer-based Force Sensor System for Enhancing Precision in Epidural Injection Procedure,"Cho, Gichan; Im, Jintaek; Kwon, Hyun-Jung; Song, Cheol",,
Task-Oriented Design Method for Monolithic Flexible Hands with Wire Drive Systems,"Kusuhara, Rina; Higashimori, Mitsuru",,
LiOn-XA: Unsupervised Domain Adaptation via LiDAR-Only Cross-Modal Adversarial Training,"Kreutz, Thomas; Lemke, Jens; Mühlhäuser, Max; Sanchez Guinea, Alejandro",,
Knowledge-based Programming by Demonstration using semantic action models for industrial assembly,"Ding, Junsheng; Zhang, Haifan; Li, Weihang; Zhou, Liangwei; Perzylo, Alexander Clifford",,
Leveraging GNSS and Onboard Visual Data from Consumer Vehicles for Robust Road Network Estimation,"Opra, István Balázs; Le Dem, Betty; Walls, Jeffrey; Lukarski, Dimitar; Stachniss, Cyrill",https://arxiv.org/abs/2408.01640,"Maps are essential for diverse applications, such as vehicle navigation and autonomous robotics. Both require spatial models for effective route planning and localization. This paper addresses the challenge of road graph construction for autonomous vehicles. Despite recent advances, creating a road graph remains labor-intensive and has yet to achieve full automation. The goal of this paper is to generate such graphs automatically and accurately. Modern cars are equipped with onboard sensors used for today's advanced driver assistance systems like lane keeping. We propose using global navigation satellite system (GNSS) traces and basic image data acquired from these standard sensors in consumer vehicles to estimate road-level maps with minimal effort. We exploit the spatial information in the data by framing the problem as a road centerline semantic segmentation task using a convolutional neural network. We also utilize the data's time series nature to refine the neural network's output by using map matching. We implemented and evaluated our method using a fleet of real consumer vehicles, only using the deployed onboard sensors. Our evaluation demonstrates that our approach not only matches existing methods on simpler road configurations but also significantly outperforms them on more complex road geometries and topologies. This work received the 2023 Woven by Toyota Invention Award."
RTTF: Rapid Tactile Transfer Framework for Contact-Rich Manipulation Tasks,"Wu, Qiwei; Peng, Xuanbin; Zhou, Jiayu; Sun, Zhuoran; Xiong, Xiaogang; Lou, Yunjiang",,
RobotGraffiti: An AR tool for semi-automated construction of workcell models to optimize robot deployment,"Zieli&#324;ski, Krzysztof; Penning, Ryan; Blumberg, Bruce; Schlette, Christian; Mikkel, Kjærgaard",,
"A low-cost, high-speed, and robust bin picking system for factory automation enabled by a none-stop, multiple-view, and active vision scheme","Fu, Xingdou; Miao, Lin; Ohnishi, Yasuhiro; Hasegawa, Yuki; Suwa, Masaki",,
SmartKit&#65306; User-Friendly Robot with Multiple Operating Systems,"Chen, Guanyu; Zhou, Yiqun; Yang, Guoqing; Lv, Pan; Li, Hong",,
Local Path Planning among Pushable Objects Based on Reinforcement Learning,"Yao, Linghong; Modugno, Valerio; Delfaki, Andromachi Maria; Liu, Yuanchang; Stoyanov, Danail; Kanoulas, Dimitrios",https://arxiv.org/abs/2303.02407,"In this paper, we introduce a method to deal with the problem of robot local path planning among pushable objects -- an open problem in robotics. In particular, we achieve that by training multiple agents simultaneously in a physics-based simulation environment, utilizing an Advantage Actor-Critic algorithm coupled with a deep neural network. The developed online policy enables these agents to push obstacles in ways that are not limited to axial alignments, adapt to unforeseen changes in obstacle dynamics instantaneously, and effectively tackle local path planning in confined areas. We tested the method in various simulated environments to prove the adaptation effectiveness to various unseen scenarios in unfamiliar settings. Moreover, we have successfully applied this policy on an actual quadruped robot, confirming its capability to handle the unpredictability and noise associated with real-world sensors and the inherent uncertainties present in unexplored object pushing tasks."
Towards Cross-View-Consistent Self-Supervised Surround Depth Estimation,"Ding, Laiyan; Jiang, Hualie; Li, Jie; CHEN, Yongquan; Huang, Rui",https://arxiv.org/abs/2407.04041,"Depth estimation is a cornerstone for autonomous driving, yet acquiring per-pixel depth ground truth for supervised learning is challenging. Self-Supervised Surround Depth Estimation (SSSDE) from consecutive images offers an economical alternative. While previous SSSDE methods have proposed different mechanisms to fuse information across images, few of them explicitly consider the cross-view constraints, leading to inferior performance, particularly in overlapping regions. This paper proposes an efficient and consistent pose estimation design and two loss functions to enhance cross-view consistency for SSSDE. For pose estimation, we propose to use only front-view images to reduce training memory and sustain pose estimation consistency. The first loss function is the dense depth consistency loss, which penalizes the difference between predicted depths in overlapping regions. The second one is the multi-view reconstruction consistency loss, which aims to maintain consistency between reconstruction from spatial and spatial-temporal contexts. Additionally, we introduce a novel flipping augmentation to improve the performance further. Our techniques enable a simple neural model to achieve state-of-the-art performance on the DDAD and nuScenes datasets. Last but not least, our proposed techniques can be easily applied to other methods. The code will be made public."
Robust Online Epistemic Replanning of Multi-Robot Missions,"Bramblett, Lauren; Miloradovic, Branko; Sherman, Patrick; Papadopoulos, Alessandro Vittorio; Bezzo, Nicola",https://arxiv.org/abs/2403.00641,"As Multi-Robot Systems (MRS) become more affordable and computing capabilities grow, they provide significant advantages for complex applications such as environmental monitoring, underwater inspections, or space exploration. However, accounting for potential communication loss or the unavailability of communication infrastructures in these application domains remains an open problem. Much of the applicable MRS research assumes that the system can sustain communication through proximity regulations and formation control or by devising a framework for separating and adhering to a predetermined plan for extended periods of disconnection. The latter technique enables an MRS to be more efficient, but breakdowns and environmental uncertainties can have a domino effect throughout the system, particularly when the mission goal is intricate or time-sensitive. To deal with this problem, our proposed framework has two main phases: i) a centralized planner to allocate mission tasks by rewarding intermittent rendezvous between robots to mitigate the effects of the unforeseen events during mission execution, and ii) a decentralized replanning scheme leveraging epistemic planning to formalize belief propagation and a Monte Carlo tree search for policy optimization given distributed rational belief updates. The proposed framework outperforms a baseline heuristic and is validated using simulations and experiments with aerial vehicles."
Learning-informed Long-Horizon Navigation under Uncertainty for Vehicles with Dynamics,"Khanal, Abhish; Bui, Hoang-Dung; Plaku, Erion; Stein, Gregory",,
Sensor-agnostic Visuo-Tactile Robot Calibration Exploiting Assembly-Precision Model Geometries,"Gomes, Manuel; Görner, Michael; Oliveira, Miguel; Zhang, Jianwei",,
Proprioception Is All You Need: Terrain Classification for Boreal Forests,"LaRocque, Damien; Guimont-Martin, William; Duclos, David-Alexandre; Giguère, Philippe; Pomerleau, Francois",https://arxiv.org/abs/2403.16877,"Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC task. Interestingly, we show that while CNN outperforms Mamba on each separate dataset, Mamba achieves greater accuracy when trained on a combination of both. In addition, we demonstrate that Mamba's learning capacity is greater than a CNN for increasing amounts of data. We show that the combination of two TC datasets yields a latent space that can be interpreted with the properties of the terrains. We also discuss the implications of merging datasets on classification. Our source code and dataset are publicly available online: https://github.com/norlab-ulaval/BorealTC."
A Convex Formulation of Frictional Contact for the Material Point Method and Rigid Bodies,"Zong, Zeshun; Han, Xuchen; Jiang, Chenfanfu",https://arxiv.org/abs/2403.13783,"In this paper, we introduce a novel convex formulation that seamlessly integrates the Material Point Method (MPM) with articulated rigid body dynamics in frictional contact scenarios. We extend the linear corotational hyperelastic model into the realm of elastoplasticity and include an efficient return mapping algorithm. This approach is particularly effective for MPM simulations involving significant deformation and topology changes, while preserving the convexity of the optimization problem. Our method ensures global convergence, enabling the use of large simulation time steps without compromising robustness. We have validated our approach through rigorous testing and performance evaluations, highlighting its superior capabilities in managing complex simulations relevant to robotics. Compared to previous MPM based robotic simulators, our method significantly improves the stability of contact resolution -- a critical factor in robot manipulation tasks. We make our method available in the open-source robotics toolkit, Drake."
DiaGBT: An Explainable and Evolvable Robot Control Framework using Dialogue Generative Behavior Trees,"Liang, Jinde; Chang, Yuan; Wang, Yanzhen; Yi, Xiaodong; Wang, Qian",,
Skin the sheep not only once: Reusing Various Depth Datasets to Drive the Learning of Optical Flow,"Huang, Sheng Chi; Chiu, Wei-Chen",https://arxiv.org/abs/2310.01833,"Optical flow estimation is crucial for various applications in vision and robotics. As the difficulty of collecting ground truth optical flow in real-world scenarios, most of the existing methods of learning optical flow still adopt synthetic dataset for supervised training or utilize photometric consistency across temporally adjacent video frames to drive the unsupervised learning, where the former typically has issues of generalizability while the latter usually performs worse than the supervised ones. To tackle such challenges, we propose to leverage the geometric connection between optical flow estimation and stereo matching (based on the similarity upon finding pixel correspondences across images) to unify various real-world depth estimation datasets for generating supervised training data upon optical flow. Specifically, we turn the monocular depth datasets into stereo ones via synthesizing virtual disparity, thus leading to the flows along the horizontal direction; moreover, we introduce virtual camera motion into stereo data to produce additional flows along the vertical direction. Furthermore, we propose applying geometric augmentations on one image of an optical flow pair, encouraging the optical flow estimator to learn from more challenging cases. Lastly, as the optical flow maps under different geometric augmentations actually exhibit distinct characteristics, an auxiliary classifier which trains to identify the type of augmentation from the appearance of the flow map is utilized to further enhance the learning of the optical flow estimator. Our proposed method is general and is not tied to any particular flow estimator, where extensive experiments based on various datasets and optical flow estimation models verify its efficacy and superiority."
CollabLoc: Collaborative Information Sharing for Real-Time Multiuser Visual Localization System,"Yu, Teng-Te; Lau, Yo-Chung; Wang, Kai-Li; Chen, Kuan-Wen",,
LEEPS: Learning End-to-End Legged Perceptive Parkour Skills on Challenging Terrains,"Qian, Tangyu; Zhang, Hao; Zhou, Zhangli; Wang, Hao; Mingyu, Cai; Kan, Zhen",,
Robotic Object Insertion with a Soft Wrist through Sim-to-Real Privileged Training,"Fuchioka, Yuni; Beltran-Hernandez, Cristian Camilo; Hai, Nguyen; Hamaya, Masashi",https://arxiv.org/abs/2408.17061,"This study addresses contact-rich object insertion tasks under unstructured environments using a robot with a soft wrist, enabling safe contact interactions. For the unstructured environments, we assume that there are uncertainties in object grasp and hole pose and that the soft wrist pose cannot be directly measured. Recent methods employ learning approaches and force/torque sensors for contact localization; however, they require data collection in the real world. This study proposes a sim-to-real approach using a privileged training strategy. This method has two steps. 1) The teacher policy is trained to complete the task with sensor inputs and ground truth privileged information such as the peg pose, and then 2) the student encoder is trained with data produced from teacher policy rollouts to estimate the privileged information from sensor history. We performed sim-to-real experiments under grasp and hole pose uncertainties. This resulted in 100\%, 95\%, and 80\% success rates for circular peg insertion with 0, +5, and -5 degree peg misalignments, respectively, and start positions randomly shifted $\pm$ 10 mm from a default position. Also, we tested the proposed method with a square peg that was never seen during training. Additional simulation evaluations revealed that using the privileged strategy improved success rates compared to training with only simulated sensor data. Our results demonstrate the advantage of using sim-to-real privileged training for soft robots, which has the potential to alleviate human engineering efforts for robotic assembly."
Transcrib3D: 3D Referring Expression Resolution through Large Language Models,"Fang, Jiading; Tan, Xiangshan; Lin, Shengjie; Vasiljevic, Igor; Guizilini, Vitor; Mei, Hongyuan; Ambrus, Rares; Shakhnarovich, Gregory; Walter, Matthew",https://arxiv.org/abs/2404.19221,"If robots are to work effectively alongside people, they must be able to interpret natural language references to objects in their 3D environment. Understanding 3D referring expressions is challenging -- it requires the ability to both parse the 3D structure of the scene and correctly ground free-form language in the presence of distraction and clutter. We introduce Transcrib3D, an approach that brings together 3D detection methods and the emergent reasoning capabilities of large language models (LLMs). Transcrib3D uses text as the unifying medium, which allows us to sidestep the need to learn shared representations connecting multi-modal inputs, which would require massive amounts of annotated 3D data. As a demonstration of its effectiveness, Transcrib3D achieves state-of-the-art results on 3D reference resolution benchmarks, with a great leap in performance from previous multi-modality baselines. To improve upon zero-shot performance and facilitate local deployment on edge computers and robots, we propose self-correction for fine-tuning that trains smaller models, resulting in performance close to that of large models. We show that our method enables a real robot to perform pick-and-place tasks given queries that contain challenging referring expressions. Project site is at https://ripl.github.io/Transcrib3D."
DeRO: Dead Reckoning Based on Radar Odometry With Accelerometers Aided for Robot Localization,"Do, Hoang Viet; Kim, Yong Hun; Lee, Joo Han; Lee, Min Ho; Song, Jin Woo",https://arxiv.org/abs/2403.05136,"In this paper, we propose a radar odometry structure that directly utilizes radar velocity measurements for dead reckoning while maintaining its ability to update estimations within the Kalman filter framework. Specifically, we employ the Doppler velocity obtained by a 4D Frequency Modulated Continuous Wave (FMCW) radar in conjunction with gyroscope data to calculate poses. This approach helps mitigate high drift resulting from accelerometer biases and double integration. Instead, tilt angles measured by gravitational force are utilized alongside relative distance measurements from radar scan matching for the filter's measurement update. Additionally, to further enhance the system's accuracy, we estimate and compensate for the radar velocity scale factor. The performance of the proposed method is verified through five real-world open-source datasets. The results demonstrate that our approach reduces position error by 62% and rotation error by 66% on average compared to the state-of-the-art radar-inertial fusion method in terms of absolute trajectory error."
SDGE:Stereo Guided Depth Estimation for 360°Camera Sets,"Xu, Jialei; Yin, Wei; Gong, Dong; Jiang, Junjun; Liu, Xianming",https://arxiv.org/abs/2402.11791,"Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360$^\circ$ perception. These 360$^\circ$ camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360$^\circ$ cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing downstream autonomous driving technologies, such as 3D object detection and occupancy prediction."
Reward-Driven Automated Curriculum Learning for Interaction-Aware Self-Driving at Unsignalized Intersections,"Peng, Zengqi; Zhou, Xiao; Zheng, Lei; Wang, Yubin; Ma, Jun",https://arxiv.org/abs/2403.13674,"In this work, we present a reward-driven automated curriculum reinforcement learning approach for interaction-aware self-driving at unsignalized intersections, taking into account the uncertainties associated with surrounding vehicles (SVs). These uncertainties encompass the uncertainty of SVs' driving intention and also the quantity of SVs. To deal with this problem, the curriculum set is specifically designed to accommodate a progressively increasing number of SVs. By implementing an automated curriculum selection mechanism, the importance weights are rationally allocated across various curricula, thereby facilitating improved sample efficiency and training outcomes. Furthermore, the reward function is meticulously designed to guide the agent towards effective policy exploration. Thus the proposed framework could proactively address the above uncertainties at unsignalized intersections by employing the automated curriculum learning technique that progressively increases task difficulty, and this ensures safe self-driving through effective interaction with SVs. Comparative experiments are conducted in $Highway\_Env$, and the results indicate that our approach achieves the highest task success rate, attains strong robustness to initialization parameters of the curriculum selection module, and exhibits superior adaptability to diverse situational configurations at unsignalized intersections. Furthermore, the effectiveness of the proposed method is validated using the high-fidelity CARLA simulator."
Tunable Stiffness Glove for Tremor Suppression Based on 3D Printed Structured Fabrics,"Chen, Yu; Li, Junwei; Yang, Xudong; Wang, Yifan",,
FedRC: A Rapid-Converged Hierarchical Federated Learning Framework in Street Scene Semantic Understanding,"Kou, Wei-Bin; Lin, Qingfeng; Tang, Ming; Wang, Shuai; Zhu, Guangxu; Wu, Yik-Chung",https://arxiv.org/abs/2407.01103,"Street Scene Semantic Understanding (denoted as TriSU) is a crucial but complex task for world-wide distributed autonomous driving (AD) vehicles (e.g., Tesla). Its inference model faces poor generalization issue due to inter-city domain-shift. Hierarchical Federated Learning (HFL) offers a potential solution for improving TriSU model generalization, but suffers from slow convergence rate because of vehicles' surrounding heterogeneity across cities. Going beyond existing HFL works that have deficient capabilities in complex tasks, we propose a rapid-converged heterogeneous HFL framework (FedRC) to address the inter-city data heterogeneity and accelerate HFL model convergence rate. In our proposed FedRC framework, both single RGB image and RGB dataset are modelled as Gaussian distributions in HFL aggregation weight design. This approach not only differentiates each RGB sample instead of typically equalizing them, but also considers both data volume and statistical properties rather than simply taking data quantity into consideration. Extensive experiments on the TriSU task using across-city datasets demonstrate that FedRC converges faster than the state-of-the-art benchmark by 38.7%, 37.5%, 35.5%, and 40.6% in terms of mIoU, mPrecision, mRecall, and mF1, respectively. Furthermore, qualitative evaluations in the CARLA simulation environment confirm that the proposed FedRC framework delivers top-tier performance."
Photometric Consistency for Precise Drone Rephotography,"Chang, Hsuan-Jui; Huang, Tzu-Chun; Xu, Hao-Liang; Chen, Kuan-Wen",,
Uncertainty-Aware Semi-Supervised Semantic Key Point Detection via Bundle Adjustment,"Li, Kai; Zhang, Yin; Zhao, Shiyu",,
Efficient Incremental Penetration Depth Estimation between Convex Geometries,"Gao, Wei",https://arxiv.org/abs/2304.07357,"Penetration depth (PD) is essential for robotics due to its extensive applications in dynamic simulation, motion planning, haptic rendering, etc. The Expanding Polytope Algorithm (EPA) is the de facto standard for this problem, which estimates PD by expanding an inner polyhedral approximation of an implicit set. In this paper, we propose a novel optimization-based algorithm that incrementally estimates minimum penetration depth and its direction. One major advantage of our method is that it can be warm-started by exploiting the spatial and temporal coherence, which emerges naturally in many robotic applications (e.g., the temporal coherence between adjacent simulation time knots). As a result, our algorithm achieves substantial speedup -- we demonstrate it is 5-30x faster than EPA on several benchmarks. Moreover, our approach is built upon the same implicit geometry representation as EPA, which enables easy integration and deployment into existing software stacks. We also provide an open-source implementation on: https://github.com/weigao95/mind-fcl"
SMORE-SLAM: Semantic Monocular SLAM with Scale Correction and Reverse Loop Utilization in Outdoor Environments,"Chen, Yushi; Zhao, Fang; Zhuge, Yue; liu, junxiong; Yan, Jiaquan; Luo, Haiyong",,
PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds,"Liu, Haotian; Qu, Sanqing; LU, FAN; Bu, Zongtao; Roehrbein, Florian; Knoll, Alois; Chen, Guang",https://arxiv.org/abs/2402.18925,"Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels. For example, event data is likely to complement contours of scene objects. In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth). Concretely, PCDepth comprises two primary components: a complementary visual representation learning module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance. Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios. Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth. Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios."
Neighborhood Consensus Guided Matching Based Place Recognition with Spatial-Channel Embedding,"Li, Kunmo; Zhang, Yunzhou; Ning, Jian; Zhao, Xinge; Wang, Guiyuan; Liu, Wei",,
Investigating Behavioral and Cognitive Changes Induced by Autonomous Delivery Robots in Incidentally Copresent Persons,"KIM, NAYOUNG; Kwak, Sonya Sona",,
Deformable Objects Perception is Just a Few Clicks Away  Dense Annotations from Sparse Inputs,"Caporali, Alessio; Galassi, Kevin; Pantano, Matteo; Palli, Gianluca",,
Backpropagation-Based Analytical Derivatives of EKF Covariance for Active Sensing,"Benhamou, Jonas; Bonnabel, Silvere; Chapdelaine, Camille",https://arxiv.org/abs/2402.17569,"To enhance accuracy of robot state estimation, active sensing (or perception-aware) methods seek trajectories that maximize the information gathered by the sensors. To this aim, one possibility is to seek trajectories that minimize the (estimation error) covariance matrix output by an extended Kalman filter (EKF), w.r.t. its control inputs over a given horizon. However, this is computationally demanding. In this article, we derive novel backpropagation analytical formulas for the derivatives of the covariance matrices of an EKF w.r.t. all its inputs. We then leverage the obtained analytical gradients as an enabling technology to derive perception-aware optimal motion plans. Simulations validate the approach, showcasing improvements in execution time, notably over PyTorch's automatic differentiation. Experimental results on a real vehicle also support the method."
Sharing Attention Mechanism in V-SLAM: Relative Pose Estimation with Messenger Tokens on Small Datasets,"Dai, Dun; Quan, Quan; Cai, Kai-Yuan",,
Reconfigurable multi-rotor for high-precision physical interaction,"Taylor, Joshua; Nursultan, Imanberdiyev; Chuah, Meng Yee (Michael); Yau, Wei-Yun; Sartoretti, Guillaume Adrien; CAMCI, Efe",,
An Octopus-inspired-configuration Sensor Array Concept toward Torso-oriented Magnetic Localization Task and Simulation Verification,"SUN, Yichong; Chan, Wai Shing; Li, Yehui; Zhang, Heng; Huang, Yisen; Hu, Haochen; Chiu, Philip, Wai-yan; Li, Zheng",,
QuerySOD: A Small Object Detection Algorithm Based on Sparse Convolutional Network and Query Mechanism,"Cao, Zhengcai; Li, Junnian; Niu, Jie; Zhou, MengChu",,
Sim-to-Real Domain Shift in Online Action Detection,"Patsch, Constantin; Torjmene, Wael; Zakour, Marsil; Wu, Yuankai; Salihu, Driton; Steinbach, Eckehard",,
Dynamic-Range Focal Sweep: Seamless Continuous Autofocus Based on High-Speed Vision for Magnified Object Tracking,"ZHANG, Tianyi; Shimasaki, Kohei; Ishii, Idaku; Namiki, Akio",,
Task-Driven Computational Framework for Simultaneously Optimizing Design and Mounted Pose of Modular Reconfigurable Manipulators,"Lei, Maolin; Romiti, Edoardo; Laurenzi, Arturo; Tsagarakis, Nikos",https://arxiv.org/abs/2405.01923,"Modular reconfigurable manipulators enable quick adaptation and versatility to address different application environments and tailor to the specific requirements of the tasks. Task performance significantly depends on the manipulator's mounted pose and morphology design, therefore posing the need of methodologies for selecting suitable modular robot configurations and mounted pose that can address the specific task requirements and required performance. Morphological changes in modular robots can be derived through a discrete optimization process involving the selective addition or removal of modules. In contrast, the adjustment of the mounted pose operates within a continuous space, allowing for smooth and precise alterations in both orientation and position. This work introduces a computational framework that simultaneously optimizes modular manipulators' mounted pose and morphology. The core of the work is that we design a mapping function that \textit{implicitly} captures the morphological state of manipulators in the continuous space. This transformation function unifies the optimization of mounted pose and morphology within a continuous space. Furthermore, our optimization framework incorporates a array of performance metrics, such as minimum joint effort and maximum manipulability, and considerations for trajectory execution error and physical and safety constraints. To highlight our method's benefits, we compare it with previous methods that framed such problem as a combinatorial optimization problem and demonstrate its practicality in selecting the modular robot configuration for executing a drilling task with the CONCERT modular robotic platform."
Towards Dynamic and Small Objects Refinement for Unsupervised Domain Adaptative Nighttime Semantic Segmentation,"Pan, Jingyi; Li, Sihang; Chen, Yucheng; Zhu, Jinjing; Wang, Lin",https://arxiv.org/abs/2310.04747,"Nighttime semantic segmentation plays a crucial role in practical applications, such as autonomous driving, where it frequently encounters difficulties caused by inadequate illumination conditions and the absence of well-annotated datasets. Moreover, semantic segmentation models trained on daytime datasets often face difficulties in generalizing effectively to nighttime conditions. Unsupervised domain adaptation (UDA) has shown the potential to address the challenges and achieved remarkable results for nighttime semantic segmentation. However, existing methods still face limitations in 1) their reliance on style transfer or relighting models, which struggle to generalize to complex nighttime environments, and 2) their ignorance of dynamic and small objects like vehicles and poles, which are difficult to be directly learned from other domains. This paper proposes a novel UDA method that refines both label and feature levels for dynamic and small objects for nighttime semantic segmentation. First, we propose a dynamic and small object refinement module to complement the knowledge of dynamic and small objects from the source domain to target the nighttime domain. These dynamic and small objects are normally context-inconsistent in under-exposed conditions. Then, we design a feature prototype alignment module to reduce the domain gap by deploying contrastive learning between features and prototypes of the same class from different domains, while re-weighting the categories of dynamic and small objects. Extensive experiments on three benchmark datasets demonstrate that our method outperforms prior arts by a large margin for nighttime segmentation. Project page: https://rorisis.github.io/DSRNSS/."
Static Modeling of the Stiffness and Contact Forces of Rolling Element Eccentric Drives for Use in Robotic Drive Systems,"Fritsch, Simon; Landler, Stefan; Otto, Michael; Vogel-Heuser, Birgit; Zimmermann, Markus; Stahl, Karsten",,
BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs,"Izzo, Riccardo Andrea; Bardaro, Gianluca; Matteucci, Matteo",https://arxiv.org/abs/2403.12761,"This paper presents a novel approach to generating behavior trees for robots using lightweight large language models (LLMs) with a maximum of 7 billion parameters. The study demonstrates that it is possible to achieve satisfying results with compact LLMs when fine-tuned on a specific dataset. The key contributions of this research include the creation of a fine-tuning dataset based on existing behavior trees using GPT-3.5 and a comprehensive comparison of multiple LLMs (namely llama2, llama-chat, and code-llama) across nine distinct tasks. To be thorough, we evaluated the generated behavior trees using static syntactical analysis, a validation system, a simulated environment, and a real robot. Furthermore, this work opens the possibility of deploying such solutions directly on the robot, enhancing its practical applicability. Findings from this study demonstrate the potential of LLMs with a limited number of parameters in generating effective and efficient robot behaviors."
SPVSoAP3D: A Second-order Average Pooling Approach to enhance 3D Place Recognition in Horticultural Environments,"Barros, Tiago; Premebida, Cristiano; Aravecchia, Stephanie; Pradalier, Cedric; Nunes, Urbano J.",,
Research on Autonomous Navigation of Dual-mode Wheel-legged Robot,"Wang, Wen; Xu, Xiaobin; Chen, Ziheng; Yang, Jian; Ran, Yingying; Tan, Zhiying; Luo, Minzhou",,
"One Problem, One Solution: Unifying Robot Design and Cell Layout Optimization","Baumgärtner, Jan; Puchta, Alexander; Fleischer, Jürgen",,
Precise Well-plate Placing Utilizing Contact During Sliding with Tactile-based Pose Estimation for Laboratory Automation,"Pai, Sameer; Takahashi, Kuniyuki; Masuda, Shimpei; Fukaya, Naoki; Yamane, Koki; Ummadisingu, Avinash",https://arxiv.org/abs/2309.16170,"Micro well-plates are an apparatus commonly used in chemical and biological experiments that are a few centimeters thick and contain wells or divets. In this paper, we aim to solve the task of placing the well-plate onto a well-plate holder (referred to as holder). This task is challenging due to the holder's raised grooves being a few millimeters in height, with a clearance of less than 1 mm between the well-plate and holder, thus requiring precise control during placing. Our placing task has the following challenges: 1) The holder's detected pose is uncertain; 2) the required accuracy is at the millimeter to sub-millimeter level due to the raised groove's shallow height and small clearance; 3) the holder is not fixed to a desk and is susceptible to movement from external forces. To address these challenges, we developed methods including a) using tactile sensors for accurate pose estimation of the grasped well-plate to handle issue (1); b) sliding the well-plate onto the target holder while maintaining contact with the holder's groove and estimating its orientation for accurate alignment. This allows for high precision control (addressing issue (2)) and prevents displacement of the holder during placement (addressing issue (3)). We demonstrate a high success rate for the well-plate placing task, even under noisy observation of the holder's pose."
Real-Time Particle Cluster Manipulation with Holographic Acoustic End-Effector under Microscope,"An, Siyuan; Zhong, Chengxi; Wang, Mingyue; LIU, Song",,
Dung Beetle Optimizer-based High-precision Localization for Magnetic-Controlled Capsule Robot,"Zeng, Zijin; Wang, Fengwu; Li, Chan; Tan, Menglu; Wang, Shengyuan; Feng, Lin",,
Kinematic Modeling of Twisted String Actuator Based on Invertible Neural Networks,"Liu, Zekun; Wei, Dunwen; Gao, Tao; Gong, Jumin",,
MCGMapper: Light-Weight Incremental Structure from Motion and Visual Localization With Planar Markers and Camera Groups,"Xie, Yusen; Huang, Zhenmin; Chen, Kai; Zhu, Lei; Ma, Jun",https://arxiv.org/abs/2405.16599,"Structure from Motion (SfM) and visual localization in indoor texture-less scenes and industrial scenarios present prevalent yet challenging research topics. Existing SfM methods designed for natural scenes typically yield low accuracy or map-building failures due to insufficient robust feature extraction in such settings. Visual markers, with their artificially designed features, can effectively address these issues. Nonetheless, existing marker-assisted SfM methods encounter problems like slow running speed and difficulties in convergence; and also, they are governed by the strong assumption of unique marker size. In this paper, we propose a novel SfM framework that utilizes planar markers and multiple cameras with known extrinsics to capture the surrounding environment and reconstruct the marker map. In our algorithm, the initial poses of markers and cameras are calculated with Perspective-n-Points (PnP) in the front-end, while bundle adjustment methods customized for markers and camera groups are designed in the back-end to optimize the 6-DOF pose directly. Our algorithm facilitates the reconstruction of large scenes with different marker sizes, and its accuracy and speed of map building are shown to surpass existing methods. Our approach is suitable for a wide range of scenarios, including laboratories, basements, warehouses, and other industrial settings. Furthermore, we incorporate representative scenarios into simulations and also supply our datasets with pose labels to address the scarcity of quantitative ground-truth datasets in this research field. The datasets and source code are available on GitHub."
Multidirectional slip detection and avoidance using dynamic 3D tactile meshes from visuotactile sensors,"Song, Peng; Corrales Ramon, Juan Antonio; Mezouar, Youcef",,
Toward Understanding Key Estimation in Learning Robust Humanoid Locomotion,"Wang, Zhicheng; Wei, Wandi; Yu, Ruiqi; Wu, Jun; Zhu, Qiuguo",https://arxiv.org/abs/2403.05868,"Accurate state estimation plays a critical role in ensuring the robust control of humanoid robots, particularly in the context of learning-based control policies for legged robots. However, there is a notable gap in analytical research concerning estimations. Therefore, we endeavor to further understand how various types of estimations influence the decision-making processes of policies. In this paper, we provide quantitative insight into the effectiveness of learned state estimations, employing saliency analysis to identify key estimation variables and optimize their combination for humanoid locomotion tasks. Evaluations assessing tracking precision and robustness are conducted on comparative groups of policies with varying estimation combinations in both simulated and real-world environments. Results validated that the proposed policy is capable of crossing the sim-to-real gap and demonstrating superior performance relative to alternative policy configurations."
Privacy-Preserving Map-Free Exploration for Confirming the Absence of a Radioactive Source,"Lepowsky, Eric; Snyder, David; Glaser, Alexander; Majumdar, Anirudha",https://arxiv.org/abs/2402.17130,"Performing an inspection task while maintaining the privacy of the inspected site is a challenging balancing act. In this work, we are motivated by the future of nuclear arms control verification, which requires both a high level of privacy and guaranteed correctness. For scenarios with limitations on sensors and stored information due to the potentially secret nature of observable features, we propose a robotic verification procedure that provides map-free exploration to perform a source verification task without requiring, nor revealing, any task-irrelevant, site-specific information. We provide theoretical guarantees on the privacy and correctness of our approach, validated by extensive simulated and hardware experiments."
Multi-Fingered End-Effector Grasp Reflex Modeling for One-Shot Tactile Servoing in Tool Manipulation Tasks,"Sheetz, Emily; Savchenko, Misha; Zemler, Emma; Presswala, Abbas; Crouch, Andrew; Azimi, Shaun; Kuipers, Benjamin",,
Optimal Robot Formations: Balancing Range-Based Observability and User-Defined Configurations,"Ahmed, Syed Shabbir; Shalaby, Mohammed Ayman; Le Ny, Jerome; Forbes, James Richard",https://arxiv.org/abs/2403.00988,"This paper introduces a set of customizable and novel cost functions that enable the user to easily specify desirable robot formations, such as a ``high-coverage'' infrastructure-inspection formation, while maintaining high relative pose estimation accuracy. The overall cost function balances the need for the robots to be close together for good ranging-based relative localization accuracy and the need for the robots to achieve specific tasks, such as minimizing the time taken to inspect a given area. The formations found by minimizing the aggregated cost function are evaluated in a coverage path planning task in simulation and experiment, where the robots localize themselves and unknown landmarks using a simultaneous localization and mapping algorithm based on the extended Kalman filter. Compared to an optimal formation that maximizes ranging-based relative localization accuracy, these formations significantly reduce the time to cover a given area with minimal impact on relative pose estimation accuracy."
VoxelContrast: Voxel Contrast-Based Unsupervised Learning for 3D Point Clouds,"Qin, Yuxiang; SUN, HAO",,
Visual Place Recognition in Unstructured Driving Environments,"RAI, UTKARSH; Gangisetty, Shankar; Abdul Hafez, A. H.; Subramanian, Anbumani; Jawahar, C.V.",https://arxiv.org/abs/2308.00688,"Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR -- a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher performance than existing approaches. We further obtain a 6% improvement in performance by characterizing the semantic properties of these features, uncovering unique domains which encapsulate datasets from similar environments. Our detailed experiments and analysis lay a foundation for building VPR solutions that may be deployed anywhere, anytime, and across anyview. We encourage the readers to explore our project page and interactive demos: https://anyloc.github.io/."
"Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity","Varley, Jacob; Singh, Sumeet; Jain, Deepali; Choromanski, Krzysztof; Zeng, Andy; Basu Roy Chowdhury, Somnath; Dubey, Avinava; Sindhwani, Vikas",https://arxiv.org/abs/2404.03570,"We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace. Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies."
Real-time Dexterous Telemanipulation with an End-Effect-Oriented Learning-based Approach,"Wang, Haoyang; BAI, HE; Zhang, Xiaoli; Jung, Yunsik; Bowman, Michael; Tao, Lingfeng",https://arxiv.org/abs/2408.00853,"Dexterous telemanipulation is crucial in advancing human-robot systems, especially in tasks requiring precise and safe manipulation. However, it faces significant challenges due to the physical differences between human and robotic hands, the dynamic interaction with objects, and the indirect control and perception of the remote environment. Current approaches predominantly focus on mapping the human hand onto robotic counterparts to replicate motions, which exhibits a critical oversight: it often neglects the physical interaction with objects and relegates the interaction burden to the human to adapt and make laborious adjustments in response to the indirect and counter-intuitive observation of the remote environment. This work develops an End-Effects-Oriented Learning-based Dexterous Telemanipulation (EFOLD) framework to address telemanipulation tasks. EFOLD models telemanipulation as a Markov Game, introducing multiple end-effect features to interpret the human operator's commands during interaction with objects. These features are used by a Deep Reinforcement Learning policy to control the robot and reproduce such end effects. EFOLD was evaluated with real human subjects and two end-effect extraction methods for controlling a virtual Shadow Robot Hand in telemanipulation tasks. EFOLD achieved real-time control capability with low command following latency (delay<0.11s) and highly accurate tracking (MSE<0.084 rad)."
Latent Disentanglement for Low Light Image Enhancement,"Zheng, Zhihao; Chuah, Mooi Choo",https://arxiv.org/abs/2408.06245,"Many learning-based low-light image enhancement (LLIE) algorithms are based on the Retinex theory. However, the Retinex-based decomposition techniques in such models introduce corruptions which limit their enhancement performance. In this paper, we propose a Latent Disentangle-based Enhancement Network (LDE-Net) for low light vision tasks. The latent disentanglement module disentangles the input image in latent space such that no corruption remains in the disentangled Content and Illumination components. For LLIE task, we design a Content-Aware Embedding (CAE) module that utilizes Content features to direct the enhancement of the Illumination component. For downstream tasks (e.g. nighttime UAV tracking and low-light object detection), we develop an effective light-weight enhancer based on the latent disentanglement framework. Comprehensive quantitative and qualitative experiments demonstrate that our LDE-Net significantly outperforms state-of-the-art methods on various LLIE benchmarks. In addition, the great results obtained by applying our framework on the downstream tasks also demonstrate the usefulness of our latent disentanglement design."
Signal Temporal Logic-Guided Apprenticeship Learning,"Puranic, Aniruddh Gopinath; Deshmukh, Jyotirmoy; Nikolaidis, Stefanos",https://arxiv.org/abs/2311.05084,"Apprenticeship learning crucially depends on effectively learning rewards, and hence control policies from user demonstrations. Of particular difficulty is the setting where the desired task consists of a number of sub-goals with temporal dependencies. The quality of inferred rewards and hence policies are typically limited by the quality of demonstrations, and poor inference of these can lead to undesirable outcomes. In this letter, we show how temporal logic specifications that describe high level task objectives, are encoded in a graph to define a temporal-based metric that reasons about behaviors of demonstrators and the learner agent to improve the quality of inferred rewards and policies. Through experiments on a diverse set of robot manipulator simulations, we show how our framework overcomes the drawbacks of prior literature by drastically improving the number of demonstrations required to learn a control policy."
Pos2VPR: Fast Position Consistency Validation with Positive Sample Mining for Hierarchical Place Recognition,"Zou, Dehao; Qian, Xiaolong; Zhang, Yunzhou; Zhao, Xinge; Wang, Zhuo",,
Modular Robot Ware for Walking Rehabilitation Assistance According to Physical Functionality,"Ogata, Kunihiro; Futawatari, Toshiki; Fujimoto, Masahiro; Imamura, Yumeko; Matsumoto, Yoshio",,
Off-dynamics Conditional Diffusion Planners,"ng, wen zheng terence; chen, jianda; Zhang, Tianwei",,
CubiX: Portable Wire-Driven Parallel Robot Connecting to and Utilizing the Environment,"Inoue, Shintaro; Kawaharazuka, Kento; Suzuki, Temma; Yuzaki, Sota; Okada, Kei; Inaba, Masayuki",,
Decentralized Linear Convoying for Underactuated Surface Craft with Partial State Coupling,"Turrisi, Raymond; Benjamin, Michael",,
Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach,"Jiang, Yongpeng; Yu, Mingrui; Zhu, Xinghao; Tomizuka, Masayoshi; LI, Xiang",https://arxiv.org/abs/2402.18897,"Dexterous in-hand manipulation is an essential skill of production and life. Nevertheless, the highly stiff and mutable features of contacts cause limitations to real-time contact discovery and inference, which degrades the performance of model-based methods. Inspired by recent advancements in contact-rich locomotion and manipulation, this paper proposes a novel model-based approach to control dexterous in-hand manipulation and overcome the current limitations. The proposed approach has the attractive feature, which allows the robot to robustly execute long-horizon in-hand manipulation without pre-defined contact sequences or separated planning procedures. Specifically, we design a contact-implicit model predictive controller at high-level to generate real-time contact plans, which are executed by the low-level tracking controller. Compared with other model-based methods, such a long-horizon feature enables replanning and robust execution of contact-rich motions to achieve large-displacement in-hand tasks more efficiently; Compared with existing learning-based methods, the proposed approach achieves the dexterity and also generalizes to different objects without any pre-training. Detailed simulations and ablation studies demonstrate the efficiency and effectiveness of our method. It runs at 20Hz on the 23-degree-of-freedom long-horizon in-hand object rotation task."
GNC Design and Orbital Performance Evaluation of ISS Onboard Autonomous Free-Flying Robot Int-Ball2,"Nishishita, Taisei; WATANABE, Keisuke; Hirano, Daichi; Mitani, Shinji",,
Development Force Control of a Series Elastic Actuator to Excavator for Mechanization of Manual Work,"Hiramatsu, Toshifumi; Saiki, Miyuki; Hara, Naohiro; Yamada, Masaki; Momii, Masaki; Uebayashi, Yuichi; Sugiura, Hisashi",,
Sensorimotor Attention and Language-based Regressions in Shared Latent Variables for Integrating Robot Motion Learning and LLM,"Suzuki, Kanata; Ogata, Tetsuya",https://arxiv.org/abs/2407.09044,"In recent years, studies have been actively conducted on combining large language models (LLM) and robotics; however, most have not considered end-to-end feedback in the robot-motion generation phase. The prediction of deep neural networks must contain errors, it is required to update the trained model to correspond to the real environment to generate robot motion adaptively. This study proposes an integration method that connects the robot-motion learning model and LLM using shared latent variables. When generating robot motion, the proposed method updates shared parameters based on prediction errors from both sensorimotor attention points and task language instructions given to the robot. This allows the model to search for latent parameters appropriate for the robot task efficiently. Through simulator experiments on multiple robot tasks, we demonstrated the effectiveness of our proposed method from two perspectives: position generalization and language instruction generalization abilities."
Iterative Reference Learning for Cartesian Impedance Control of Robot Manipulators,"Salt Ducaju, Julian Mauricio; Olofsson, Bjorn; Johansson, Rolf",,
Robust Locomotion via Zero-order Stochastic Nonlinear Model Predictive Control with Guard Saltation Matrix,"Katayama, Sotaro; Takasugi, Noriaki; Kaneko, Mitsuhisa; Nagatsuka, Norio; Kinoshita, Masaya",https://arxiv.org/abs/2403.14159,"This paper presents a stochastic/robust nonlinear model predictive control (NMPC) to enhance the robustness of legged locomotion against contact uncertainties. We integrate the contact uncertainties into the covariance propagation of stochastic/robust NMPC framework by leveraging the guard saltation matrix and an extended Kalman filter-like covariance update. We achieve fast stochastic/robust NMPC computation by utilizing the zero-order stochastic/robust NMPC algorithm with additional improvements in computational efficiency concerning the feedback gains. We conducted numerical experiments and demonstrate that the proposed method can accurately forecast future state covariance and generate trajectories that satisfies constraints even in the presence of the contact uncertainties. Hardware experiments on the perceptive locomotion of a wheeled-legged robot were also carried out, validating the feasibility of the proposed method in a real-world system with limited on-board computation."
Autonomous Storytelling for Social Robot with Human-Centered Reinforcement Learning,"Zhang, Lei; Zheng, Chuanxiong; Wang, Hui; Gomez, Randy; Nichols, Eric; Li, Guangliang",,
Model Agnostic Defense against Adversarial Patch Attacks on Object Detection in Unmanned Aerial Vehicles,"Pathak, Saurabh; Shrestha, Samridha; AlMahmoud, Abdelrahman",https://arxiv.org/abs/2405.19179,"Object detection forms a key component in Unmanned Aerial Vehicles (UAVs) for completing high-level tasks that depend on the awareness of objects on the ground from an aerial perspective. In that scenario, adversarial patch attacks on an onboard object detector can severely impair the performance of upstream tasks. This paper proposes a novel model-agnostic defense mechanism against the threat of adversarial patch attacks in the context of UAV-based object detection. We formulate adversarial patch defense as an occlusion removal task. The proposed defense method can neutralize adversarial patches located on objects of interest, without exposure to adversarial patches during training. Our lightweight single-stage defense approach allows us to maintain a model-agnostic nature, that once deployed does not require to be updated in response to changes in the object detection pipeline. The evaluations in digital and physical domains show the feasibility of our method for deployment in UAV object detection pipelines, by significantly decreasing the Attack Success Ratio without incurring significant processing costs. As a result, the proposed defense solution can improve the reliability of object detection for UAVs."
Object Pose Estimation by Camera Arm Control Based on Viewpoint Estimation,"Mizuno, Tomoki; Yabashi, Kazuya; Tasaki, Tsuyoshi",,
Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights within Unknown Environments,"Rocha, Lidia; Bidinotto, Jorge; Heintz, Fredrik; Tiger, Mattias; Vivaldini, Kelen Cristiane Teixeira",,
WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization,"wang, jialu; zhou, kaichen; Markham, Andrew; Trigoni, Niki",https://arxiv.org/abs/2403.15272,"Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor. While current weakly supervised methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views. In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under weakly-supervised and sparse view conditions. This is realized with two stages. In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-NeRF to co-optimize image reconstruction quality and initial pose information. To ensure a stable learning process, we incorporate temporal information as input. Furthermore, instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to explicitly enforce a scale constraint. In the second stage, we co-optimize the pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by Time-Encoding based Random View Synthesis and supervised by inter-frame geometric constraints that consider pose, depth, and RGB information. We validate our approaches on two publicly available datasets, one outdoor and one indoor. Our experimental results demonstrate that our weakly-supervised relocalization solutions achieve superior pose estimation accuracy in sparse-view scenarios, comparable to state-of-the-art camera relocalization methods. We will make our code publicly available."
Joint-Level IS-MPC: a Whole-Body MPC with Centroidal Feasibility for Humanoid Locomotion,"Belvedere, Tommaso; Scianca, Nicola; Lanari, Leonardo; Oriolo, Giuseppe",,
Making the Flow Glow -- Robot Perception under Severe Lighting Conditions using Normalizing Flow Gradients,"Kristoffersson Lind, Simon; Krueger, Volker; Triebel, Rudolph",,
Real-time Perceptive Motion Control using Control Barrier Functions with Analytical Smoothing for Six-Wheeled-Telescopic-Legged Robot Tachyon 3,"Takasugi, Noriaki; Kinoshita, Masaya; Kamikawa, Yasuhisa; Tsuzaki, Ryoichi; Sakamoto, Atsushi; Kai, Toshimitsu; Kawanami, Yasunori",https://arxiv.org/abs/2310.11792,"To achieve safe legged locomotion, it is important to generate motion in real-time considering various constraints in robots and environments. In this study, we propose a lightweight real-time perspective motion control system for the newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the proposed method, analytically smoothed constraints including Smooth Separating Axis Theorem (Smooth SAT) as a novel higher order differentiable collision detection for 3D shapes is applied to the Control Barrier Function (CBF). The proposed system integrating the CBF achieves online motion generation in a short control cycle of 1 ms that satisfies joint limitations, environmental collision avoidance and safe convex foothold constraints. The efficiency of Smooth SAT is shown from the collision detection time of 1 us or less and the CBF constraint computation time for Tachyon3 of several us. Furthermore, the effectiveness of the proposed system is verified through the stair-climbing motion, integrating online recognition in a simulation and a real machine."
Online Adaptation of Learned Vehicle Dynamics Model with Meta-Learning Approach,"Tsuchiya, Yuki; Balch, Thomas; Drews, Paul; Rosman, Guy",https://arxiv.org/abs/2004.11345,"Transporting suspended payloads is challenging for autonomous aerial vehicles because the payload can cause significant and unpredictable changes to the robot's dynamics. These changes can lead to suboptimal flight performance or even catastrophic failure. Although adaptive control and learning-based methods can in principle adapt to changes in these hybrid robot-payload systems, rapid mid-flight adaptation to payloads that have a priori unknown physical properties remains an open problem. We propose a meta-learning approach that ""learns how to learn"" models of altered dynamics within seconds of post-connection flight data. Our experiments demonstrate that our online adaptation approach outperforms non-adaptive methods on a series of challenging suspended payload transportation tasks. Videos and other supplemental material are available on our website: https://sites.google.com/view/meta-rl-for-flight"
Optimizing Kubernetes Deployment of Robotic Applications with HEFT-based Container Orchestration,"Lumpp, Francesco; Fummi, Franco; Bombieri, Nicola",,
Multiple Visual Features in Topological Map for Vision-and-Language Navigation,"Liu, Ruonan; Kong, Ping; Zhang, Weidong",,
Self-Selecting Semi-Supervised Transformer-Attention Convolutional Network for Four Class EEG-Based Motor Imagery Decoding,"Ng, Han Wei; Guan, Cuntai",,
Augmenting Vision with Radar for All-weather Geo-localization without a Prior HD Map,"DONG, CAN; Hong, Ziyang; Li, Siru; Hu, Liang; Gao, Huijun",,
How Physics and Background Attributes Impact Video Transformers in Robotic Manipulation: A Case Study on Planar Pushing,"Jin, Shutong; WANG, RUIYU; Zahid, Muhammad; Pokorny, Florian T.",https://arxiv.org/abs/2310.02044,"As model and dataset sizes continue to scale in robot learning, the need to understand how the composition and properties of a dataset affect model performance becomes increasingly urgent to ensure cost-effective data collection and model performance. In this work, we empirically investigate how physics attributes (color, friction coefficient, shape) and scene background characteristics, such as the complexity and dynamics of interactions with background objects, influence the performance of Video Transformers in predicting planar pushing trajectories. We investigate three primary questions: How do physics attributes and background scene characteristics influence model performance? What kind of changes in attributes are most detrimental to model generalization? What proportion of fine-tuning data is required to adapt models to novel scenarios? To facilitate this research, we present CloudGripper-Push-1K, a large real-world vision-based robot pushing dataset comprising 1278 hours and 460,000 videos of planar pushing interactions with objects with different physics and background attributes. We also propose Video Occlusion Transformer (VOT), a generic modular video-transformer-based trajectory prediction framework which features 3 choices of 2D-spatial encoders as the subject of our case study. The dataset and source code are available at https://cloudgripper.org."
Development of a Spherical Wheel-legged Composite Mobile Robot with Multimodal Motion Capabilities,"Du, Yuyang; Ye, Ruihua; Xu, Wenfu",,
High-Accuracy 3D AoA Estimation Using Lightweight UWB Arrays,"Li, Yi; Zhao, Hanying; Liu, Yiman; Wang, Tianyu; Jincheng, Yu; Shen, Yuan",,
Automatic Image Annotation for Mapped Features Detection,"Noizet, Maxime; Xu, Philippe; Bonnifait, Philippe",https://arxiv.org/abs/1705.00771,"We propose an automatic diabetic retinopathy (DR) analysis algorithm based on two-stages deep convolutional neural networks (DCNN). Compared to existing DCNN-based DR detection methods, the proposed algorithm have the following advantages: (1) Our method can point out the location and type of lesions in the fundus images, as well as giving the severity grades of DR. Moreover, since retina lesions and DR severity appear with different scales in fundus images, the integration of both local and global networks learn more complete and specific features for DR analysis. (2) By introducing imbalanced weighting map, more attentions will be given to lesion patches for DR grading, which significantly improve the performance of the proposed algorithm. In this study, we label 12,206 lesion patches and re-annotate the DR grades of 23,595 fundus images from Kaggle competition dataset. Under the guidance of clinical ophthalmologists, the experimental results show that our local lesion detection net achieve comparable performance with trained human observers, and the proposed imbalanced weighted scheme also be proved to significantly improve the capability of our DCNN-based DR grading algorithm."
Real-time Bandwidth-efficient Occupancy Grid Map Synchronization for Multi-Robot Systems,"SHI, LIUYU; Yin, Longji; Kong, Fanze; Ren, Yunfan; Zhu, Fangcheng; Tang, Benxu; Zhang, Fu",,
Decentralized Communication-Maintained Coordination for Multi-Robot Exploration: Achieving Connectivity and Adaptability,"Tang, Wei; Li, Chao; Wu, Jun; Zhu, Qiuguo",,
CaFNet: A Confidence-Driven Framework for Radar Camera Depth Estimation,"Sun, Huawei; Feng, Hao; Ott, Julius; Servadei, Lorenzo; Wille, Robert",https://arxiv.org/abs/2407.00697,"Depth estimation is critical in autonomous driving for interpreting 3D scenes accurately. Recently, radar-camera depth estimation has become of sufficient interest due to the robustness and low-cost properties of radar. Thus, this paper introduces a two-stage, end-to-end trainable Confidence-aware Fusion Net (CaFNet) for dense depth estimation, combining RGB imagery with sparse and noisy radar point cloud data. The first stage addresses radar-specific challenges, such as ambiguous elevation and noisy measurements, by predicting a radar confidence map and a preliminary coarse depth map. A novel approach is presented for generating the ground truth for the confidence map, which involves associating each radar point with its corresponding object to identify potential projection surfaces. These maps, together with the initial radar input, are processed by a second encoder. For the final depth estimation, we innovate a confidence-aware gated fusion mechanism to integrate radar and image features effectively, thereby enhancing the reliability of the depth map by filtering out radar noise. Our methodology, evaluated on the nuScenes dataset, demonstrates superior performance, improving upon the current leading model by 3.2% in Mean Absolute Error (MAE) and 2.7% in Root Mean Square Error (RMSE). Code: https://github.com/harborsarah/CaFNet"
Kinodynamic Motion Planning for a Team of Multirotors Transporting a Cable-Suspended Payload in Cluttered Environments,"Wahba, Khaled; Ortiz-Haro, Joaquim; Toussaint, Marc; Hoenig, Wolfgang",https://arxiv.org/abs/2310.03394,"We propose a motion planner for cable-driven payload transportation using multiple unmanned aerial vehicles (UAVs) in an environment cluttered with obstacles. Our planner is kinodynamic, i.e., it considers the full dynamics model of the transporting system including actuation constraints. Due to the high dimensionality of the planning problem, we use a hierarchical approach where we first solve the geometric motion planning using a sampling-based method with a novel sampler, followed by constrained trajectory optimization that considers the full dynamics of the system. Both planning stages consider inter-robot and robot/obstacle collisions. We demonstrate in a software-in-the-loop simulation and real flight experiments that there is a significant benefit in kinodynamic motion planning for such payload transport systems with respect to payload tracking error and energy consumption compared to the standard methods of planning for the payload alone. Notably, we observe a significantly higher success rate in scenarios where the team formation changes are needed to move through tight spaces."
Decentralized Acceleration-Based Bird-Inspired Flocking,"Iacone, Luca; Lejeune, Erwin Edouard Kossi; Manoni, Tiziano; Manfredi, Sabato; Albani, Dario",,
Optimal and Bounded Suboptimal Any-Angle Multi-agent Pathfinding,"Yakovlev, Konstantin; Andreychuk, Anton; Stern, Roni",https://arxiv.org/abs/2404.16379,"Multi-agent pathfinding (MAPF) is the problem of finding a set of conflict-free paths for a set of agents. Typically, the agents' moves are limited to a pre-defined graph of possible locations and allowed transitions between them, e.g. a 4-neighborhood grid. We explore how to solve MAPF problems when each agent can move between any pair of possible locations as long as traversing the line segment connecting them does not lead to a collision with the obstacles. This is known as any-angle pathfinding. We present the first optimal any-angle multi-agent pathfinding algorithm. Our planner is based on the Continuous Conflict-based Search (CCBS) algorithm and an optimal any-angle variant of the Safe Interval Path Planning (TO-AA-SIPP). The straightforward combination of those, however, scales poorly since any-angle path finding induces search trees with a very large branching factor. To mitigate this, we adapt two techniques from classical MAPF to the any-angle setting, namely Disjoint Splitting and Multi-Constraints. Experimental results on different combinations of these techniques show they enable solving over 30% more problems than the vanilla combination of CCBS and TO-AA-SIPP. In addition, we present a bounded-suboptimal variant of our algorithm, that enables trading runtime for solution cost in a controlled manner."
Deep Stochastic Kinematic Models for Probabilistic Motion Forecasting in Traffic,"Zheng, Laura; Son, Sanghyun; Liang, Jing; Wang, Xijun; Clipp, Brian; Lin, Ming C.",https://arxiv.org/abs/2406.01431,"In trajectory forecasting tasks for traffic, future output trajectories can be computed by advancing the ego vehicle's state with predicted actions according to a kinematics model. By unrolling predicted trajectories via time integration and models of kinematic dynamics, predicted trajectories should not only be kinematically feasible but also relate uncertainty from one timestep to the next. While current works in probabilistic prediction do incorporate kinematic priors for mean trajectory prediction, _variance_ is often left as a learnable parameter, despite uncertainty in one time step being inextricably tied to uncertainty in the previous time step. In this paper, we show simple and differentiable analytical approximations describing the relationship between variance at one timestep and that at the next with the kinematic bicycle model. In our results, we find that encoding the relationship between variance across timesteps works especially well in unoptimal settings, such as with small or noisy datasets. We observe up to a 50% performance boost in partial dataset settings and up to an 8% performance boost in large-scale learning compared to previous kinematic prediction methods on SOTA trajectory forecasting architectures out-of-the-box, with no fine-tuning."
MultiGripperGrasp: A Dataset for Robotic Grasping from Parallel Jaw Grippers to Dexterous Hands,"Casas Murrilo, Luis Felipe; Khargonkar, Ninad; Prabhakaran, B; Xiang, Yu",https://arxiv.org/abs/2403.09841,"We introduce a large-scale dataset named MultiGripperGrasp for robotic grasping. Our dataset contains 30.4M grasps from 11 grippers for 345 objects. These grippers range from two-finger grippers to five-finger grippers, including a human hand. All grasps in the dataset are verified in the robot simulator Isaac Sim to classify them as successful and unsuccessful grasps. Additionally, the object fall-off time for each grasp is recorded as a grasp quality measurement. Furthermore, the grippers in our dataset are aligned according to the orientation and position of their palms, allowing us to transfer grasps from one gripper to another. The grasp transfer significantly increases the number of successful grasps for each gripper in the dataset. Our dataset is useful to study generalized grasp planning and grasp transfer across different grippers. Data, code and videos for the project are available at https://irvlutd.github.io/MultiGripperGrasp"
"Tr&#780;iVis: Versatile, Reliable, and High-Performance Tool for Computing Visibility in Polygonal Environments","Mikula, Jan; Kulich, Miroslav",,
Physically Consistent Online Inertial Adaptation for Humanoid Loco-manipulation,"Foster, James Paul; McCrory, Stephen; DeBuys, Christian; Bertrand, Sylvain; Griffin, Robert J.",https://arxiv.org/abs/2405.07901,"The ability to accomplish manipulation and locomotion tasks in the presence of significant time-varying external loads is a remarkable skill of humans that has yet to be replicated convincingly by humanoid robots. Such an ability will be a key requirement in the environments we envision deploying our robots: dull, dirty, and dangerous. External loads constitute a large model bias, which is typically unaccounted for. In this work, we enable our humanoid robot to engage in loco-manipulation tasks in the presence of significant model bias due to external loads. We propose an online estimation and control framework involving the combination of a physically consistent extended Kalman filter for inertial parameter estimation coupled to a whole-body controller. We showcase our results both in simulation and in hardware, where weights are mounted on Nadia's wrist links as a proxy for engaging in tasks where large external loads are applied to the robot."
A Generic Trajectory Planning Method for Constrained All-Wheel-Steering Robots,"XIN, Ren; Liu, Hongji; Chen, Yingbing; WANG, Sheng; Liu, Ming",https://arxiv.org/abs/2404.09677,"This paper presents a trajectory planning method for wheeled robots with fixed steering axes while the steering angle of each wheel is constrained. In the past, All-Wheel-Steering(AWS) robots, incorporating modes such as rotation-free translation maneuvers, in-situ rotational maneuvers, and proportional steering, exhibited inefficient performance due to time-consuming mode switches. This inefficiency arises from wheel rotation constraints and inter-wheel cooperation requirements. The direct application of a holonomic moving strategy can lead to significant slip angles or even structural failure. Additionally, the limited steering range of AWS wheeled robots exacerbates nonlinearity issues, thereby complicating control processes. To address these challenges, we developed a novel planning method termed Constrained AWS(C-AWS), which integrates second-order discrete search with predictive control techniques. Experimental results demonstrate that our method adeptly generates feasible and smooth trajectories for C-AWS while adhering to steering angle constraints."
On Predicting Terrain Changes Induced by Mobile Robot Traversal,"Pragr, Milos; Bayer, Jan; Faigl, Jan",,
Programming Passive Fingertip Deformation for Improved Grasping and Manipulation,"Puhlmann, Steffen; Weber, Lion-Constantin; Hoeppner, Hannes",,
Trajectory Planning of Multiple Robots using Vision-Based Continuous Deep Reinforcement Learning and Model Predictive Control,"Ceder, Kristian; Zhang, Ze; Burman, Adam; Kuangaliyev, Ilya; Mattsson, Krister; Nyman, Gabriel; Petersén, Arvid; Wisell, Lukas; Akesson, Knut",,
Integrating Model-Based Footstep Planning with Model-Free Reinforcement Learning for Dynamic Legged Locomotion,"Lee, Ho Jae; Hong, Seungwoo; Kim, Sangbae",https://arxiv.org/abs/2408.02662,"In this work, we introduce a control framework that combines model-based footstep planning with Reinforcement Learning (RL), leveraging desired footstep patterns derived from the Linear Inverted Pendulum (LIP) dynamics. Utilizing the LIP model, our method forward predicts robot states and determines the desired foot placement given the velocity commands. We then train an RL policy to track the foot placements without following the full reference motions derived from the LIP model. This partial guidance from the physics model allows the RL policy to integrate the predictive capabilities of the physics-informed dynamics and the adaptability characteristics of the RL controller without overfitting the policy to the template model. Our approach is validated on the MIT Humanoid, demonstrating that our policy can achieve stable yet dynamic locomotion for walking and turning. We further validate the adaptability and generalizability of our policy by extending the locomotion task to unseen, uneven terrain. During the hardware deployment, we have achieved forward walking speeds of up to 1.5 m/s on a treadmill and have successfully performed dynamic locomotion maneuvers such as 90-degree and 180-degree turns."
Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning,"P, Jishnu Jaykumar; Palanisamy, Kamalesh; Chao, Yu-Wei; Du, Xinya; Xiang, Yu",https://arxiv.org/abs/2307.03073,"We propose a novel framework for few-shot learning by leveraging large-scale vision-language models such as CLIP. Motivated by unimodal prototypical networks for few-shot learning, we introduce Proto-CLIP which utilizes image prototypes and text prototypes for few-shot learning. Specifically, Proto-CLIP adapts the image and text encoder embeddings from CLIP in a joint fashion using few-shot examples. The embeddings from the two encoders are used to compute the respective prototypes of image classes for classification. During adaptation, we propose aligning the image and text prototypes of the corresponding classes. Such alignment is beneficial for few-shot classification due to the reinforced contributions from both types of prototypes. Proto-CLIP has both training-free and fine-tuned variants. We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning, as well as in the real world for robot perception. The project page is available at https://irvlutd.github.io/Proto-CLIP"
Tactile Comfort: Lowering Heart Rate Through Touch Interactions with a Therapeutic Pocket Robot,"Frederiksen, Morten Roed; Stoy, Kasper; Mataric, Maja",,
In-Hand Following of Deformable Linear Objects Using Dexterous Fingers with Tactile Sensing,"Yu, Mingrui; Liang, Boyuan; Zhang, Xiang; Zhu, Xinghao; LI, Xiang; Tomizuka, Masayoshi",https://arxiv.org/abs/2403.12676,"Most research on deformable linear object (DLO) manipulation assumes rigid grasping. However, beyond rigid grasping and re-grasping, in-hand following is also an essential skill that humans use to dexterously manipulate DLOs, which requires continuously changing the grasp point by in-hand sliding while holding the DLO to prevent it from falling. Achieving such a skill is very challenging for robots without using specially designed but not versatile end-effectors. Previous works have attempted using generic parallel grippers, but their robustness is unsatisfactory owing to the conflict between following and holding, which is hard to balance with a one-degree-of-freedom gripper. In this work, inspired by how humans use fingers to follow DLOs, we explore the usage of a generic dexterous hand with tactile sensing to imitate human skills and achieve robust in-hand DLO following. To enable the hardware system to function in the real world, we develop a framework that includes Cartesian-space arm-hand control, tactile-based in-hand 3-D DLO pose estimation, and task-specific motion design. Experimental results demonstrate the significant superiority of our method over using parallel grippers, as well as its great robustness, generalizability, and efficiency."
Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments,"Singh, Kurran; Hong, Jungseok; Rypkema, Nicholas Rahardiyan; Leonard, John",https://arxiv.org/abs/2403.12837,"Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater. This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene. The method performs unsupervised object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization. Probabilistic data association is used to determine observation to landmark correspondences. Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates. Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available. Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene."
TOPPQuad: Dynamically-Feasible Time-Optimal Path Parametrization for Quadrotors,"Mao, Katherine; Spasojevic, Igor; Hsieh, M. Ani; Kumar, Vijay",https://arxiv.org/abs/2309.11637,"Planning time-optimal trajectories for quadrotors in cluttered environments is a challenging, non-convex problem. This paper addresses minimizing the traversal time of a given collision-free geometric path without violating bounds on individual motor thrusts of the vehicle. Previous approaches have either relied on convex relaxations that do not guarantee dynamic feasibility, or have generated overly conservative time parametrizations. We propose TOPPQuad, a time-optimal path parameterization algorithm for quadrotors which explicitly incorporates quadrotor rigid body dynamics and constraints such as bounds on inputs (including motor speeds) and state of the vehicle (including the pose, linear and angular velocity and acceleration). We demonstrate the ability of the planner to generate faster trajectories that respect hardware constraints of the robot compared to several planners with relaxed notions of dynamic feasibility. We also demonstrate how TOPPQuad can be used to plan trajectories for quadrotors that utilize bidirectional motors. Overall, the proposed approach paves a way towards maximizing the efficacy of autonomous micro aerial vehicles while ensuring their safety."
Adaptive Model Predictive Control for Differential-Algebraic Systems towards a Higher Path Accuracy for Physically Coupled Robots,"Ye, Xin; Handwerker, Karl; Hohmann, Sören",,
AutoJoin: Efficient Adversarial Training against Gradient-Free Perturbations for Robust Maneuvering via Denoising Autoencoder and Joint Learning,"Villarreal, Michael; Poudel, Bibek; Wickman, Ryan; Shen, Yu; Li, Weizi",,
PA-LOCO: Learning Perturbation-Adaptive Locomotion for Quadruped Robots,"Xiao, Zhiyuan; Zhang, Xinyu; Zhou, Xiang; Zhang, Qingrui",https://arxiv.org/abs/2407.04224,"Numerous locomotion controllers have been designed based on Reinforcement Learning (RL) to facilitate blind quadrupedal locomotion traversing challenging terrains. Nevertheless, locomotion control is still a challenging task for quadruped robots traversing diverse terrains amidst unforeseen disturbances. Recently, privileged learning has been employed to learn reliable and robust quadrupedal locomotion over various terrains based on a teacher-student architecture. However, its one-encoder structure is not adequate in addressing external force perturbations. The student policy would experience inevitable performance degradation due to the feature embedding discrepancy between the feature encoder of the teacher policy and the one of the student policy. Hence, this paper presents a privileged learning framework with multiple feature encoders and a residual policy network for robust and reliable quadruped locomotion subject to various external perturbations. The multi-encoder structure can decouple latent features from different privileged information, ultimately leading to enhanced performance of the learned policy in terms of robustness, stability, and reliability. The efficiency of the proposed feature encoding module is analyzed in depth using extensive simulation data. The introduction of the residual policy network helps mitigate the performance degradation experienced by the student policy that attempts to clone the behaviors of a teacher policy. The proposed framework is evaluated on a Unitree GO1 robot, showcasing its performance enhancement over the state-of-the-art privileged learning algorithm through extensive experiments conducted on diverse terrains. Ablation studies are conducted to illustrate the efficiency of the residual policy network."
An Attention-aware Deep Reinforcement Learning Framework for UAV-UGV Collaborative Route Planning,"Mondal, Mohammad Safwan; Ramasamy, Subramanian; Bhounsule, Pranav",,
PCT: Perspective Cue Training Framework for Multi-Camera BEV Segmentation,"Ishikawa, Haruya; Iida, Takumi; Konishi, Yoshinori; Aoki, Yoshimitsu",https://arxiv.org/abs/2403.12530,"Generating annotations for bird's-eye-view (BEV) segmentation presents significant challenges due to the scenes' complexity and the high manual annotation cost. In this work, we address these challenges by leveraging the abundance of unlabeled data available. We propose the Perspective Cue Training (PCT) framework, a novel training framework that utilizes pseudo-labels generated from unlabeled perspective images using publicly available semantic segmentation models trained on large street-view datasets. PCT applies a perspective view task head to the image encoder shared with the BEV segmentation head, effectively utilizing the unlabeled data to be trained with the generated pseudo-labels. Since image encoders are present in nearly all camera-based BEV segmentation architectures, PCT is flexible and applicable to various existing BEV architectures. PCT can be applied to various settings where unlabeled data is available. In this paper, we applied PCT for semi-supervised learning (SSL) and unsupervised domain adaptation (UDA). Additionally, we introduce strong input perturbation through Camera Dropout (CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are crucial for enhancing SSL capabilities using our teacher-student framework. Our comprehensive approach is simple and flexible but yields significant improvements over various baselines for SSL and UDA, achieving competitive performances even against the current state-of-the-art."
Harmonic Mobile Manipulation,"Yang, Ruihan; Kim, Yejin; Hendrix, Rose; Kembhavi, Aniruddha; Wang, Xiaolong; ehsani, kiana",https://arxiv.org/abs/2312.06639,"Recent advancements in robotics have enabled robots to navigate complex scenes or manipulate diverse objects independently. However, robots are still impotent in many household tasks requiring coordinated behaviors such as opening doors. The factorization of navigation and manipulation, while effective for some tasks, fails in scenarios requiring coordinated actions. To address this challenge, we introduce, HarmonicMM, an end-to-end learning method that optimizes both navigation and manipulation, showing notable improvement over existing techniques in everyday tasks. This approach is validated in simulated and real-world environments and adapts to novel unseen settings without additional tuning. Our contributions include a new benchmark for mobile manipulation and the successful deployment in a real unseen apartment, demonstrating the potential for practical indoor robot deployment in daily life. More results are on our project site: https://rchalyang.github.io/HarmonicMM/"
Robot Design Optimization with Rotational and Prismatic Joints Using Black-Box Multi-Objective Optimization,"Kawaharazuka, Kento; Okada, Kei; Inaba, Masayuki",,
Intelligent Fish Detection System with Similarity-Aware Transformer,"Li, Shengchen; Zuo, Haobo; Fu, Changhong; Wang, Zhiyong; Xu, Zhiqiang",,
Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization,"Wu, Cho-Ying; Zhong, Yiqi; Wang, Junying; Neumann, Ulrich",https://arxiv.org/abs/2409.02486,"Indoor robots rely on depth to perform tasks like navigation or obstacle detection, and single-image depth estimation is widely used to assist perception. Most indoor single-image depth prediction focuses less on model generalizability to unseen datasets, concerned with in-the-wild robustness for system deployment. This work leverages gradient-based meta-learning to gain higher generalizability on zero-shot cross-dataset inference. Unlike the most-studied meta-learning of image classification associated with explicit class labels, no explicit task boundaries exist for continuous depth values tied to highly varying indoor environments regarding object arrangement and scene composition. We propose fine-grained task that treats each RGB-D mini-batch as a task in our meta-learning formulation. We first show that our method on limited data induces a much better prior (max 27.8% in RMSE). Then, finetuning on meta-learned initialization consistently outperforms baselines without the meta approach. Aiming at generalization, we propose zero-shot cross-dataset protocols and validate higher generalizability induced by our meta-initialization, as a simple and useful plugin to many existing depth estimation methods. The work at the intersection of depth and meta-learning potentially drives both research to step closer to practical robotic and machine perception usage."
Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers,"Wu, Yuning; Wei, Jiaying; Oh, Jean; Cardoso Llach, Daniel",https://arxiv.org/abs/2403.19060,"In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ""work companion rover"" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive and collaborative human-robot workforce."
Dual Process Optimization for Multi-Vehicle Route Planning and Parts Collection Sequencing,"Higa, Ryota; Kato, Takuro; Ho, Florence",,
A Collaborative Stereo Camera with Two UAVs for Long-distance Mapping of Urban Buildings,"Wang, Zhaoying; Dong, Wei",,
RPMArt: Towards Robust Perception and Manipulation for Articulated Objects,"Wang, Junbo; Liu, Wenhai; Yu, Qiaojun; You, Yang; Liu, Liu; Wang, Weiming; Lu, Cewu",https://arxiv.org/abs/2403.16023,"Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer. Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects. After learning only from synthetic data, RPMArt is able to transfer zero-shot to real-world articulated objects. Experimental results confirm our approach's effectiveness, with our framework achieving state-of-the-art performance in both noise-added simulation and real-world environments. The code and data will be open-sourced for reproduction. More results are published on the project website at https://r-pmart.github.io ."
Design and Preliminary Validation of A Reconfigurable Quadrotor Aerial-Aquatic Vehicle with Tilting Mechanism,"Liu, Mengyao; Chen, Bai; Wang, Lingyu; MAO, ZEBING; Shen, Yayi",,
BronchoCopilot: Towards Autonomous Robotic Bronchoscopy via Multimodal Reinforcement Learning,"Zhao, Jianbo; Chen, Hao; Tian, Qingyao; Chen, Jian; Yang, Bingyu; Zhang, Zihui; Liu, Hongbin",https://arxiv.org/abs/2403.01483,"Bronchoscopy plays a significant role in the early diagnosis and treatment of lung diseases. This process demands physicians to maneuver the flexible endoscope for reaching distal lesions, particularly requiring substantial expertise when examining the airways of the upper lung lobe. With the development of artificial intelligence and robotics, reinforcement learning (RL) method has been applied to the manipulation of interventional surgical robots. However, unlike human physicians who utilize multimodal information, most of the current RL methods rely on a single modality, limiting their performance. In this paper, we propose BronchoCopilot, a multimodal RL agent designed to acquire manipulation skills for autonomous bronchoscopy. BronchoCopilot specifically integrates images from the bronchoscope camera and estimated robot poses, aiming for a higher success rate within challenging airway environment. We employ auxiliary reconstruction tasks to compress multimodal data and utilize attention mechanisms to achieve an efficient latent representation of this data, serving as input for the RL module. This framework adopts a stepwise training and fine-tuning approach to mitigate the challenges of training difficulty. Our evaluation in the realistic simulation environment reveals that BronchoCopilot, by effectively harnessing multimodal information, attains a success rate of approximately 90\% in fifth generation airways with consistent movements. Additionally, it demonstrates a robust capacity to adapt to diverse cases."
PSS-BA: LiDAR Bundle Adjustment with Progressive Spatial Smoothing,"li, jianping; Nguyen, Thien-Minh; Yuan, Shenghai; Xie, Lihua",https://arxiv.org/abs/2403.06124,"Accurate and consistent construction of point clouds from LiDAR scanning data is fundamental for 3D modeling applications. Current solutions, such as multiview point cloud registration and LiDAR bundle adjustment, predominantly depend on the local plane assumption, which may be inadequate in complex environments lacking of planar geometries or substantial initial pose errors. To mitigate this problem, this paper presents a LiDAR bundle adjustment with progressive spatial smoothing, which is suitable for complex environments and exhibits improved convergence capabilities. The proposed method consists of a spatial smoothing module and a pose adjustment module, which combines the benefits of local consistency and global accuracy. With the spatial smoothing module, we can obtain robust and rich surface constraints employing smoothing kernels across various scales. Then the pose adjustment module corrects all poses utilizing the novel surface constraints. Ultimately, the proposed method simultaneously achieves fine poses and parametric surfaces that can be directly employed for high-quality point cloud reconstruction. The effectiveness and robustness of our proposed approach have been validated on both simulation and real-world datasets. The experimental results demonstrate that the proposed method outperforms the existing methods and achieves better accuracy in complex environments with low planar structures."
Realistic Rainy Weather Simulation for LiDARs in CARLA Simulator,"Yang, Donglin; Cai, Xinyu; Liu, Zhenfeng; Jiang, Wentao; Zhang, Bo; Yan, Guohang; Gao, Xing; Liu, Si; Shi, Botian",https://arxiv.org/abs/2312.12772,"Employing data augmentation methods to enhance perception performance in adverse weather has attracted considerable attention recently. Most of the LiDAR augmentation methods post-process the existing dataset by physics-based models or machine-learning methods. However, due to the limited environmental annotations and the fixed vehicle trajectories in the existing dataset, it is challenging to edit the scene and expand the diversity of traffic flow and scenario. To this end, we propose a simulator-based physical modeling approach to augment LiDAR data in rainy weather in order to improve the perception performance of LiDAR in this scenario. We complete the modeling task of the rainy weather in the CARLA simulator and establish a pipeline for LiDAR data collection. In particular, we pay special attention to the spray and splash rolled up by the wheels of surrounding vehicles in rain and complete the simulation of this special scenario through the Spray Emitter method we developed. In addition, we examine the influence of different weather conditions on the intensity of the LiDAR echo, develop a prediction network for the intensity of the LiDAR echo, and complete the simulation of 4-feat LiDAR point cloud data. In the experiment, we observe that the model augmented by the synthetic data improves the object detection task's performance in the rainy sequence of the Waymo Open Dataset. Both the code and the dataset will be made publicly available at https://github.com/PJLab-ADG/PCSim#rainypcsim."
HabiCrowd: A High Performance Simulator for Crowd-Aware Visual Navigation,"Vuong, An Dinh; Nguyen, Tien Toan; Vu, Minh Nhat; Huang, Baoru; Binh, Huynh Thi Thanh; Vo, Thieu; Nguyen, Anh",https://arxiv.org/abs/2306.11377,"Visual navigation, a foundational aspect of Embodied AI (E-AI), has been significantly studied in the past few years. While many 3D simulators have been introduced to support visual navigation tasks, scarcely works have been directed towards combining human dynamics, creating the gap between simulation and real-world applications. Furthermore, current 3D simulators incorporating human dynamics have several limitations, particularly in terms of computational efficiency, which is a promise of E-AI simulators. To overcome these shortcomings, we introduce HabiCrowd, the first standard benchmark for crowd-aware visual navigation that integrates a crowd dynamics model with diverse human settings into photorealistic environments. Empirical evaluations demonstrate that our proposed human dynamics model achieves state-of-the-art performance in collision avoidance, while exhibiting superior computational efficiency compared to its counterparts. We leverage HabiCrowd to conduct several comprehensive studies on crowd-aware visual navigation tasks and human-robot interactions. The source code and data can be found at https://habicrowd.github.io/."
ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking with Limited Active Localization Updates,"Puthumanaillam, Gokul; Vora, Manav Ketan; Ornik, Melkior",https://arxiv.org/abs/2403.01564,"Optimal decision-making for trajectory tracking in partially observable, stochastic environments where the number of active localization updates -- the process by which the agent obtains its true state information from the sensors -- are limited, presents a significant challenge. Traditional methods often struggle to balance resource conservation, accurate state estimation and precise tracking, resulting in suboptimal performance. This problem is particularly pronounced in environments with large action spaces, where the need for frequent, accurate state data is paramount, yet the capacity for active localization updates is restricted by external limitations. This paper introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN) and Model Predictive Control (MPC) to optimize trajectory tracking with constrained active localization updates. The meta-trained DQN ensures adaptive active localization scheduling, while the MPC leverages available state information to improve tracking. The central contribution of this work is their reciprocal interaction: DQN's update decisions inform MPC's control strategy, and MPC's outcomes refine DQN's learning, creating a cohesive, adaptive system. Empirical evaluations in simulated and real-world settings demonstrate that ComTraQ-MPC significantly enhances operational efficiency and accuracy, providing a generalizable and approximately optimal solution for trajectory tracking in complex partially observable environments."
DeepBHMR: Learning Bidirectional Hybrid Mixture Models for Generalized Rigid Point Set Registration,"Min, Zhe; Zhang, Zhengyan; Zhang, Ang; Song, Rui; Li, Yibin; Meng, Max Q.-H.",,
Every Dataset Counts: Scaling up Monocular 3D Object Detection with Joint Datasets Training,"Ma, Fulong; YAN, Xiaoyang; ZHAO, Guoyang; Xu, Xiaojie; LIU, Yuxuan; Liu, Ming",https://arxiv.org/abs/2310.00920,"Monocular 3D object detection plays a crucial role in autonomous driving. However, existing monocular 3D detection algorithms depend on 3D labels derived from LiDAR measurements, which are costly to acquire for new datasets and challenging to deploy in novel environments. Specifically, this study investigates the pipeline for training a monocular 3D object detection model on a diverse collection of 3D and 2D datasets. The proposed framework comprises three components: (1) a robust monocular 3D model capable of functioning across various camera settings, (2) a selective-training strategy to accommodate datasets with differing class annotations, and (3) a pseudo 3D training approach using 2D labels to enhance detection performance in scenes containing only 2D labels. With this framework, we could train models on a joint set of various open 3D/2D datasets to obtain models with significantly stronger generalization capability and enhanced performance on new dataset with only 2D labels. We conduct extensive experiments on KITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scaling ability of the proposed method."
Emotional Tandem Robots: How Different Robot Behaviors Affect Human Perception While Controlling a Mobile Robot,"Kaduk, Julian; Weilbeer, Friederike; Hamann, Heiko",https://arxiv.org/abs/2403.03746,"In human-robot interaction (HRI), we study how humans interact with robots, but also the effects of robot behavior on human perception and well-being. Especially, the influence on humans by tandem robots with one human controlled and one autonomous robot or even semi-autonomous multi-robot systems is not yet fully understood. Here, we focus on a leader-follower scenario and study how emotionally expressive motion patterns of a small, mobile follower robot affect the perception of a human operator controlling the leading robot. We examined three distinct emotional behaviors for the follower compared to a neutral condition: angry, happy and sad. We analyzed how participants maneuvered the leader robot along a set path while experiencing each follower behavior in a randomized order. We identified a significant shift in attention toward the follower with emotionally expressive behaviors compared to the neutral condition. For example, the angry behavior significantly heightened participant stress levels and was considered the least preferred behavior. The happy behavior was the most preferred and associated with increased excitement by the participants. Integrating the proposed behaviors in robots can profoundly influence the human operator's attention, emotional state, and overall experience. These insights are valuable for future HRI tandem robot designs."
Design and Control of a Soft Supernumerary Robotic Limb Based on Fiber-Reinforced Actuator,"Zhang, Tianyi; XU, Jiajun; Zhao, Mengcheng; Huang, Kaizhen; Chen, Bai; Li, You-Fu",,
Zero-Shot Transfer of a Tactile-based Continuous Force Control Policy from Simulation to Robot,"Lach, Luca; Haschke, Robert; Tateo, Davide; Peters, Jan; Ritter, Helge Joachim; Borràs Sol, Júlia; Torras, Carme",,
Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on Floor Plane And Object Segmentation,"Niijima, Shun; Suzuki, Atsushi; Tsuzaki, Ryoichi; Kinoshita, Masaya",https://arxiv.org/abs/2403.14161,"Mobile robots equipped with multiple light detection and ranging (LiDARs) and capable of recognizing their surroundings are increasing due to the minitualization and cost reduction of LiDAR. This paper proposes a target-less extrinsic calibration method of multiple LiDARs with non-overlapping field of view (FoV). The proposed method uses accumulated point clouds of floor plane and objects while in motion. It enables accurate calibration with challenging configuration of LiDARs that directed towards the floor plane, caused by biased feature values. Additionally, the method includes a noise removal module that considers the scanning pattern to address bleeding points, which are noises of significant source of error in point cloud alignment using high-density LiDARs. Evaluations through simulation demonstrate that the proposed method achieved higher accuracy extrinsic calibration with two and four LiDARs than conventional methods, regardless type of objects. Furthermore, the experiments using a real mobile robot has shown that our proposed noise removal module can eliminate noise more precisely than conventional methods, and the estimated extrinsic parameters have successfully created consistent 3D maps."
Accurate Relative Position Tracking on Moving Trackers based on UWB Ranging and Inertial Sensing,"Armani, Rayan; Holz, Christian",,
CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition,"Fu, Shaowei; Duan, Yifan; Li, Yao; Meng, Chengzhen; Wang, Yingjie; Ji, Jianmin; Zhang, Yanyong",https://arxiv.org/abs/2403.15183,"The integration of complementary characteristics from camera and radar data has emerged as an effective approach in 3D object detection. However, such fusion-based methods remain unexplored for place recognition, an equally important task for autonomous systems. Given that place recognition relies on the similarity between a query scene and the corresponding candidate scene, the stationary background of a scene is expected to play a crucial role in the task. As such, current well-designed camera-radar fusion methods for 3D object detection can hardly take effect in place recognition because they mainly focus on dynamic foreground objects. In this paper, a background-attentive camera-radar fusion-based method, named CRPlace, is proposed to generate background-attentive global descriptors from multi-view images and radar point clouds for accurate place recognition. To extract stationary background features effectively, we design an adaptive module that generates the background-attentive mask by utilizing the camera BEV feature and radar dynamic points. With the guidance of a background mask, we devise a bidirectional cross-attention-based spatial fusion strategy to facilitate comprehensive spatial interaction between the background information of the camera BEV feature and the radar BEV feature. As the first camera-radar fusion-based place recognition network, CRPlace has been evaluated thoroughly on the nuScenes dataset. The results show that our algorithm outperforms a variety of baseline methods across a comprehensive set of metrics (recall@1 reaches 91.2%)."
Biodegradable Gliding Paper Flyers Fabricated Through Inkjet Printing,"Girardi, Luca; Wu, Rui; Fukatsu, Yuki; Shigemune, Hiroki; Mintchev, Stefano",,
Spatio-Temporal Consistent Mapping of Growing Plants for Agricultural Robots in the Wild,"Lobefaro, Luca; Malladi, Meher Venkata Ramakrishna; Guadagnino, Tiziano; Stachniss, Cyrill",,
Coordinated Multi-arm 3D Printing using Reeb Decomposition,"Khatkar, Jayant; Sukkar, Fouad; Clemon, Lee; Mettu, Ramgopal",,
Development of a Compact Robust Passive Transformable Omni-Ball for Enhanced Step-Climbing and Vibration Reduction,"Hongo, Kazuo; Kito, Takashi; Kamikawa, Yasuhisa; Kinoshita, Masaya; Kawanami, Yasunori",https://arxiv.org/abs/2403.14160,"This paper introduces the Passive Transformable Omni-Ball (PTOB), an advanced omnidirectional wheel engineered to enhance step-climbing performance, incorporate built-in actuators, diminish vibrations, and fortify structural integrity. By modifying the omni-ball's structure from two to three segments, we have achieved improved in-wheel actuation and a reduction in vibrational feedback. Additionally, we have implemented a sliding mechanism in the follower wheels to boost the wheel's step-climbing abilities. A prototype with a 127 mm diameter PTOB was constructed, which confirmed its functionality for omnidirectional movement and internal actuation. Compared to a traditional omni-wheel, the PTOB demonstrated a comparable level of vibration while offering superior capabilities. Extensive testing in varied settings showed that the PTOB can adeptly handle step obstacles up to 45 mm, equivalent to 35 $\%$ of the wheel's diameter, in both the forward and lateral directions. The PTOB showcased robust construction and proved to be versatile in navigating through environments with diverse obstacles."
MFC-EQ: Mean-Field Control with Envelope $Q$-learning for Moving Decentralized Agents in Formation,"Lin, Qiushi; Ma, Hang",,
A Heterogeneous System of Systems Framework for Proactive Path Planning of a UAV-assisted UGV in Uncertain Environments,"Sherman, Patrick; Bezzo, Nicola",,
PoCo: Point Context Cluster for RGBD Indoor Place Recognition,"Liang, Jing; deng, zhuo; Zhou, Zheming; Ghasemalizadeh, Omid; Manocha, Dinesh; Sun, Min; KUO, CHENG-HAO; Sen, Arnab",https://arxiv.org/abs/2404.02885,"We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place recognition task, aimed at identifying the most likely match for a given query frame within a reference database. The task presents inherent challenges attributed to the constrained field of view and limited range of perception sensors. We propose a new network architecture, which generalizes the recent Context of Clusters (CoCs) to extract global descriptors directly from the noisy point clouds through end-to-end learning. Moreover, we develop the architecture by integrating both color and geometric modalities into the point features to enhance the global descriptor representation. We conducted evaluations on public datasets ScanNet-PR and ARKit with 807 and 5047 scenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, we achieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis (61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the best-published result CGis (39.82%). In addition, PoCo shows higher efficiency than CGis in inference time (1.75X-faster), and we demonstrate the effectiveness of PoCo in recognizing places within a real-world laboratory environment."
Direct TPS-based 3D non-rigid motion estimation on 3D colored point cloud in eye-in-hand configuration,"Cuau, Lénaïc; Cavalcanti Santos, Joao; Poignet, Philippe; Zemiti, Nabil",,
GDM-Net: Gas Distribution Mapping with a Mobile Robot Using Deep Reinforcement Learning and Gaussian Process Regression,"Kulbaka, Iliya; Dutta, Ayan; Kreidl, Patrick; Bölöni, Ladislau; Roy, Swapnoneel",,
Towards a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning,"Gomaa, Amr; Mahdy, Bilal; Kleer, Niko; Krüger, Antonio",https://arxiv.org/abs/2311.17693,"Robot-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems cannot accommodate individual surgeons' unique preferences and requirements. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are unsuitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose an image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon's skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach trains reinforcement and imitation learning agents simultaneously using curriculum learning approaches guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon's actions and preferences into the training process, our approach enables the robot to implicitly learn and adapt to the individual surgeon's unique techniques through surgeon-in-the-loop demonstrations. This results in a more intuitive and personalized surgical experience for the surgeon while ensuring consistent performance for the autonomous robotic apprentice. We define and evaluate the effectiveness of our approach in a simulated environment using our proposed metrics and highlight the trade-off between a generic agent and a surgeon-centered adapted agent. Finally, our approach has the potential to extend to other ophthalmic and microsurgical procedures, opening the door to a new generation of surgeon-in-the-loop autonomous surgical robots. We provide an open-source simulation framework for future development and reproducibility at https://github.com/amrgomaaelhady/CataractAdaptSurgRobot."
An Intelligent Robotic System for Perceptive Pancake Batter Stirring and Precise Pouring,"Luo, Xinyuan; Jin, Shengmiao; Huang, Hung-Jui; Yuan, Wenzhen",https://arxiv.org/abs/2407.01755,"Cooking robots have long been desired by the commercial market, while the technical challenge is still significant. A major difficulty comes from the demand of perceiving and handling liquid with different properties. This paper presents a robot system that mixes batter and makes pancakes out of it, where understanding and handling the viscous liquid is an essential component. The system integrates Haptic Sensing and control algorithms to autonomously stir flour and water to achieve the desired batter uniformity, estimate the batter's properties such as the water-flour ratio and liquid level, as well as perform precise manipulations to pour the batter into any specified shape. Experimental results show the system's capability to always produce batter of desired uniformity, estimate water-flour ratio and liquid level precisely, and accurately pour it into complex shapes. This research showcases the potential for robots to assist in kitchens and step towards commercial culinary automation."
METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation,"Seo, Junwon; Kim, Taekyung; Ahn, Seongyong; Kwak, Kiho",https://arxiv.org/abs/2307.13991,"Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences. To conduct a comprehensive evaluation, we collect driving data from various terrains and demonstrate that our method can obtain a global model that minimizes uncertainty. Moreover, by integrating our model with a model predictive controller, we demonstrate that the reduced uncertainty results in safe and stable navigation in unstructured and unknown terrains."
Consistent Distributed Cooperative Localization: A Coordinate Transformation Approach,"Tian, Chungeng; Hao, Ning; He, Fenghua; Yao, Haodi",https://arxiv.org/abs/2303.01205,"This paper considers the problem of distributed cooperative localization (CL) via robot-to-robot measurements for a multi-robot system. We propose a distributed consistent CL algorithm. The key idea is to perform the EKF-based state estimation in a transformed coordinate system. Specifically, a coordinate transformation is constructed by decomposing the state-propagation Jacobian by which the correct observability properties are guaranteed. Moreover, the transformed state-propagation Jacobian becomes an identity matrix which is more suitable for distribution. In the proposed algorithm, a server-based framework is adopted to distributely estimate the robot pose in which each robot propagates its pose estimations and the server maintains the correlations. To reduce communication costs, only when the multi-robot system takes a robot-to-robot relative measurement, the robots and the server exchange information to update the pose estimations and the correlations. In addition, no assumptions are made about the type of robots or relative measurements. The proposed algorithm has been validated by experiments and shown to outperform the state-of-art algorithms in terms of consistency and accuracy."
3D Ultrasound Image Acquisition and Diagnostic Analysis of the Common Carotid Artery with a Portable Robotic Device,"Tan, Longyue; Deng, Zhaokun; Hao, Mingrui; Hou, Xilong; Chen, Chen; Gu, Xiaolin; Hou, Zeng-Guang; Wang, Shuangyi",,
DD-VNB: A Depth-based Dual-Loop Framework for Real-time Visually Navigated Bronchoscopy,"Tian, Qingyao; Liao, Huai; Huang, Xinyan; Chen, Jian; Zhang, Zihui; Yang, Bingyu; Ourselin, Sebastien; Liu, Hongbin",https://arxiv.org/abs/2403.01683,"Real-time 6 DOF localization of bronchoscopes is crucial for enhancing intervention quality. However, current vision-based technologies struggle to balance between generalization to unseen data and computational speed. In this study, we propose a Depth-based Dual-Loop framework for real-time Visually Navigated Bronchoscopy (DD-VNB) that can generalize across patient cases without the need of re-training. The DD-VNB framework integrates two key modules: depth estimation and dual-loop localization. To address the domain gap among patients, we propose a knowledge-embedded depth estimation network that maps endoscope frames to depth, ensuring generalization by eliminating patient-specific textures. The network embeds view synthesis knowledge into a cycle adversarial architecture for scale-constrained monocular depth estimation. For real-time performance, our localization module embeds a fast ego-motion estimation network into the loop of depth registration. The ego-motion inference network estimates the pose change of the bronchoscope in high frequency while depth registration against the pre-operative 3D model provides absolute pose periodically. Specifically, the relative pose changes are fed into the registration process as the initial guess to boost its accuracy and speed. Experiments on phantom and in-vivo data from patients demonstrate the effectiveness of our framework: 1) monocular depth estimation outperforms SOTA, 2) localization achieves an accuracy of Absolute Tracking Error (ATE) of 4.7 $\pm$ 3.17 mm in phantom and 6.49 $\pm$ 3.88 mm in patient data, 3) with a frame-rate approaching video capture speed, 4) without the necessity of case-wise network retraining. The framework's superior speed and accuracy demonstrate its promising clinical potential for real-time bronchoscopic navigation."
Efficient Balance Detection for Modular Robots,"YAZIDI, C45; Piranda, Benoit; Ouisse, Morvan; Bourgeois, Julien",https://arxiv.org/abs/1006.3394,"The Natural Immune System (NIS) is a distributed system that solves challenging search and response problems while operating under constraints imposed by physical space and resource availability. Remarkably, NIS search and response times do not scale appreciably with the physical size of the animal in which its search is conducted. Many distributed systems are engineered to solve analogous problems, and the NIS demonstrates how such engineered systems can achieve desirable scalability. We hypothesize that the architecture of the NIS, composed of a hierarchical decentralized detection network of lymph nodes (LN) facilitates efficient search and response. A sub-modular architecture in which LN numbers and size both scale with organism size is shown to efficiently balance tradeoffs between local antigen detection and global antibody production, leading to nearly scale-invariant detection and response. We characterize the tradeoffs as balancing local and global communication and show that similar tradeoffs exist in distributed systems like LN inspired artificial immune system (AIS) applications and peer-to-peer (P2P) systems. Taking inspiration from the architecture of the NIS, we propose a modular RADAR (Robust Adaptive Decentralized search with Automated Response) strategy for distributed systems. We demonstrate how two existing distributed systems (a LN inspired multi-robot control application and a P2P system) can be improved by a modular RADAR strategy. Such a sub-modular architecture is shown to balance the tradeoffs between local communication (within artificial LNs and P2P clusters) and global communication (between artificial LNs and P2P clusters), leading to efficient search and response."
STAIR: Semantic-Targeted Active Implicit Reconstruction,"Jin, Liren; Kuang, Haofei; Pan, Yue; Stachniss, Cyrill; Popovic, Marija",https://arxiv.org/abs/2403.11233,"Many autonomous robotic applications require object-level understanding when deployed. Actively reconstructing objects of interest, i.e. objects with specific semantic meanings, is therefore relevant for a robot to perform downstream tasks in an initially unknown environment. In this work, we propose a novel framework for semantic-targeted active reconstruction using posed RGB-D measurements and 2D semantic labels as input. The key components of our framework are a semantic implicit neural representation and a compatible planning utility function based on semantic rendering and uncertainty estimation, enabling adaptive view planning to target objects of interest. Our planning approach achieves better reconstruction performance in terms of mesh and novel view rendering quality compared to implicit reconstruction baselines that do not consider semantics for view planning. Our framework further outperforms a state-of-the-art semantic-targeted active reconstruction pipeline based on explicit maps, justifying our choice of utilising implicit neural representations to tackle semantic-targeted active reconstruction problems."
Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion,"Li, Ke; Bacher, Reinhard; Schmidt, Susanne; Leemans, Wim; Steinicke, Frank",https://arxiv.org/abs/2408.01225,"We introduce Reality Fusion, a novel robot teleoperation system that localizes, streams, projects, and merges a typical onboard depth sensor with a photorealistic, high resolution, high framerate, and wide field of view (FoV) rendering of the complex remote environment represented as 3D Gaussian splats (3DGS). Our framework enables robust egocentric and exocentric robot teleoperation in immersive VR, with the 3DGS effectively extending spatial information of a depth sensor with limited FoV and balancing the trade-off between data streaming costs and data visual quality. We evaluated our framework through a user study with 24 participants, which revealed that Reality Fusion leads to significantly better user performance, situation awareness, and user preferences. To support further research and development, we provide an open-source implementation with an easy-to-replicate custom-made telepresence robot, a high-performance virtual reality 3DGS renderer, and an immersive robot control package. (Source code: https://github.com/uhhhci/RealityFusion)"
Automatic Dietary Monitoring Using Inertial Sensor in Smartwatch,"Pavlov, Konstantin; Tsepulin, Vladimir; Lutsyak, Nikolay; Khasianov, Rasul; Simchuk, Egor; Perchik, Alexey; Elena, Volkova",,
Path Re-Planning with Stochastic Obstacle Modeling: A Monte Carlo Tree Search Approach,"Trotti, Francesco; Farinelli, Alessandro; Muradore, Riccardo",,
Development of a Bilateral Control Teleoperation System for Bipedal Humanoid Robot Utilizing Foot Sole Haptics Feedback,"Shen, Yang; Kanazawa, Masanobu; Mori, Kazuki; Isono, Ryu; Nakazawa, Yuri; Takanishi, Atsuo; Otani, Takuya",,
Collaborative Object Manipulation on the Water Surface by a UAV-USV Team Using Tethers,"Novák, Filip; Baca, Tomas; Saska, Martin",https://arxiv.org/abs/2407.08580,"This paper introduces an innovative methodology for object manipulation on the surface of water through the collaboration of an Unmanned Aerial Vehicle (UAV) and an Unmanned Surface Vehicle (USV) connected to the object by tethers. We propose a novel mathematical model of a robotic system that combines the UAV, USV, and the tethered floating object. A novel Model Predictive Control (MPC) framework is designed for using this model to achieve precise control and guidance for this collaborative robotic system. Extensive simulations in the realistic robotic simulator Gazebo demonstrate the system's readiness for real-world deployment, highlighting its versatility and effectiveness. Our multi-robot system overcomes the state-of-the-art single-robot approach, exhibiting smaller control errors during the tracking of the floating object's reference. Additionally, our multi-robot system demonstrates a shorter recovery time from a disturbance compared to the single-robot approach."
Safe Reinforcement Learning via Hierarchical Adaptive Chance-Constraint Safeguards,"Chen, Zhaorun; Zhao, Zhuokai; He, Tairan; Chen, BinHao; Zhao, Xuhao; Gong, Liang; Liu, Chengliang",https://arxiv.org/abs/2310.03379,"Ensuring safety in Reinforcement Learning (RL), typically framed as a Constrained Markov Decision Process (CMDP), is crucial for real-world exploration applications. Current approaches in handling CMDP struggle to balance optimality and feasibility, as direct optimization methods cannot ensure state-wise in-training safety, and projection-based methods correct actions inefficiently through lengthy iterations. To address these challenges, we propose Adaptive Chance-constrained Safeguards (ACS), an adaptive, model-free safe RL algorithm using the safety recovery rate as a surrogate chance constraint to iteratively ensure safety during exploration and after achieving convergence. Theoretical analysis indicates that the relaxed probabilistic constraint sufficiently guarantees forward invariance to the safe set. And extensive experiments conducted on both simulated and real-world safety-critical tasks demonstrate its effectiveness in enforcing safety (nearly zero-violation) while preserving optimality (+23.8%), robustness, and fast response in stochastic real-world settings."
Reducing Performance Variability and Overcoming Limited Spatial Ability: Targeted Training for Remote Robot Teleoperation,"Lin, Tsung-Chi; Chen, Juo-Tung; Huang, Chien-Ming",,
"Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks","Sejnova, Gabriela; Vavrecka, Michal; Stepanova, Karla",https://arxiv.org/abs/2404.01932,"In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length. Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language."
StereoNavNet: Learning to Navigate using Stereo Cameras with Auxiliary Occupancy Voxels,"Li, Hongyu; Padir, Taskin; Jiang, Huaizu",https://arxiv.org/abs/2403.12039,"Visual navigation has received significant attention recently. Most of the prior works focus on predicting navigation actions based on semantic features extracted from visual encoders. However, these approaches often rely on large datasets and exhibit limited generalizability. In contrast, our approach draws inspiration from traditional navigation planners that operate on geometric representations, such as occupancy maps. We propose StereoNavNet (SNN), a novel visual navigation approach employing a modular learning framework comprising perception and policy modules. Within the perception module, we estimate an auxiliary 3D voxel occupancy grid from stereo RGB images and extract geometric features from it. These features, along with user-defined goals, are utilized by the policy module to predict navigation actions. Through extensive empirical evaluation, we demonstrate that SNN outperforms baseline approaches in terms of success rates, success weighted by path length, and navigation error. Furthermore, SNN exhibits better generalizability, characterized by maintaining leading performance when navigating across previously unseen environments."
HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signal Through Contrastive Learning,"Li, Hongyu; Dikhale, Snehal; Cui, Jinda; Iba, Soshi; Jamali, Nawid",https://arxiv.org/abs/2408.08312,"To achieve dexterity comparable to that of humans, robots must intelligently process tactile sensor data. Taxel-based tactile signals often have low spatial-resolution, with non-standardized representations. In this paper, we propose a novel framework, HyperTaxel, for learning a geometrically-informed representation of taxel-based tactile signals to address challenges associated with their spatial resolution. We use this representation and a contrastive learning objective to encode and map sparse low-resolution taxel signals to high-resolution contact surfaces. To address the uncertainty inherent in these signals, we leverage joint probability distributions across multiple simultaneous contacts to improve taxel hyper-resolution. We evaluate our representation by comparing it with two baselines and present results that suggest our representation outperforms the baselines. Furthermore, we present qualitative results that demonstrate the learned representation captures the geometric features of the contact surface, such as flatness, curvature, and edges, and generalizes across different objects and sensor configurations. Moreover, we present results that suggest our representation improves the performance of various downstream tasks, such as surface classification, 6D in-hand pose estimation, and sim-to-real transfer."
Bayesian Deep Predictive Coding for Snake-like Robotic Control in Unknown Terrains,"Qu, William Ziming; Qu, Jessica; Li, Li; Yang, Jie; Jia, Yuanyuan",,
DailySTR: A Daily Human Activity Pattern Recognition Dataset for Spatio-temporal Reasoning,"QIU, YUE; Egami, Shusaku; Fukuda, Ken; Miyata, Natsuki; Yagi, Takuma; Hara, Kensho; Iwata, Kenji; Sagawa, Ryusuke",,
State Estimation Transformers for Agile Legged Locomotion,"Yu, Chen; Yang, Yichu; Liu, Tianlin; You, Yangwei; zhou, mingliang; Xiang, Diyun",,
Toward Precise Robotic Weed Flaming Using a Mobile Manipulator with a Flamethrower,"Wang, Di; Hu, Chengsong; Xie, Shuangyu; Johnson, Joe; Ji, Hojun; Jiang, Yingtao; Bagavathiannan, Muthukumar; Song, Dezhen",https://arxiv.org/abs/2407.04929,"Robotic weed flaming is a new and environmentally friendly approach to weed removal in the agricultural field. Using a mobile manipulator equipped with a flamethrower, we design a new system and algorithm to enable effective weed flaming, which requires robotic manipulation with a soft and deformable end effector, as the thermal coverage of the flame is affected by dynamic or unknown environmental factors such as gravity, wind, atmospheric pressure, fuel tank pressure, and pose of the nozzle. System development includes overall design, hardware integration, and software pipeline. To enable precise weed removal, the greatest challenge is to detect and predict dynamic flame coverage in real time before motion planning, which is quite different from a conventional rigid gripper in grasping or a spray gun in painting. Based on the images from two onboard infrared cameras and the pose information of the flamethrower nozzle on a mobile manipulator, we propose a new dynamic flame coverage model. The flame model uses a center-arc curve with a Gaussian cross-section model to describe the flame coverage in real time. The experiments have demonstrated the working system and shown that our model and algorithm can achieve a mean average precision (mAP) of more than 76\% in the reprojected images during online prediction."
Efficient Estimation of Frequency Response Functions of Industrial Robots Using the Local Rational Method,"Zimmermann, Stefanie Antonia; Moberg, Stig",,
Elliptical K-Nearest Neighbors - Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles,"Zhang, Liding; Bing, Zhenshan; Zhang, Yu; Chen, Lingyun; Wu, Fan; Haddadin, Sami; Knoll, Alois",,
$nu$-DBA: Neural Implicit Dense Bundle Adjustment Enables Image-Only Driving Scene Reconstruction,"Mao, Yunxuan; Shen, Bingqi; Yang, Yifei; Wang, Kai; Xiong, Rong; Liao, Yiyi; Wang, Yue",https://arxiv.org/abs/1411.0543,We obtain the wave function of mesonic systems interacting via a Cornell potential by applying improved variational approach. We study Isgur-Wise function and its parameters. Then we report our calculations on the parameters of semileptonic decay of B and Bs mesons. B-->D*l$nu$ process is also investigated in this work.
GazeMotion: Gaze-guided Human Motion Forecasting,"Hu, Zhiming; Schmitt, Syn; Haeufle, Daniel Florian Benedict; Bulling, Andreas",https://arxiv.org/abs/2403.09885,"We present GazeMotion, a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose graph, and finally uses a residual graph convolutional network to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO benchmark datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information."
Confidence-Aware Decision-Making and Control for Tool Selection,"Anil Meera, Ajith; Lanillos, Pablo",https://arxiv.org/abs/2403.03808,"Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence. To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task. The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces). Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable. Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems."
GS-Planner: A Gaussian-Splatting-based Planning Framework for Active High-Fidelity Reconstruction,"Jin, Rui; Gao, Yuman; Lu, Haojian; Xu, Chao; Gao, Fei",https://arxiv.org/abs/2405.10142,"Active reconstruction technique enables robots to autonomously collect scene data for full coverage, relieving users from tedious and time-consuming data capturing process. However, designed based on unsuitable scene representations, existing methods show unrealistic reconstruction results or the inability of online quality evaluation. Due to the recent advancements in explicit radiance field technology, online active high-fidelity reconstruction has become achievable. In this paper, we propose GS-Planner, a planning framework for active high-fidelity reconstruction using 3D Gaussian Splatting. With improvement on 3DGS to recognize unobserved regions, we evaluate the reconstruction quality and completeness of 3DGS map online to guide the robot. Then we design a sampling-based active reconstruction strategy to explore the unobserved areas and improve the reconstruction geometric and textural quality. To establish a complete robot active reconstruction system, we choose quadrotor as the robotic platform for its high agility. Then we devise a safety constraint with 3DGS to generate executable trajectories for quadrotor navigation in the 3DGS map. To validate the effectiveness of our method, we conduct extensive experiments and ablation studies in highly realistic simulation scenes."
Deep Sensor Fusion with Constraint Safety Bounds for High Precision Localization,"Schmidt, Sebastian; Stumpp, Ludwig; Valverde Garrro, Diego; Günnemann, Stephan",,
NanoNeRF: Robot-assisted Nanoscale 360° reconstruction with neural radiance field under scanning electron microscope,"Fu, Xiang; Xu, Yifan; su, hu; LIU, Song",,
UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric Estimation and Model Predictive Control,"Wang, Gaotian; Ren, Kejia; Hang, Kaiyu",https://arxiv.org/abs/2403.13274,"Nonprehensile manipulation through precise pushing is an essential skill that has been commonly challenged by perception and physical uncertainties, such as those associated with contacts, object geometries, and physical properties. For this, we propose a unified framework that jointly addresses system modeling, action generation, and control. While most existing approaches either heavily rely on a priori system information for analytic modeling, or leverage a large dataset to learn dynamic models, our framework approximates a system transition function via non-parametric learning only using a small number of exploratory actions (ca. 10). The approximated function is then integrated with model predictive control to provide precise pushing manipulation. Furthermore, we show that the approximated system transition functions can be robustly transferred across novel objects while being online updated to continuously improve the manipulation accuracy. Through extensive experiments on a real robot platform with a set of novel objects and comparing against a state-of-the-art baseline, we show that the proposed unified framework is a light-weight and highly effective approach to enable precise pushing manipulation all by itself. Our evaluation results illustrate that the system can robustly ensure millimeter-level precision and can straightforwardly work on any novel object."
Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation,"Lee, Joonhyung; Park, Sangbeom; Kwon, Yongin; Lee, Jemin; Ahn, Minwook; Choi, Sungjoon",https://arxiv.org/abs/2403.11513,"In robotic object manipulation, human preferences can often be influenced by the visual attributes of objects, such as color and shape. These properties play a crucial role in operating a robot to interact with objects and align with human intention. In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw visual observations in tabletop manipulation environments with a variety of object types, named Visual Preference Inference (VPI). To facilitate visual reasoning in the context of manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR employs a prompting mechanism that describes the difference between the consecutive images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user's preference. This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual environment during manipulation tasks. Furthermore, we incorporate such texts along with a sequence of images to infer the user's preferences. Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in both simulation and real-world environments. Code and videos are available at: \href{https://joonhyung-lee.github.io/vpi/}{https://joonhyung-lee.github.io/vpi/}"
Explicit Interaction for Fusion-Based Place Recognition,"Xu, Jingyi; Ma, Junyi; Wu, Qi; Zhou, Zijie; Wang, Yue; Chen, Xieyuanli; Yu, Wenxian; Pei, Ling",https://arxiv.org/abs/2402.17264,"Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based place recognition methods combine multi-modal features in implicit manners. While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system. Therefore, the benefit of multi-modal feature fusion may not be fully explored. In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities. EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds. In addition, we develop a new benchmark for the place recognition task based on the nuScenes dataset. To establish this benchmark for future research with comprehensive comparisons, we introduce both supervised and self-supervised training schemes alongside evaluation protocols. We conduct extensive experiments on the proposed benchmark, and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches. Our open-source code and benchmark are released at: https://github.com/BIT-XJY/EINet."
Stable Object Placing using Curl and Diff Features of Vision-based Tactile Sensors,"Takahashi, Kuniyuki; Masuda, Shimpei; Taniguchi, Tadahiro",https://arxiv.org/abs/2403.19129,"Ensuring stable object placement is crucial to prevent objects from toppling over, breaking, or causing spills. When an object makes initial contact to a surface, and some force is exerted, the moment of rotation caused by the instability of the object's placing can cause the object to rotate in a certain direction (henceforth referred to as direction of corrective rotation). Existing methods often employ a Force/Torque (F/T) sensor to estimate the direction of corrective rotation by detecting the moment of rotation as a torque. However, its effectiveness may be hampered by sensor noise and the tension of the external wiring of robot cables. To address these issues, we propose a method for stable object placing using GelSights, vision-based tactile sensors, as an alternative to F/T sensors. Our method estimates the direction of corrective rotation of objects using the displacement of the black dot pattern on the elastomeric surface of GelSight. We calculate the Curl from vector analysis, indicative of the rotational field magnitude and direction of the displacement of the black dots pattern. Simultaneously, we calculate the difference (Diff) of displacement between the left and right fingers' GelSight's black dots. Then, the robot can manipulate the objects' pose using Curl and Diff features, facilitating stable placing. Across experiments, handling 18 differently characterized objects, our method achieves precise placing accuracy (less than 1-degree error) in nearly 100% of cases. An accompanying video is available at the following link: https://youtu.be/fQbmCksVHlU"
NeuSurfEmb: A Plug-and-Play Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models,"Milano, Francesco; Chung, Jen Jen; Blum, Hermann; Siegwart, Roland; Ott, Lionel",,
OVGNet: An Unified Visual-Linguistic Framework for Open-Vocabulary Robotic Grasping,"Li, Meng; Zhao, Qi; Lyu, Shuchang; Wang, Chunlei; Ma, Yujing; Cheng, Guangliang; Yang, Chenguang",https://arxiv.org/abs/2407.13175,"Recognizing and grasping novel-category objects remains a crucial yet challenging problem in real-world robotic applications. Despite its significance, limited research has been conducted in this specific domain. To address this, we seamlessly propose a novel framework that integrates open-vocabulary learning into the domain of robotic grasping, empowering robots with the capability to adeptly handle novel objects. Our contributions are threefold. Firstly, we present a large-scale benchmark dataset specifically tailored for evaluating the performance of open-vocabulary grasping tasks. Secondly, we propose a unified visual-linguistic framework that serves as a guide for robots in successfully grasping both base and novel objects. Thirdly, we introduce two alignment modules designed to enhance visual-linguistic perception in the robotic grasping process. Extensive experiments validate the efficacy and utility of our approach. Notably, our framework achieves an average accuracy of 71.2\% and 64.4\% on base and novel categories in our new dataset, respectively."
OTVIC: A Dataset with Online Transmission for Vehicle-to-Infrastructure Cooperative 3D Object Detection,"Zhu, He; Wang, Yunkai; Kong, Quyu; Xia, Xunlong; Deng, Bing; Xiong, Rong; Wang, Yue",,
GroupTrack: Multi-Object Tracking by Using Group Motion Patterns,"Xu, Xinglong; Ren, Weihong; Sun, Gan; Ji, Haoyu; Gao, Yu; Liu, Honghai",,
A CT-guided Control Framework of a Robotic Flexible Endoscope for the Diagnosis of the Maxillary Sinusitis,"ZHU, Puchen; Zhang, Huayu; ma, xin; Zheng, Xiaoyin; Wang, Xuchen; Au, K. W. Samuel",,
ARDuP: Active Region Video Diffusion for Universal Policies,"Huang, Shuaiyi; Levy, Mara; Jiang, Zhenyu; Anandkumar, Anima; Zhu, Yuke; Fan, Linxi; Huang, De-An; Shrivastava, Abhinav",https://arxiv.org/abs/2406.13301,"Sequential decision-making can be formulated as a text-conditioned video generation problem, where a video planner, guided by a text-defined goal, generates future frames visualizing planned actions, from which control actions are subsequently derived. In this work, we introduce Active Region Video Diffusion for Universal Policies (ARDuP), a novel framework for video-based policy learning that emphasizes the generation of active regions, i.e. potential interaction areas, enhancing the conditional policy's focus on interactive areas critical for task execution. This innovative framework integrates active region conditioning with latent diffusion models for video planning and employs latent representations for direct action decoding during inverse dynamic modeling. By utilizing motion cues in videos for automatic active region discovery, our method eliminates the need for manual annotations of active regions. We validate ARDuP's efficacy via extensive experiments on simulator CLIPort and the real-world dataset BridgeData v2, achieving notable improvements in success rates and generating convincingly realistic video plans."
Proposal and Demonstration of a Robot Behavior Planning System Utilizing Video with an Open Source Model in Real-world Environments,"Akutsu, Yuki; Kato, Yuki; Yoshida, Takahiro; Sueoka, Yuichiro; Osuka, Koichi",,
ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer,"Ding, Tianye; Li, Hongyu; Jiang, Huaizu",https://arxiv.org/abs/2403.14626,"Obstacle detection and tracking represent a critical component in robot autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based model to address both obstacle detection and tracking problems. For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids. We further track the obstacles by matching the voxels between consecutive frames. The entire model can be optimized in an end-to-end manner. Through extensive experiments on DrivingStereo and KITTI benchmarks, our model achieves state-of-the-art performance in the obstacle detection task. We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less. The code and model weights will be publicly released."
MG-VLN: Benchmarking Multi-Goal and Long-Horizon Vision-Language Navigation with Language Enhanced Memory Map,"Zhang, Junbo; Ma, Kaisheng",,
RNR-Nav: A Real-World Visual Navigation System Using Renderable Neural Radiance Maps,"Kim, Minsoo; Kwon, Obin; Jun, Howoong; Oh, Songhwai",,
Environment Transformer and Policy Optimization for Model-Based Offline Reinforcement Learning,"Wang, Pengqin; Zhu, Meixin; Shen, Shaojie",https://arxiv.org/abs/2303.03811,"Interacting with the actual environment to acquire data is often costly and time-consuming in robotic tasks. Model-based offline reinforcement learning (RL) provides a feasible solution. On the one hand, it eliminates the requirements of interaction with the actual environment. On the other hand, it learns the transition dynamics and reward function from the offline datasets and generates simulated rollouts to accelerate training. Previous model-based offline RL methods adopt probabilistic ensemble neural networks (NN) to model aleatoric uncertainty and epistemic uncertainty. However, this results in an exponential increase in training time and computing resource requirements. Furthermore, these methods are easily disturbed by the accumulative errors of the environment dynamics models when simulating long-term rollouts. To solve the above problems, we propose an uncertainty-aware sequence modeling architecture called Environment Transformer. It models the probability distribution of the environment dynamics and reward function to capture aleatoric uncertainty and treats epistemic uncertainty as a learnable noise parameter. Benefiting from the accurate modeling of the transition dynamics and reward function, Environment Transformer can be combined with arbitrary planning, dynamics programming, or policy optimization algorithms for offline RL. In this case, we perform Conservative Q-Learning (CQL) to learn a conservative Q-function. Through simulation experiments, we demonstrate that our method achieves or exceeds state-of-the-art performance in widely studied offline RL benchmarks. Moreover, we show that Environment Transformer's simulated rollout quality, sample efficiency, and long-term rollout simulation capability are superior to those of previous model-based offline RL methods."
Driving Animatronic Robot Facial Expression From Speech,"Li, Boren; Li, Hang; Liu, Hangxin",https://arxiv.org/abs/2403.12670,"Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from speech input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient speech-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots' ability to replicate nuanced human expressions for natural interaction. To foster further research and development in this field, the code has been made publicly available at: \url{https://github.com/library87/OpenRoboExp}."
Pseudo-rigid body networks: learning interpretable deformable object dynamics from partial observations,"Mamedov, Shamil; Geist, Andreas René; Swevers, Jan; Trimpe, Sebastian",https://arxiv.org/abs/2307.07975,"Accurately predicting deformable linear object (DLO) dynamics is challenging, especially when the task requires a model that is both human-interpretable and computationally efficient. In this work, we draw inspiration from the pseudo-rigid body method (PRB) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. This dynamics network is trained jointly with a physics-informed encoder that maps observed motion variables to the DLO's hidden state. To encourage the state to acquire a physically meaningful representation, we leverage the forward kinematics of the PRB model as a decoder. We demonstrate in robot experiments that the proposed DLO dynamics model provides physically interpretable predictions from partial observations while being on par with black-box models regarding prediction accuracy. The project code is available at: http://tinyurl.com/prb-networks"
Adaptive Visual-Aided 4D Radar Odometry Through Transformer-Based Feature Fusion,"Zhang, Yuanfan; XIAO, Renxiang; Hong, Ziyang; Hu, Liang; Liu, Jie",,
Planning for Long-Term Monitoring Missions in Time-Varying Environments,"Stephens, Alex; Lacerda, Bruno; Hawes, Nick",,
Decentralized Multi-Robot Navigation Coupled with Spatial-Temporal RetNet Based on Deep Reinforcement Learning,"Chen, Lin; Wang, Yaonan; Miao, Zhiqiang; Feng, Mingtao; Wang, Yuanzhe; Mo, Yang; zhou, zhen; Wang, Hesheng; Wang, Danwei",,
Whole-body Compliance Control for Quadruped Manipulator with Actuation Saturation of Joint Torque and Ground Friction,"Zhang, Tianlin; Lin, Fenghao; Peng, Xuanbin; Xiong, Xiaogang; Lou, Yunjiang",,
Dynamic Modeling of Robotic Fish considering Background Flow using Koopman Operators,"Lin, Xiaozhu; LIU, Song; Wang, Yang",,
Data-Driven Modeling of Ground Effect For UAV Landing on a Vertical Oscillating Platform,"He, Binglin; Zhang, Heng; LIU, Song; Wang, Yang",,
Enhancing Reinforcement Learning in Sensor Fusion: A Comparative Analysis of Cubature and Sampling-based Integration Methods for Rover Search Planning,"Ewers, Jan-Hendrik; Swinton, Sarah; Anderson, Dave; McGookin, Euan William; Thomson, Douglas",https://arxiv.org/abs/2405.08691,"This study investigates the computational speed and accuracy of two numerical integration methods, cubature and sampling-based, for integrating an integrand over a 2D polygon. Using a group of rovers searching the Martian surface with a limited sensor footprint as a test bed, the relative error and computational time are compared as the area was subdivided to improve accuracy in the sampling-based approach. The results show that the sampling-based approach exhibits a $14.75\%$ deviation in relative error compared to cubature when it matches the computational performance at $100\%$. Furthermore, achieving a relative error below $1\%$ necessitates a $10000\%$ increase in relative time to calculate due to the $\mathcal{O}(N^2)$ complexity of the sampling-based method. It is concluded that for enhancing reinforcement learning capabilities and other high iteration algorithms, the cubature method is preferred over the sampling-based method."
Safe Imitation Learning of Nonlinear Model Predictive Control for Flexible Robots,"Mamedov, Shamil; Reiter, Rudolf; Basiri Azad, Seyed Mahdi; Viljoen, Ruan Matthys; Boedecker, Joschka; Diehl, Moritz; Swevers, Jan",https://arxiv.org/abs/2212.02941,"Flexible robots may overcome some of the industry's major challenges, such as enabling intrinsically safe human-robot collaboration and achieving a higher payload-to-mass ratio. However, controlling flexible robots is complicated due to their complex dynamics, which include oscillatory behavior and a high-dimensional state space. Nonlinear model predictive control (NMPC) offers an effective means to control such robots, but its significant computational demand often limits its application in real-time scenarios. To enable fast control of flexible robots, we propose a framework for a safe approximation of NMPC using imitation learning and a predictive safety filter. Our framework significantly reduces computation time while incurring a slight loss in performance. Compared to NMPC, our framework shows more than an eightfold improvement in computation time when controlling a three-dimensional flexible robot arm in simulation, all while guaranteeing safety constraints. Notably, our approach outperforms state-of-the-art reinforcement learning methods. The development of fast and safe approximate NMPC holds the potential to accelerate the adoption of flexible robots in industry. The project code is available at: tinyurl.com/anmpc4fr"
Are Large Language Models Aligned with People's Social Intuitions for HumanRobot Interactions?,"Wachowiak, Lennart; Coles, Andrew; Celiktutan, Oya; Canal, Gerard",,
Conditional Variational Autoencoders for Probabilistic Pose Regression,"Zangeneh, Fereidoon; Bruns, Leonard; Dekel, Amit; Pieropan, Alessandro; Jensfelt, Patric",,
Kiri-Spoon: A Soft Shape-Changing Utensil for Robot-Assisted Feeding,"Keely, Maya; Nemlekar, Heramb; Losey, Dylan",https://arxiv.org/abs/2403.05784,"Assistive robot arms have the potential to help disabled or elderly adults eat everyday meals without relying on a caregiver. To provide meaningful assistance, these robots must reach for food items, pick them up, and then carry them to the human's mouth. Current work equips robot arms with standard utensils (e.g., forks and spoons). But -- although these utensils are intuitive for humans -- they are not easy for robots to control. If the robot arm does not carefully and precisely orchestrate its motion, food items may fall out of a spoon or slide off of the fork. Accordingly, in this paper we design, model, and test Kiri-Spoon, a novel utensil specifically intended for robot-assisted feeding. Kiri-Spoon combines the familiar shape of traditional utensils with the capabilities of soft grippers. By actuating a kirigami structure the robot can rapidly adjust the curvature of Kiri-Spoon: at one extreme the utensil wraps around food items to make them easier for the robot to pick up and carry, and at the other extreme the utensil returns to a typical spoon shape so that human users can easily take a bite of food. Our studies with able-bodied human operators suggest that robot arms equipped with Kiri-Spoon carry foods more robustly than when leveraging traditional utensils. See videos here: https://youtu.be/nddAniZLFPk"
FRAGG-Map: Frustum Accelerated GPU-Based Grid Map,"Grimaldi, Michele; Palomeras, Narcis; Carlucho, Ignacio; Petillot, Yvan R.; Ridao, Pere",,
VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation,"Wang, Weiyao; Lei, Yutian; Jin, Shiyu; Hager, Gregory; Zhang, Liangjun",https://arxiv.org/abs/2403.11461,"In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a novel method designed to enhance 3D manipulation capabilities through action-aware view rendering. VIHE autoregressively refines actions in multiple stages by conditioning on rendered views posed from action predictions in the earlier stages. These virtual in-hand views provide a strong inductive bias for effectively recognizing the correct pose for the hand, especially for challenging high-precision tasks such as peg insertion. On 18 manipulation tasks in RLBench simulated environments, VIHE achieves a new state-of-the-art, with a 12% absolute improvement, increasing from 65% to 77% over the existing state-of-the-art model using 100 demonstrations per task. In real-world scenarios, VIHE can learn manipulation tasks with just a handful of demonstrations, highlighting its practical utility. Videos and code implementation can be found at our project site: https://vihe-3d.github.io."
Asynchronous Spatial-Temporal Allocation for Trajectory Planning of Heterogeneous Multi-Agent Systems,"Chen, Yuda; Dong, Haoze; Li, Zhongkui",https://arxiv.org/abs/2309.07431,"To plan the trajectories of a large-scale heterogeneous swarm, sequentially or synchronously distributed methods usually become intractable due to the lack of global clock synchronization. To this end, we provide a novel asynchronous spatial-temporal allocation method. Specifically, between a pair of agents, the allocation is proposed to determine their corresponding derivable time-stamped space and can be updated in an asynchronous way, by inserting a waiting duration between two consecutive replanning steps. Via theoretical analysis, the inter-agent collision is proved to be avoided and the allocation ensures timely updates. Comprehensive simulations and comparisons with five baselines validate the effectiveness of the proposed method and illustrate its improvement in completion time and moving distance. Finally, hardware experiments are carried out, where $8$ heterogeneous unmanned ground vehicles with onboard computation navigate in cluttered scenarios with high agility."
Climbing Gait for a Snake Robot by Adapting to a Flexible Net,"Yoshida, Kodai; Tanaka, Motoyasu",,
Hierarchical Action Chunking Transformer: Learning Temporal Multimodality from Demonstrations with Fast Imitation Behavior,"Park, J. hyeon; Choi, Wonhyuk; Hong, Sunpyo; Seo, Hoseong; Ahn, Joonmo; Ha, Changsu; Han, Heungwoo; Kwon, Junghyun",,
DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision,"Hu, Yutong; Wen, Kehan; Yu, Fisher",https://arxiv.org/abs/2403.14300,"Learning dexterous locomotion policy for legged robots is becoming increasingly popular due to its ability to handle diverse terrains and resemble intelligent behaviors. However, joint manipulation of moving objects and locomotion with legs, such as playing soccer, receive scant attention in the learning community, although it is natural for humans and smart animals. A key challenge to solve this multitask problem is to infer the objectives of locomotion from the states and targets of the manipulated objects. The implicit relation between the object states and robot locomotion can be hard to capture directly from the training experience. We propose adding a feedback control block to compute the necessary body-level movement accurately and using the outputs as dynamic joint-level locomotion supervision explicitly. We further utilize an improved ball dynamic model, an extended context-aided estimator, and a comprehensive ball observer to facilitate transferring policy learned in simulation to the real world. We observe that our learning scheme can not only make the policy network converge faster but also enable soccer robots to perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a capability that was lacking in previous methods. Video and code are available at https://github.com/SysCV/soccer-player"
Reconfigurable Robot Identification from Motion Data,"Hu, Yuhang; Wang, Yunzhe; Liu, Ruibo; Shen, Zhou; Lipson, Hod",https://arxiv.org/abs/2403.10496,"Integrating Large Language Models (VLMs) and Vision-Language Models (VLMs) with robotic systems enables robots to process and understand complex natural language instructions and visual information. However, a fundamental challenge remains: for robots to fully capitalize on these advancements, they must have a deep understanding of their physical embodiment. The gap between AI models cognitive capabilities and the understanding of physical embodiment leads to the following question: Can a robot autonomously understand and adapt to its physical form and functionalities through interaction with its environment? This question underscores the transition towards developing self-modeling robots without reliance on external sensory or pre-programmed knowledge about their structure. Here, we propose a meta self modeling that can deduce robot morphology through proprioception (the internal sense of position and movement). Our study introduces a 12 DoF reconfigurable legged robot, accompanied by a diverse dataset of 200k unique configurations, to systematically investigate the relationship between robotic motion and robot morphology. Utilizing a deep neural network model comprising a robot signature encoder and a configuration decoder, we demonstrate the capability of our system to accurately predict robot configurations from proprioceptive signals. This research contributes to the field of robotic self-modeling, aiming to enhance understanding of their physical embodiment and adaptability in real world scenarios."
SWCF-Net: Similarity-Weighted Convolution and Local-Global Fusion for Efficient Large-Scale Point Cloud Semantic Segmentation,"Lin, Zhenchao; He, Li; Yang, Hongqiang; xiaoqun, sun; zhang, guojin; Chen, Weinan; Guan, Yisheng; Zhang, Hong",https://arxiv.org/abs/2406.11441,"Large-scale point cloud consists of a multitude of individual objects, thereby encompassing rich structural and underlying semantic contextual information, resulting in a challenging problem in efficiently segmenting a point cloud. Most existing researches mainly focus on capturing intricate local features without giving due consideration to global ones, thus failing to leverage semantic context. In this paper, we propose a Similarity-Weighted Convolution and local-global Fusion Network, named SWCF-Net, which takes into account both local and global features. We propose a Similarity-Weighted Convolution (SWConv) to effectively extract local features, where similarity weights are incorporated into the convolution operation to enhance the generalization capabilities. Then, we employ a downsampling operation on the K and V channels within the attention module, thereby reducing the quadratic complexity to linear, enabling the Transformer to deal with large-scale point clouds. At last, orthogonal components are extracted in the global features and then aggregated with local features, thereby eliminating redundant information between local and global features and consequently promoting efficiency. We evaluate SWCF-Net on large-scale outdoor datasets SemanticKITTI and Toronto3D. Our experimental results demonstrate the effectiveness of the proposed network. Our method achieves a competitive result with less computational cost, and is able to handle large-scale point clouds efficiently."
GRID: Scene-Graph-based Instruction-driven Robotic Task Planning,"Ni, Zhe; Deng, Xiaoxin; Tai, Cong; Zhu, Xinyue; Xie, Qinghongbing; Huang, Weihang; wu, xiang; Zeng, Long",https://arxiv.org/abs/2309.07726,"Recent works have shown that Large Language Models (LLMs) can facilitate the grounding of instructions for robotic task planning. Despite this progress, most existing works have primarily focused on utilizing raw images to aid LLMs in understanding environmental information. However, this approach not only limits the scope of observation but also typically necessitates extensive multimodal data collection and large-scale models. In this paper, we propose a novel approach called Graph-based Robotic Instruction Decomposer (GRID), which leverages scene graphs instead of images to perceive global scene information and iteratively plan subtasks for a given instruction. Our method encodes object attributes and relationships in graphs through an LLM and Graph Attention Networks, integrating instruction features to predict subtasks consisting of pre-defined robot actions and target objects in the scene graph. This strategy enables robots to acquire semantic knowledge widely observed in the environment from the scene graph. To train and evaluate GRID, we establish a dataset construction pipeline to generate synthetic datasets for graph-based robotic task planning. Experiments have shown that our method outperforms GPT-4 by over 25.4% in subtask accuracy and 43.6% in task accuracy. Moreover, our method achieves a real-time speed of 0.11s per inference. Experiments conducted on datasets of unseen scenes and scenes with varying numbers of objects demonstrate that the task accuracy of GRID declined by at most 3.8%, showcasing its robust cross-scene generalization ability. We validate our method in both physical simulation and the real world. More details can be found on the project page https://jackyzengl.github.io/GRID.github.io/."
A Decentralized Partially Observable Markov Decision Process for Dynamic Obstacle Avoidance and Complete Area Coverage using Multiple Reconfigurable Robots,"Pey, Javier Jia Jie; Samarakoon Mudiyanselage, Bhagya Prasangi Samarakoon; Muthugala Arachchige, Viraj Jagathpriya Muthugala; Elara, Mohan Rajesh",,
Modeling of Hydraulic Soft Hand with Rubber Sheet Reservoir and Evaluation of its Grasping Flexibility and Control,"Ishibashi, Kyosuke; Ishikawa, Hiroki; Azami, Osamu; Yamamoto, Ko",,
GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration,"Tahiraj, Ilir; Fent, Felix; Hafemann, Philipp; Ye, Egon; lienkamp, Markus",https://arxiv.org/abs/2404.03427,"State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants. These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization. This often leads to misalignments in the calibration process. Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations. This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems. Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms. We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment. We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results. Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms. The code is open source and available on GitHub."
SDPL-SLAM: Introducing Lines in Dynamic Visual SLAM and Multi-Object Tracking,"Manetas, Argyris; Mermigkas, Panagiotis; Maragos, Petros",,
Online Adaptive Impedance Control with Gravity Compensation for an Interactive Lower-Limb Exoskeleton,"Janna, Run; Tarapongnivat, Kanut; Sricom, Natchaya; Akkawutvanich, Akkawutvanich; Xiong, Xiaofeng; Manoonpong, Poramate",,
Neuromorphic force-control in an industrial task: validating energy and latency benefits,"Amaya, Camilo; Eames, Evan; Palinauskas, Gintautas; Perzylo, Alexander Clifford; Sandamirskaya, Yulia; von Arnim, Axel",https://arxiv.org/abs/2403.08928,"As robots become smarter and more ubiquitous, optimizing the power consumption of intelligent compute becomes imperative towards ensuring the sustainability of technological advancements. Neuromorphic computing hardware makes use of biologically inspired neural architectures to achieve energy and latency improvements compared to conventional von Neumann computing architecture. Applying these benefits to robots has been demonstrated in several works in the field of neurorobotics, typically on relatively simple control tasks. Here, we introduce an example of neuromorphic computing applied to the real-world industrial task of object insertion. We trained a spiking neural network (SNN) to perform force-torque feedback control using a reinforcement learning approach in simulation. We then ported the SNN to the Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At inference time we show latency competitive with current CPU/GPU architectures, and one order of magnitude less energy usage in comparison to state-of-the-art low-energy edge-hardware. We offer this example as a proof of concept implementation of a neuromoprhic controller in real-world robotic setting, highlighting the benefits of neuromorphic hardware for the development of intelligent controllers for robots."
Wheelchair Maneuvering with a Single-Spherical-Wheeled Balancing Mobile Manipulator,"Dai, Cunxi; Liu, Xiaohan; Shu, Roberto; Hollis, Ralph",https://arxiv.org/abs/2404.13206,"In this work, we present a control framework to effectively maneuver wheelchairs with a dynamically stable mobile manipulator. Wheelchairs are a type of nonholonomic cart system, maneuvering such systems with mobile manipulators (MM) is challenging mostly due to the following reasons: 1) These systems feature nonholonomic constraints and considerably varying inertial parameters that require online identification and adaptation. 2) These systems are widely used in human-centered environments, which demand the MM to operate in potentially crowded spaces while ensuring compliance for safe physical human-robot interaction (pHRI). We propose a control framework that plans whole-body motion based on quasi-static analysis to maneuver heavy nonholonomic carts while maintaining overall compliance. We validated our approach experimentally by maneuvering a wheelchair with a bimanual mobile manipulator, the CMU ballbot. The experiments demonstrate the proposed framework is able to track desired wheelchair velocity with loads varying from 11.8 kg to 79.4 kg at a maximum linear velocity of 0.45 m/s and angular velocity of 0.3 rad/s. Furthermore, we verified that the proposed method can generate human-like motion smoothness of the wheelchair while ensuring safe interactions with the environment."
Multi-Robot Path Planning with Boolean Specification Tasks under Motion Uncertainties,"Zhang, Zhe; He, Zhou; Ran, Ning; Reniers, Michel",,
Blending Distributed NeRFs with Tri-stage Robust Pose Optimization,"YE, Baijun; Liu, Caiyun; Xiaoyu, Ye; Chen, Yuantao; Wang, Yuhai; Yan, Zike; Shi, Yongliang; Zhao, Hao; Zhou, Guyue",https://arxiv.org/abs/2405.02880,"Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at https://github.com/boilcy/Distributed-NeRF."
Towards More Accurate Dynamics and Reward Modeling for Model-Based Offline Inverse Reinforcement Learning,"Zhang, Gengyu; Yan, Yan",,
"CASRL: Collision Avoidance with Spiking Reinforcement Learning Among Dynamic, Decision-Making Agents","Zhang, Chengjun; Yip, Ka-Wa; Yang, Bo; Zhang, Zhiyong; Yuan, Mengwen; Tang, Huajin",,
Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for Cooperative Push Manipulation by Mobile Robots,"Kuroki, So; Nishimura, Mai; Kozuno, Tadashi",https://arxiv.org/abs/2312.02008,"Due to the complex interactions between agents, learning multi-agent control policy often requires a prohibited amount of data. This paper aims to enable multi-agent systems to effectively utilize past memories to adapt to novel collaborative tasks in a data-efficient fashion. We propose the Multi-Agent Coordination Skill Database, a repository for storing a collection of coordinated behaviors associated with key vectors distinctive to them. Our Transformer-based skill encoder effectively captures spatio-temporal interactions that contribute to coordination and provides a unique skill representation for each coordinated behavior. By leveraging only a small number of demonstrations of the target task, the database enables us to train the policy using a dataset augmented with the retrieved demonstrations. Experimental evaluations demonstrate that our method achieves a significantly higher success rate in push manipulation tasks compared with baseline methods like few-shot imitation learning. Furthermore, we validate the effectiveness of our retrieve-and-learn framework in a real environment using a team of wheeled robots."
I2EKF-LO: A Dual-Iteration Extended Kalman Filter Based LiDAR Odometry,"Yu, Wenlu; Xu, Jie; Zhao, Chengwei; zhao, lijun; Nguyen, Thien-Minh; Yuan, Shenghai; Bai, Mingming; Xie, Lihua",https://arxiv.org/abs/2407.02190,"LiDAR odometry is a pivotal technology in the fields of autonomous driving and autonomous mobile robotics. However, most of the current works focus on nonlinear optimization methods, and still existing many challenges in using the traditional Iterative Extended Kalman Filter (IEKF) framework to tackle the problem: IEKF only iterates over the observation equation, relying on a rough estimate of the initial state, which is insufficient to fully eliminate motion distortion in the input point cloud; the system process noise is difficult to be determined during state estimation of the complex motions; and the varying motion models across different sensor carriers. To address these issues, we propose the Dual-Iteration Extended Kalman Filter (I2EKF) and the LiDAR odometry based on I2EKF (I2EKF-LO). This approach not only iterates over the observation equation but also leverages state updates to iteratively mitigate motion distortion in LiDAR point clouds. Moreover, it dynamically adjusts process noise based on the confidence level of prior predictions during state estimation and establishes motion models for different sensor carriers to achieve accurate and efficient state estimation. Comprehensive experiments demonstrate that I2EKF-LO achieves outstanding levels of accuracy and computational efficiency in the realm of LiDAR odometry. Additionally, to foster community development, our code is open-sourced.https://github.com/YWL0720/I2EKF-LO."
Semantic SLAM Fusing Moving Constraint for Dynamic Objects under Indoor Environments,"Yang, Zhenyuan; Rishan Sachinthana, Wijenayaka Kankanamge; Samarakoon Mudiyanselage, Bhagya Prasangi Samarakoon; Elara, Mohan Rajesh",,
"C3P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel Mapping","Yang, Xu; Wenhao, Li; Ge, Qijie; Suo, Lulu; Tang, Weijie; WEI, ZHENGYU; Huang, Long-Xiang; Wang, Bo",,
Bistable valve for electronics-free soft robots,"Kan, Longxin; Lam, Jia Qing Joshua; Qin, Zhihang; Li, Keyi; Tang, Zhiqiang; Laschi, Cecilia",https://arxiv.org/abs/2110.01743,"Although research studies in pneumatic soft robots develop rapidly, most pneumatic actuators are still controlled by rigid valves and conventional electronics. The existence of these rigid, electronic components sacrifices the compliance and adaptability of soft robots.} Current electronics-free valve designs based on soft materials are facing challenges in behaviour consistency, design flexibility, and fabrication complexity. Taking advantages of soft material 3D printing, this paper presents a new design of a bi-stable pneumatic valve, which utilises two soft, pneumatically-driven, and symmetrically-oriented conical shells with structural bistability to stabilise and regulate the airflow. The critical pressure required to operate the valve can be adjusted by changing the design features of the soft bi-stable structure. Multi-material printing simplifies the valve fabrication, enhances the flexibility in design feature optimisations, and improves the system repeatability. In this work, both a theoretical model and physical experiments are introduced to examine the relationships between the critical operating pressure and the key design features. Results with valve characteristic tuning via material stiffness changing show better effectiveness compared to the change of geometry design features (demonstrated largest tunable critical pressure range from 15.3 to 65.2 kPa and fastest response time $\leq$ 1.8 s."
Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation,"Taioli, Francesco; Rosa, Stefano; Castellini, Alberto; Natale, Lorenzo; Del Bue, Alessio; Farinelli, Alessandro; Cristani, Marco; Wang, Yiming",https://arxiv.org/abs/2403.10700,"Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable performance drop (up to -25%) in Success Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark. Moreover, we formally define the task of Instruction Error Detection and Localization, and establish an evaluation protocol on top of our benchmark dataset. We also propose an effective method, based on a cross-modal transformer architecture, that achieves the best performance in error detection and localization, compared to baselines. Surprisingly, our proposed method has revealed errors in the validation set of the two commonly used datasets for VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in other tasks. Code and dataset will be made available upon acceptance at https://intelligolabs.github.io/R2RIE-CE"
PreAfford: An Affordance-based Pre-grasping Framework with high Adaptability,"Ding, Kairui; Chen, Boyuan; Wu, Ruihai; Li, Yuyang; Zhang, Zongzheng; Gao, Huan-ang; Li, Siqi; Zhu, Yixin; Zhou, Guyue; Dong, Hao; Zhao, Hao",,
NRDF - Neural Region Descriptor Fields as Implicit ROI Representation for Robotic 3D Surface Processing,"Pratheepkumar, Anish; Ikeda, Markus; Hofmann, Michael; Widmoser, Fabian; Pichler, Andreas; Vincze, Markus",,
Force and Velocity Prediction in Human-Robot Collaborative Transportation Tasks through Video Retentive Networks,"Dominguez-Vidal, Jose Enrique; Sanfeliu, Alberto",,
Time-Ordered Ad-hoc Resource Sharing for Independent Robotic Agents,"Chakravarty, Arjo; Grey, Michael; Muthugala Arachchige, Viraj Jagathpriya Muthugala; Elara, Mohan Rajesh",https://arxiv.org/abs/2408.07942,Resource sharing is a crucial part of a multi-robot system. We propose a Boolean satisfiability based approach to resource sharing. Our key contributions are an algorithm for converting any constrained assignment to a weighted-SAT based optimization. We propose a theorem that allows optimal resource assignment problems to be solved via repeated application of a SAT solver. Additionally we show a way to encode continuous time ordering constraints using Conjunctive Normal Form (CNF). We benchmark our new algorithms and show that they can be used in an ad-hoc setting. We test our algorithms on a fleet of simulated and real world robots and show that the algorithms are able to handle real world situations. Our algorithms and test harnesses are opensource and build on Open-RMFs fleet management system.
RelationGrasp: Object-Oriented Prompt Learning for Simultaneously Grasp Detection and Manipulation Relationship in Open Vocabulary,"Liu, Songting; Teo, Tat Joo; Lin, Zhiping; Zhu, Haiyue",,
FOCWS: A High Sensitive Flexible Optical Curvature Sensor Inspired by Arthropod Sensory Systems,"Wei, Jiachen; Li, Zhengwei; Liu, Zeyu; Cheng, Long",,
SNF-Feat: Semantic-Guided Negative-Sample-Free Representation Learning for Local Feature Extraction,"Zhou, Xun; Yan, Qingqing; Zhu, Minghao; hu, mengxian; Liu, Chengju; Chen, Qijun",,
QTrack: Embracing Quality Clues for Robust 3D Multi-Object Tracking,"Yang, Jinrong; Yu, En; Li, Zeming; Li, Xiaoping; Tao, Wenbing",https://arxiv.org/abs/2208.10976,"3D Multi-Object Tracking (MOT) has achieved tremendous achievement thanks to the rapid development of 3D object detection and 2D MOT. Recent advanced works generally employ a series of object attributes, e.g., position, size, velocity, and appearance, to provide the clues for the association in 3D MOT. However, these cues may not be reliable due to some visual noise, such as occlusion and blur, leading to tracking performance bottleneck. To reveal the dilemma, we conduct extensive empirical analysis to expose the key bottleneck of each clue and how they correlate with each other. The analysis results motivate us to efficiently absorb the merits among all cues, and adaptively produce an optimal tacking manner. Specifically, we present Location and Velocity Quality Learning, which efficiently guides the network to estimate the quality of predicted object attributes. Based on these quality estimations, we propose a quality-aware object association (QOA) strategy to leverage the quality score as an important reference factor for achieving robust association. Despite its simplicity, extensive experiments indicate that the proposed strategy significantly boosts tracking performance by 2.2% AMOTA and our method outperforms all existing state-of-the-art works on nuScenes by a large margin. Moreover, QTrack achieves 48.0% and 51.1% AMOTA tracking performance on the nuScenes validation and test sets, which significantly reduces the performance gap between pure camera and LiDAR based trackers."
Evidential Semantic Mapping in Off-road Environments with Uncertainty-aware Bayesian Kernel Inference,"Kim, Junyoung; Seo, Junwon; Min, Jihong",https://arxiv.org/abs/2403.14138,"Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in creating semantic maps by effectively leveraging local spatial information. However, existing semantic mapping methods face challenges in constructing reliable maps in unstructured outdoor scenarios due to unreliable semantic predictions. To address this issue, we propose an evidential semantic mapping, which can enhance reliability in perceptually challenging off-road environments. We integrate Evidential Deep Learning into the semantic segmentation network to obtain the uncertainty estimate of semantic prediction. Subsequently, this semantic uncertainty is incorporated into an uncertainty-aware BKI, tailored to prioritize more confident semantic predictions when accumulating semantic information. By adaptively handling semantic uncertainties, the proposed framework constructs robust representations of the surroundings even in previously unseen environments. Comprehensive experiments across various off-road datasets demonstrate that our framework enhances accuracy and robustness, consistently outperforming existing methods in scenes with high perceptual uncertainties."
LV-MOTPO: Multi-object Tracking and Pose Optimization Using LiDAR and Camera,"Wang, TingTing; Zhang, Yunzhou; Yang, Linghao; Lv, Yuezhang; Li, Wu; Zhang, Jinpeng",,
Procedural generation of tunnel networks for unsupervised training and testing in underground applications.,"Cano, Lorenzo; Mosteo, Alejandro R.; Tardioli, Danilo",,
A Facile one-step injection novel composite sensor for robot tactile assistance,"Zhang, Yuyin; Wang, Yue; Liu, Na; Zhong, Songyi; Li, Long; Zhang, Quan; Yue, Tao; Fukuda, Toshio",,
Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation,"Röfer, Adrian; Nematollahi, Iman; Welschehold, Tim; Burgard, Wolfram; Valada, Abhinav",https://arxiv.org/abs/2403.14305,"Sample efficient learning of manipulation skills poses a major challenge in robotics. While recent approaches demonstrate impressive advances in the type of task that can be addressed and the sensing modalities that can be incorporated, they still require large amounts of training data. Especially with regard to learning actions on robots in the real world, this poses a major problem due to the high costs associated with both demonstrations and real-world robot interactions. To address this challenge, we introduce BOpt-GMM, a hybrid approach that combines imitation learning with own experience collection. We first learn a skill model as a dynamical system encoded in a Gaussian Mixture Model from a few demonstrations. We then improve this model with Bayesian optimization building on a small number of autonomous skill executions in a sparse reward setting. We demonstrate the sample efficiency of our approach on multiple complex manipulation skills in both simulations and real-world experiments. Furthermore, we make the code and pre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de."
Distilling Knowledge for Short-to-Long Term Trajectory Prediction,"Das, Sourav; Camporese, Guglielmo; Cheng, Shaokang; Ballan, Lamberto",https://arxiv.org/abs/2305.08553,"Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The teacher's task is less uncertain, and we use its accurate predictions to guide the student through our knowledge distillation framework, reducing long-term future uncertainty. Our experiments show that our proposed Di-Long method is effective for long-term forecasting and achieves state-of-the-art performance on the Intersection Drone Dataset (inD) and the Stanford Drone Dataset (SDD)."
Streamlining Object Pushing: Behavior Tree-Based Coordination of Control and Planning,"Bertoncelli, Filippo; Sabattini, Lorenzo",,
AutoNeRF: Training Implicit Scene Representations with Autonomous Agents,"Marza, Pierre; Matignon, Laetitia; Simonin, Olivier; Batra, Dhruv; Wolf, Christian; Chaplot, Devendra Singh",https://arxiv.org/abs/2304.11241,"Implicit representations such as Neural Radiance Fields (NeRF) have been shown to be very effective at novel view synthesis. However, these models typically require manual and careful human data collection for training. In this paper, we present AutoNeRF, a method to collect data required to train NeRFs using autonomous embodied agents. Our method allows an agent to explore an unseen environment efficiently and use the experience to build an implicit map representation autonomously. We compare the impact of different exploration strategies including handcrafted frontier-based exploration, end-to-end and modular approaches composed of trained high-level planners and classical low-level path followers. We train these models with different reward functions tailored to this problem and evaluate the quality of the learned representations on four different downstream tasks: classical viewpoint rendering, map reconstruction, planning, and pose refinement. Empirical results show that NeRFs can be trained on actively collected data using just a single episode of experience in an unseen environment, and can be used for several downstream robotic tasks, and that modular trained exploration models outperform other classical and end-to-end baselines. Finally, we show that AutoNeRF can reconstruct large-scale scenes, and is thus a useful tool to perform scene-specific adaptation as the produced 3D environment models can be loaded into a simulator to fine-tune a policy of interest."
Speeding up 6-DoF Grasp Sampling with Quality-Diversity,"Huber, Johann; Hélénon, François; Kappel, Mathilde; Chelly, Elie; Khoramshahi, Mahdi; BEN AMAR, Faiz; Doncieux, Stéphane",https://arxiv.org/abs/2403.06173,"Recent advances in AI have led to significant results in robotic learning, including natural language-conditioned planning and efficient optimization of controllers using generative models. However, the interaction data remains the bottleneck for generalization. Getting data for grasping is a critical challenge, as this skill is required to complete many manipulation tasks. Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse, high-performing solutions to a given problem. This paper investigates how QD can be combined with priors to speed up the generation of diverse grasps poses in simulation compared to standard 6-DoF grasp sampling schemes. Experiments conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD outperforms commonly used methods by a large margin. Further experiments show that QD optimization automatically finds some efficient priors that are usually hard coded. The deployment of generated grasps on a 2-finger gripper and an Allegro hand shows that the diversity produced maintains sim-to-real transferability. We believe these results to be a significant step toward the generation of large datasets that can lead to robust and generalizing robotic grasping policies."
Data-driven Force Observer for Human-Robot Interaction with Series Elastic Actuators using Gaussian Processes,"Tesfazgi, Samuel; Keßler, Markus; Trigili, Emilio; Lederer, Armin; Hirche, Sandra",https://arxiv.org/abs/2405.08711,"Ensuring safety and adapting to the user's behavior are of paramount importance in physical human-robot interaction. Thus, incorporating elastic actuators in the robot's mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components. While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments. In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression. By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate. Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications. We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario."
Solving Multi-Robot Task Allocation and Planning in Trans-media Scenarios,"de La Rochefoucauld, Virgile; Lacroix, Simon; Ratsamee, Photchara; Takemura, Haruo",,
Smooth Invariant Interpolation on Lie groups with Prescribed Terminal Conditions for Robot Motion Planning and Modeling of Soft Robots,"Mueller, Andreas; Gattringer, Hubert; Marauli, Tobias",,
RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning,"Trumpp, Raphael; Javanmardi, Ehsan; Nakazato, Jin; Tsukada, Manabu; Caccamo, Marco",https://arxiv.org/abs/2403.07129,"The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. Accordingly, this paper introduces RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that depend on predefined racing lines, RaceMOP operates without a map, relying solely on local observations to overtake other race cars at high speed. Our approach combines an artificial potential field method as a base policy with residual policy learning to introduce long-horizon planning capabilities. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Our experiments for twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners while generalizing to unknown racetracks, paving the way for further use of our method in robotics. We make the open-source code for RaceMOP available at http://github.com/raphajaner/racemop."
SCP: Soft Conditional Prompt Learning for Aerial Video Action Recognition,"Wang, Xijun; Xian, Ruiqi; Guan, Tianrui; Liu, Fuxiao; Manocha, Dinesh",https://arxiv.org/abs/2305.12437,"We present a new learning approach, Soft Conditional Prompt Learning (SCP), which leverages the strengths of prompt learning for aerial video action recognition. Our approach is designed to predict the action of each agent by helping the models focus on the descriptions or instructions associated with actions in the input videos for aerial/robot visual perception. Our formulation supports various prompts, including learnable prompts, auxiliary visual information, and large vision models to improve the recognition performance. We present a soft conditional prompt method that learns to dynamically generate prompts from a pool of prompt experts under different video inputs. By sharing the same objective with the task, our proposed SCP can optimize prompts that guide the model's predictions while explicitly learning input-invariant (prompt experts pool) and input-specific (data-dependent) prompt knowledge. In practice, we observe a 3.17-10.2% accuracy improvement on the aerial video datasets (Okutama, NECDrone), which consist of scenes with single-agent and multi-agent actions. We further evaluate our approach on ground camera videos to verify the effectiveness and generalization and achieve a 1.0-3.6% improvement on dataset SSV2. We integrate our method into the ROS2 as well."
Scalability of Platoon-based Coordination for Mixed Autonomy Intersections,"Yan, Zhongxia; Wu, Cathy",,
Contextual Emotion Recognition using Large Vision Language Models,"Etesam, Yasaman; Yalcin, Ozge; Zhang, Chuxuan; Lim, Angelica",https://arxiv.org/abs/2405.08992,"""How does the person in the bounding box feel?"" Achieving human-level recognition of the apparent emotion of a person in real world situations remains an unsolved task in computer vision. Facial expressions are not enough: body pose, contextual knowledge, and commonsense reasoning all contribute to how humans perform this emotional theory of mind task. In this paper, we examine two major approaches enabled by recent large vision language models: 1) image captioning followed by a language-only LLM, and 2) vision language models, under zero-shot and fine-tuned setups. We evaluate the methods on the Emotions in Context (EMOTIC) dataset and demonstrate that a vision language model, fine-tuned even on a small dataset, can significantly outperform traditional baselines. The results of this work aim to help robots and agents perform emotionally sensitive decision-making and interaction in the future."
The Design of the Barkour Benchmark for Robot Agility,"Yu, Wenhao; Caluwaerts, Ken; Iscen, Atil; Kew, J. Chase; Zhang, Tingnan; Freeman, Daniel; Lee, Lisa; Saliceti, Stefano; Zhuang, Vincent; Batchelor, Nathan; Bohez, Steven; Casarini, Federico; CHEN, Jose Enrique; Coumans, Erwin; Dostmohamed, Adil; Dulac-Arnold, Gabriel; Escontrela, Alejandro; Frey, Erik; Hafner, Roland; Jain, Deepali; Jyenis, Bauyrjan; Kuang, Yuheng; Lee, Edward; Nachum, Ofir; Oslund, Kenneth; Romano, Francesco; Sadeghi, Fereshteh; Tabanpour, Baruch; Zheng, Daniel; Neunert, Michael; Hadsell, Raia; Heess, Nicolas; Nori, Francesco; Seto, Jeff; Parada, Carolina; Sindhwani, Vikas; Vanhoucke, Vincent; Tan, Jie; Lee, Kuang-Huei",,
Estimating the Joint Angles of a Magnetic Surgical Tool using Monocular 3D Keypoint Detection and Particle Filtering,"Fredin, Erik; Diller, Eric D.",,
Robot Swarm Control Based on Smoothed Particle Hydrodynamics for Obstacle-Unaware Navigation,"Eguchi, Michikuni; Nishimura, Mai; Yoshida, Shigeo; Hiraki, Takefumi",https://arxiv.org/abs/2404.16309,"Robot swarms hold immense potential for performing complex tasks far beyond the capabilities of individual robots. However, the challenge in unleashing this potential is the robots' limited sensory capabilities, which hinder their ability to detect and adapt to unknown obstacles in real-time. To overcome this limitation, we introduce a novel robot swarm control method with an indirect obstacle detector using a smoothed particle hydrodynamics (SPH) model. The indirect obstacle detector can predict the collision with an obstacle and its collision point solely from the robot's velocity information. This approach enables the swarm to effectively and accurately navigate environments without the need for explicit obstacle detection, significantly enhancing their operational robustness and efficiency. Our method's superiority is quantitatively validated through a comparative analysis, showcasing its significant navigation and pattern formation improvements under obstacle-unaware conditions."
Segmented Safety Docking Control for Mobile Self-Reconfigurable Robots,"Zheng, Zhi; Jiang, Tao; Tan, Senqi; Zhang, Hao; Ye, Jianchuan",,
Tracking Control with Uncertainty Smoothing Estimation under Aggressive Maneuvers of Aerial Vehicles,"Zhang, Hao; Jiang, Tao; Tan, Senqi; Ye, Jianchuan; Zheng, Zhi",,
Design and Control of a Novel Soft-Rigid Lower Limb Exoskeleton Robot,"Wang, Yuxuan; yuan, shaoke; Pu, Zihan; Wang, Jiangbei; Yanqiong, Fei",,
Task-Oriented Dexterous Hand Pose Synthesis via Differentiable Grasp Wrench Boundary Estimator,"Chen, Jiayi; Chen, Yuxing; Zhang, Jialiang; Wang, He",,
A Series Variable-stiffness Joint for Robot-assisted Resistance Training,"Hu, Xingyu; Li, Yuebing; Zhang, Wuxiang; Feng, Yanggang",,
Agile and Safe Trajectory Planning for Quadruped Navigation with Motion Anisotropy Awareness,"Zhang, Wentao; Xu, Shaohang; Cai, Peiyuan; Zhu, Lijun",https://arxiv.org/abs/2403.10101,"Quadruped robots demonstrate robust and agile movements in various terrains; however, their navigation autonomy is still insufficient. One of the challenges is that the motion capabilities of the quadruped robot are anisotropic along different directions, which significantly affects the safety of quadruped robot navigation. This paper proposes a navigation framework that takes into account the motion anisotropy of quadruped robots including kinodynamic trajectory generation, nonlinear trajectory optimization, and nonlinear model predictive control. In simulation and real robot tests, we demonstrate that our motion-anisotropy-aware navigation framework could: (1) generate more efficient trajectories and realize more agile quadruped navigation; (2) significantly improve the navigation safety in challenging scenarios. The implementation is realized as an open-source package at https://github.com/ZWT006/agile_navigation."
Low-cost air hockey robot using a five-bar linkage mechanism driven by position-control servomotors,"Shinjo, Mirai; Beltran-Hernandez, Cristian Camilo; Hamaya, Masashi; Tanaka, Kazutoshi",,
Real-time Birds-Eye-View Panoptic Segmentation for Monocular-based Indoor Navigation,"Kim, Dawit; Koo, Jungmo; YUN, Jongseob; Park, Soonyong",,
NFPDE: Normalizing Flow-based Parameter Distribution Estimation for Offline Adaptive Domain Randomization,"Takano, Rin; Takaya, Kei; Oyama, Hiroyuki",,
Analysis of Lockable Passive Prismatic and Revolute Joints,"Rosyid, Abdur; El-Khasawneh, Bashar",https://arxiv.org/abs/1705.05563,"A family of reconfigurable parallel robots can change motion modes by passing through constraint singularities by locking and releasing some passive joints of the robot. This paper is about the kinematics, the workspace and singularity analysis of a 3-PRPiR parallel robot involving lockable Pi and R (revolute) joints. Here a Pi joint may act as a 1-DOF planar parallelogram if its lock-able P (prismatic) joint is locked or a 2-DOF RR serial chain if its lockable P joint is released. The operation modes of the robot include a 3T operation modes to three 2T1R operation modes with two different directions of the rotation axis of the moving platform. The inverse kinematics and forward kinematics of the robot in each operation modes are dealt with in detail. The workspace analysis of the robot allow us to know the regions of the workspace that the robot can reach in each operation mode. A prototype built at Heriot-Watt University is used to illustrate the results of this work."
Peristaltic Soft Robot for Long-distance Pipe Inspection with an Endoskeletal Structure for Propulsion and Traction Amplification,"Okuma, Ryusei; Naruse, Yuta; Ito, Fumio; Nakamura, Taro",,
Ensuring Safety in LLM-Driven Robotics: A Cross-Layer Sequence Supervision Mechanism,"Wang, Ziming; Liu, Qingchen; Qin, Jiahu; Li, Man",,
Mastering Scene Rearrangement with Expert-assisted Curriculum Learning and Adaptive Trade-Off Tree-Search,"Wang, Hanqing; Wang, Zan; Liang, Wei",,
A Novel Ducted Fan UAV for Safe Aerial Grabbing and Transfer of Multiple Loads Using Electromagnets,"Yin, Zhong; Pei, Hai-Long",,
ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place Recognition,"Xie, Weidong; Luo, Lun; Ye, Nanfei; Ren, Yi; Du, Shaoyi; Wang, Minhang; Xu, Jintao; Ai, Rui; Gu, Weihao; Chen, Xieyuanli",https://arxiv.org/abs/2403.18762,"Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps. While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem. Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision. In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors. We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images. This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance. We further design a non-negative factorization-based encoder to extract mutually consistent semantic features between point clouds and images. This encoder yields more distinctive global descriptors for retrieval. Experimental results on the KITTI dataset show that our proposed methods achieve state-of-the-art performance while running in real time. Additional evaluation on the HAOMO dataset covering a 17 km trajectory further shows the practical generalization capabilities. We have released the implementation of our methods as open source at: https://github.com/haomo-ai/ModaLink.git."
Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding,"Tang, Huijie; Berto, Federico; Park, Jinkyoo",https://arxiv.org/abs/2403.07559,"Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations. Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions. We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF. We open-source our code at https://github.com/ai4co/eph-mapf."
EgoVM: Achieving Precise Ego-Localization using Lightweight Vectorized Maps,"He, Yuzhe; Liang, Shuang; Rui, XiaoFei; Cai, Chengying; Wan, Guowei",https://arxiv.org/abs/2307.08991,"Accurate and reliable ego-localization is critical for autonomous driving. In this paper, we present EgoVM, an end-to-end localization network that achieves comparable localization accuracy to prior state-of-the-art methods, but uses lightweight vectorized maps instead of heavy point-based maps. To begin with, we extract BEV features from online multi-view images and LiDAR point cloud. Then, we employ a set of learnable semantic embeddings to encode the semantic types of map elements and supervise them with semantic segmentation, to make their feature representation consistent with BEV features. After that, we feed map queries, composed of learnable semantic embeddings and coordinates of map elements, into a transformer decoder to perform cross-modality matching with BEV features. Finally, we adopt a robust histogram-based pose solver to estimate the optimal pose by searching exhaustively over candidate poses. We comprehensively validate the effectiveness of our method using both the nuScenes dataset and a newly collected dataset. The experimental results show that our method achieves centimeter-level localization accuracy, and outperforms existing methods using vectorized maps by a large margin. Furthermore, our model has been extensively tested in a large fleet of autonomous vehicles under various challenging urban scenes."
A velocity dependent delayed output feedback control (v-DOFC) for gait assistance with an ergonomically designed bi-directional cable-driven hip assist device,"Kim, Dong Hyun; Park, Junghoon; Shin, Gyowook; Yoon, Chiyul; Kim, Yongtae Giovanni; Kim, Sang-Hun; Hyung, SeungYong; KANG, SUNG-CHUL; Lee, Minhyung",,
X-Ray-Guided Magnetic Fields for Wireless Control of Untethered Magnetic Robots in Cerebral Vascular Phantoms,"Ligtenberg, Leendert-Jan Wouter; de Boer, Marcus Cornelis Johannes; Mulder, Iris; Lomme, Roger MLM; Wasserberg, Dorothee; Klein Rot, Emily A. M.; Ben Ami, Doron; Sadeh, Udi; Liefers, Herman Remco; Shoseyov, Oded; Jonkheijm, Pascal; Warle, Michiel; Khalil, Islam S.M.",,
Feasible Region Construction by Polygon Merging for Continuous Bipedal Walking,"Li, Chao; Chen, Xuechao; hengbo, qi; Li, Qingqing; Zhao, Qingrui; Shi, Yongliang; YU, Zhangguo; Jiang, Zhihong",,
CLAT: Convolutional Local Attention Tracker for Real-time UAV Target Tracking System with Feedback Information,"Sun, XiaoLou; Quan, Zhibin; si, wufei; wang, chunyan; Li, Yuntian; wu, yuan",,
Collision-Free Robot Navigation in Crowded Environments using Learning based Convex Model Predictive Control,"Wen, ZhuangLei; Dong, Mingze; chen, xiai",https://arxiv.org/abs/2403.01450,"Navigating robots safely and efficiently in crowded and complex environments remains a significant challenge. However, due to the dynamic and intricate nature of these settings, planning efficient and collision-free paths for robots to track is particularly difficult. In this paper, we uniquely bridge the robot's perception, decision-making and control processes by utilizing the convex obstacle-free region computed from 2D LiDAR data. The overall pipeline is threefold: (1) We proposes a robot navigation framework that utilizes deep reinforcement learning (DRL), conceptualizing the observation as the convex obstacle-free region, a departure from general reliance on raw sensor inputs. (2) We design the action space, derived from the intersection of the robot's kinematic limits and the convex region, to enable efficient sampling of inherently collision-free reference points. These actions assists in guiding the robot to move towards the goal and interact with other obstacles during navigation. (3) We employ model predictive control (MPC) to track the trajectory formed by the reference points while satisfying constraints imposed by the convex obstacle-free region and the robot's kinodynamic limits. The effectiveness of proposed improvements has been validated through two sets of ablation studies and a comparative experiment against the Timed Elastic Band (TEB), demonstrating improved navigation performance in crowded and complex environments."
Behavior Tree Based Decentralized Multi-agent Coordination for Balanced Servicing of Time Varying Task Queues,"Dahlquist, Niklas; Nikolakopoulos, George; Saradagi, Akshit",,
Perception-aware Full Body Trajectory Planning for Autonomous Systems using Motion Primitives,"Kuhne, Moritz; Giubilato, Riccardo; Roa, Maximo A.",,
Improved Contact Stability for Admittance Control of Industrial Robots with Inverse Model Compensation,"Samuel, Kangwagye; Haninger, Kevin; Haddadin, Sami; Oh, Sehoon",,
GV-Bench: Benchmarking Local Feature Matching for Geometric Verification of Long-term Loop Closure Detection,"YU, Jingwen; Ye, Hanjing; JIAO, Jianhao; Tan, Ping; Zhang, Hong",https://arxiv.org/abs/2407.11736,"Visual loop closure detection is an important module in visual simultaneous localization and mapping (SLAM), which associates current camera observation with previously visited places. Loop closures correct drifts in trajectory estimation to build a globally consistent map. However, a false loop closure can be fatal, so verification is required as an additional step to ensure robustness by rejecting the false positive loops. Geometric verification has been a well-acknowledged solution that leverages spatial clues provided by local feature matching to find true positives. Existing feature matching methods focus on homography and pose estimation in long-term visual localization, lacking references for geometric verification. To fill the gap, this paper proposes a unified benchmark targeting geometric verification of loop closure detection under long-term conditional variations. Furthermore, we evaluate six representative local feature matching methods (handcrafted and learning-based) under the benchmark, with in-depth analysis for limitations and future directions."
Exploring Constrained Reinforcement Learning Algorithms for Quadrupedal Locomotion,"Lee, Joonho; Schroth, Lukas; Klemm, Victor; Bjelonic, Marko; Reske, Alexander; Hutter, Marco",,
Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic Violations,"Innes, Craig; Ramamoorthy, Subramanian",https://arxiv.org/abs/2405.15771,"Autonomous Vehicles (AVs) are often tested in simulation to estimate the probability they will violate safety specifications. Two common issues arise when using existing techniques to produce this estimation: If violations occur rarely, simple Monte-Carlo sampling techniques can fail to produce efficient estimates; if simulation horizons are too long, importance sampling techniques (which learn proposal distributions from past simulations) can fail to converge. This paper addresses both issues by interleaving rare-event sampling techniques with online specification monitoring algorithms. We use adaptive multi-level splitting to decompose simulations into partial trajectories, then calculate the distance of those partial trajectories to failure by leveraging robustness metrics from Signal Temporal Logic (STL). By caching those partial robustness metric values, we can efficiently re-use computations across multiple sampling stages. Our experiments on an interstate lane-change scenario show our method is viable for testing simulated AV-pipelines, efficiently estimating failure probabilities for STL specifications based on real traffic rules. We produce better estimates than Monte-Carlo and importance sampling in fewer simulations."
ICR-based Kinematics for Wheeled Skid-Steer Vehicles on Firm Slopes,"Martinez, Jorge L.; Morales, Jesús; Sánchez-Montero, Manuel; García-Cerezo, Alfonso",,
"MOE: A Dense LiDAR Moving Event Dataset, Detection Benchmark and LeaderBoard","Chen, Zhiming; Fang, Haozhe; Chen, Jiapeng; Wang, Michael Yu; Yu, Hongyu",,
Toward An Analytic Theory of Intrinsic Robustness for Dexterous Grasping,"Li, Albert H.; Culbertson, Preston; Ames, Aaron",https://arxiv.org/abs/2403.07249,"Conventional approaches to grasp planning require perfect knowledge of an object's pose and geometry. Uncertainties in these quantities induce uncertainties in the quality of planned grasps, which can lead to failure. Classically, grasp robustness refers to the ability to resist external disturbances after grasping an object. In contrast, this work studies robustness to intrinsic sources of uncertainty like object pose or geometry affecting grasp planning before execution. To do so, we develop a novel analytic theory of grasping that reasons about this intrinsic robustness by characterizing the effect of friction cone uncertainty on a grasp's force closure status. We apply this result in two ways. First, we analyze the theoretical guarantees on intrinsic robustness of two grasp metrics in the literature, the classical Ferrari-Canny metric and more recent min-weight metric. We validate these results with hardware trials that compare grasps synthesized with and without robustness guarantees, showing a clear improvement in success rates. Second, we use our theory to develop a novel analytic notion of probabilistic force closure, which we show can generate unique, uncertainty-aware grasps in simulation."
Robot-Enabled Machine Learning-Based Diagnosis of Gastric Cancer Polyps Using Partial Surface Tactile Imaging,"Kapuria, Siddhartha; Bonyun, Jeff; Kulkarni, Yash; Ikoma, Naruhiko; Chinchali, Sandeep; Alambeigi, Farshid",https://arxiv.org/abs/2408.01554,"In this paper, to collectively address the existing limitations on endoscopic diagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we propose (i) utilization and evaluation of our recently developed Vision-based Tactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm for classifying tumors using their textural features. Leveraging a seven DoF robotic manipulator and unique custom-designed and additively-manufactured realistic AGC tumor phantoms, we demonstrated the advantages of automated data collection using the VTS addressing the problem of data scarcity and biases encountered in traditional ML-based approaches. Our synthetic-data-trained ML model was successfully evaluated and compared with traditional ML models utilizing various statistical metrics even under mixed morphological characteristics and partial sensor contact."
Reward-field Guided Motion Planner for Navigation with Limited Sensing Range,"Bayer, Jan; Faigl, Jan",,
WasteGAN: Data Augmentation for Robotic Waste Sorting through Generative Adversarial Networks,"Bacchin, Alberto; Barcellona, Leonardo; Terreran, Matteo; Ghidoni, Stefano; Menegatti, Emanuele; Kiyokawa, Takuya",,
A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets,"George Philip, Allen; Ren, Zhongqiang; Rathinam, Sivakumar; Choset, Howie",https://arxiv.org/abs/2403.04917,"This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) solver for the MT-TSP. The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60\% tighter optimality gap. We also show that the solution cost from the convex relaxation of our formulation provides significantly tighter lower bounds for the MT-TSP than the ones from the MICP."
VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training,"Nazeri, Mohammad; Wang, Junzhe; payandeh, amirreza; Xiao, Xuesu",https://arxiv.org/abs/2403.08109,"Humans excel at efficiently navigating through crowds without collision by focusing on specific visual regions relevant to navigation. However, most robotic visual navigation methods rely on deep learning models pre-trained on vision tasks, which prioritize salient objects -- not necessarily relevant to navigation and potentially misleading. Alternative approaches train specialized navigation models from scratch, requiring significant computation. On the other hand, self-supervised learning has revolutionized computer vision and natural language processing, but its application to robotic navigation remains underexplored due to the difficulty of defining effective self-supervision signals. Motivated by these observations, in this work, we propose a Self-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP). Instead of detecting salient objects that are beneficial for tasks such as classification or detection, VANP learns to focus only on specific visual regions that are relevant to the navigation task. To achieve this, VANP uses a history of visual observations, future actions, and a goal image for self-supervision, and embeds them using two small Transformer Encoders. Then, VANP maximizes the information between the embeddings by using a mutual information maximization objective function. We demonstrate that most VANP-extracted features match with human navigation intuition. VANP achieves comparable performance as models learned end-to-end with half the training time and models trained on a large-scale, fully supervised dataset, i.e., ImageNet, with only 0.08% data."
RADAR: Robotics Assembly by Demonstration via Augmented Reality,"Yang, Wenhao; Bai, Shi; Zhang, Yunbo",,
NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification,"Han, Juyeop; Lao Beyer, Lukas; Cavalheiro, Guilherme; Karaman, Sertac",https://arxiv.org/abs/2404.01400,"In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful tool for 3D reconstruction and novel view synthesis. However, the computational cost of NeRF rendering and degradation in quality due to the presence of artifacts pose significant challenges for its application in real-time and robust robotic tasks, especially on embedded systems. This paper introduces a novel framework that integrates NeRF-derived localization information with Visual-Inertial Odometry (VIO) to provide a robust solution for real-time robotic navigation. By training an absolute pose regression network with augmented image data rendered from a NeRF and quantifying its uncertainty, our approach effectively counters positional drift and enhances system reliability. We also establish a mathematically sound foundation for combining visual inertial navigation with camera localization neural networks, considering uncertainty under a Bayesian framework. Experimental validation in a photorealistic simulation environment demonstrates significant improvements in accuracy compared to a conventional VIO approach."
A Circular Soft Pneumatic Actuator with Bi-Directional Bending Behavior,"Circe, Jeannette; Giglia, Michael; Rivera, Isaiah; Vardanyan, Ani; Bunt, Brandon, Kiau; Rosen, Michelle",,
Wing twist and folding work in synergy to propel flapping wing animals and robots,"Fan, Xiaozhou; Gehrke, Alexander; Breuer, Kenneth",https://arxiv.org/abs/2408.15577,"We designed and built a three degrees-of-freedom (DOF) flapping wing robot, Flapperoo, to study the aerodynamic benefits of wing folding and twisting. Forces and moments of this physical model are measured in wind tunnel tests over a Strouhal number range of St = 0.2 - 0.4, typical for animal flight. We perform particle image velocimetry (PIV) measurements to visualize the air jet produced by wing clapping under the ventral side of the body when wing folding is at the extreme. The results show that this jet can be directed by controlling the wing twist at the moment of clapping, which leads to greatly enhanced cycle-averaged thrust, especially at high St or low flight speeds. Additional benefits of more thrust and less negative lift are gained during upstroke using wing twist. Remarkably, less total actuating force, or less total power, is required during upstroke with wing twist. These findings emphasize the benefits of critical wing articulation for the future flapping wing/fin robots and for an accurate test platform to study natural flapping wing flight or underwater vehicles."
A Contact Model based on Denoising Diffusion to Learn Variable Impedance Control for Contact-rich Manipulation,"Okada, Masashi; Komatsu, Mayumi; Taniguchi, Tadahiro",https://arxiv.org/abs/2403.13221,"In this paper, a novel approach is proposed for learning robot control in contact-rich tasks such as wiping, by developing Diffusion Contact Model (DCM). Previous methods of learning such tasks relied on impedance control with time-varying stiffness tuning by performing Bayesian optimization by trial-and-error with robots. The proposed approach aims to reduce the cost of robot operation by predicting the robot contact trajectories from the variable stiffness inputs and using neural models. However, contact dynamics are inherently highly nonlinear, and their simulation requires iterative computations such as convex optimization. Moreover, approximating such computations by using finite-layer neural models is difficult. To overcome these limitations, the proposed DCM used the denoising diffusion models that could simulate the complex dynamics via iterative computations of multi-step denoising, thus improving the prediction accuracy. Stiffness tuning experiments conducted in simulated and real environments showed that the DCM achieved comparable performance to a conventional robot-based optimization method while reducing the number of robot trials."
Do One Thing and Do It Well: Delegate Responsibilities in Classical Planning,"Lai, Tin; Morere, Philippe",,
Real-to-Sim Adaptation via High-Fidelity Simulation to Control a Wheeled-Humanoid Robot with Unknown Dynamics,"Baek, DongHoon; Sim, Youngwoo; Purushottam, Amartya; Gupta, Saurabh; Ramos, Joao",,
Development of Contextual Collision Risk framework for Operational Envelope of autonomous navigation system,"Kim, Inbeom; Ko, Kwangsung",,
Time-varying Control Barrier Function for Safe and Precise Landing of a UAV on a Moving Target,"Sankaranarayanan, Viswa Narayanan; Saradagi, Akshit; Satpute, Sumeet; Nikolakopoulos, George",,
Learning Variable Compliance Control From a Few Demonstrations for Bimanual Robot with Haptic Feedback Teleoperation System,"Kamijo, Tatsuya; Beltran-Hernandez, Cristian Camilo; Hamaya, Masashi",https://arxiv.org/abs/2406.14990,"Automating dexterous, contact-rich manipulation tasks using rigid robots is a significant challenge in robotics. Rigid robots, defined by their actuation through position commands, face issues of excessive contact forces due to their inability to adapt to contact with the environment, potentially causing damage. While compliance control schemes have been introduced to mitigate these issues by controlling forces via external sensors, they are hampered by the need for fine-tuning task-specific controller parameters. Learning from Demonstrations (LfD) offers an intuitive alternative, allowing robots to learn manipulations through observed actions. In this work, we introduce a novel system to enhance the teaching of dexterous, contact-rich manipulations to rigid robots. Our system is twofold: firstly, it incorporates a teleoperation interface utilizing Virtual Reality (VR) controllers, designed to provide an intuitive and cost-effective method for task demonstration with haptic feedback. Secondly, we present Comp-ACT (Compliance Control via Action Chunking with Transformers), a method that leverages the demonstrations to learn variable compliance control from a few demonstrations. Our methods have been validated across various complex contact-rich manipulation tasks using single-arm and bimanual robot setups in simulated and real-world environments, demonstrating the effectiveness of our system in teaching robots dexterous manipulations with enhanced adaptability and safety."
Indoor 3D Reconstruction Based on Shading and Motion Clues,"Zhou, Yuwei; Lu, Guoyu",,
Physically-Based Photometric Bundle Adjustment in Non-Lambertian Environments,"Hu, Junpeng; Cheng, Lei; Yan, Haodong; Gladkova, Mariia; Huang, Tianyu; Liu, Yunhui; Cremers, Daniel; Li, Haoang",,
In-Flight Initialization of Global Visual-Inertial Estimators using Geospatial Data,"Li, Chunyu; He, Mengfan; Lyu, Xu; Meng, Ziyang",,
Robot Generating Data for Learning Generalizable Visual Robotic Manipulation,"Li, Yunfei; Yuan, Ying; Cui, Jingzhi; Huan, Haoran; Fu, Wei; Gao, Jiaxuan; Xu, Zekai; Wu, Yi",https://arxiv.org/abs/1910.11215,"Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the promise of providing effective generalization to a wide range of open-world environments. However, these same methods typically require large amounts of diverse training data to generalize effectively. In contrast, most robotic learning experiments are small-scale, single-domain, and single-robot. This leads to a frequent tension in robotic learning: how can we learn generalizable robotic controllers without having to collect impractically large amounts of data for each separate experiment? In this paper, we propose RoboNet, an open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation. We combine the dataset with two different learning algorithms: visual foresight, which uses forward video prediction models, and supervised inverse models. Our experiments test the learned algorithms' ability to work across new objects, new tasks, new scenes, new camera viewpoints, new grippers, or even entirely new robots. In our final experiment, we find that by pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka robot, we can exceed the performance of a robot-specific training approach that uses 4x-20x more data. For videos and data, see the project webpage: https://www.robonet.wiki/"
Multi-Robot Navigation Among Movable Obstacles: Implicit Coordination to Deal with Conflicts and Deadlocks,"Renault, Benoit; Saraydaryan, Jacques; Brown, David; Simonin, Olivier",,
Automatic Field of View Adjustment of an RCM Constraint-Free Continuum Laparoscopic Robot,"Zhang, Jing; Wang, Baichuan; Pan, Zhijie; Li, Weiqi; Li, Mengtang",,
Multimodal Haptic Interface for Walker-Assisted Navigation,"Wang, Yikun; Sierra M., Sergio D.; Harris, Nigel; Munera, Marcela; Cifuentes, Carlos A.",,
Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions,"Dasgupta, Saptarshi; Gupta, Akshat; Tuli, Shreshth; Paul, Rohan",https://arxiv.org/abs/2404.01812,"Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility. Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction. Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with benchmark objects demonstrate an improvement of (i) 14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth reconstruction of the object surface (F-score) and (iii) 71% in the task success rate of manipulating objects a-priori unseen orientations/stable configurations in the scene; over current methods. The project page can be found here: https://actnerf.github.io."
Magnetic tactile sensor with load tolerance and flexibility using frame structures for estimating triaxial contact force distribution of humanoid,"Hiraoka, Takuma; Kunita, Ren; Kojima, Kunio; Hiraoka, Naoki; Konishi, Masanori; Makabe, Tasuku; Okada, Kei; Inaba, Masayuki",,
Development of a Novel Redundant Parallel Mechanism with Enlarged Workspace and Enhanced Dexterity for Fracture Reduction Surgery,"Yuan, Quan; Liang, Xu; Su, Tingting; Bai, Weibang",,
Visuo-Tactile Exploration of Unknown Rigid 3D Curvatures by Vision-Augmented Unified Force-Impedance Control,"Karacan, Kübra; Zhang, Anran; Sadeghian, Hamid; Wu, Fan; Haddadin, Sami",https://arxiv.org/abs/2408.14219,"Despite recent advancements in torque-controlled tactile robots, integrating them into manufacturing settings remains challenging, particularly in complex environments. Simplifying robotic skill programming for non-experts is crucial for increasing robot deployment in manufacturing. This work proposes an innovative approach, Vision-Augmented Unified Force-Impedance Control (VA-UFIC), aimed at intuitive visuo-tactile exploration of unknown 3D curvatures. VA-UFIC stands out by seamlessly integrating vision and tactile data, enabling the exploration of diverse contact shapes in three dimensions, including point contacts, flat contacts with concave and convex curvatures, and scenarios involving contact loss. A pivotal component of our method is a robust online contact alignment monitoring system that considers tactile error, local surface curvature, and orientation, facilitating adaptive adjustments of robot stiffness and force regulation during exploration. We introduce virtual energy tanks within the control framework to ensure safety and stability, effectively addressing inherent safety concerns in visuo-tactile exploration. Evaluation using a Franka Emika research robot demonstrates the efficacy of VA-UFIC in exploring unknown 3D curvatures while adhering to arbitrarily defined force-motion policies. By seamlessly integrating vision and tactile sensing, VA-UFIC offers a promising avenue for intuitive exploration of complex environments, with potential applications spanning manufacturing, inspection, and beyond."
Current-Based Impedance Control for Interacting with Mobile Manipulators,"de Wolde, Jelmer; Knoedler, Luzia; Garofalo, Gianluca; Alonso-Mora, Javier",https://arxiv.org/abs/2403.13079,"As robots shift from industrial to human-centered spaces, adopting mobile manipulators, which expand workspace capabilities, becomes crucial. In these settings, seamless interaction with humans necessitates compliant control. Two common methods for safe interaction, admittance, and impedance control, require force or torque sensors, often absent in lower-cost or lightweight robots. This paper presents an adaption of impedance control that can be used on current-controlled robots without the use of force or torque sensors and its application for compliant control of a mobile manipulator. A calibration method is designed that enables estimation of the actuators' current/torque ratios and frictions, used by the adapted impedance controller, and that can handle model errors. The calibration method and the performance of the designed controller are experimentally validated using the Kinova GEN3 Lite arm. Results show that the calibration method is consistent and that the designed controller for the arm is compliant while also being able to track targets with five-millimeter precision when no interaction is present. Additionally, this paper presents two operational modes for interacting with the mobile manipulator: one for guiding the robot around the workspace through interacting with the arm and another for executing a tracking task, both maintaining compliance to external forces. These operational modes were tested in real-world experiments, affirming their practical applicability and effectiveness."
Multi-Robot Multi-Goal Mission Planning in Terrains of Varying Energy Consumption,"Herynek, Jáchym; Edelkamp, Stefan",,
Vertebrea-based Global X-ray to CT Registration for Thoracic Surgeries,"Liu, Lilu; Jiao, Yanmei; An, Zhou; Ma, Honghai; Zhou, Chunlin; Lu, Haojian; Hu, Jian; Xiong, Rong; Wang, Yue",,
Collision Detection between Smooth Convex Bodies via Riemannian Optimization Framework,"An, Seoki; Lee, Somang; Lee, Jeongmin; Park, Sunkyung; Lee, Dongjun",,
GraspContrast: Self-supervised Contrastive Learning with False Negative Elimination for 6-DoF Grasp Detection,"Wang, Wenshuo; Zhu, Haiyue; Ang Jr, Marcelo H",,
CFD-enabled Approach for Optimizing CPG Control Network for Underwater Soft Robotic Fish,"Wang, Yunfei; Sun, Weiyuan; Tang, Wei; Zhang, Xianrui; Yu, Zhenping; Cao, Shunxiang; Qu, Juntian",,
NeSyMoF: A Neuro-Symbolic Model for Motion Forecasting,"Doula, Achref; Yin, Huijie; Mühlhäuser, Max; Sanchez Guinea, Alejandro",,
OW3Det: Toward Open-World 3D Object Detection for Autonomous Driving,"Hu, Wenfei; Lin, Weikai; Fang, Hongyu; Wang, Yi; Luo, Dingsheng",,
Intraocular Reflection Modeling and Avoidance Planning in Image-Guided Ophthalmic Surgeries,"Yang, Junjie; Zhao, Zhihao; Zhao, Yinzheng; Zapp, Daniel; Maier, Mathias; Huang, Kai; Navab, Nassir; Nasseri, M. Ali",,
SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios,"huang, dingtao; Lin, Ente; Chen, Lipeng; Liu, lifu; Zeng, Long",https://arxiv.org/abs/2403.09317,"Despite the success in 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects and real world scenarios. The primary bottlenecks include 1) the ambiguity keypoints caused by object symmetries; 2) the domain gap between real and synthetic data. To circumvent these problem, we propose a new 6D pose estimation network with symmetric-aware keypoint prediction and self-training domain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and deep hough voting to perform reliable detection keypoint under clutter and occlusion. Specifically, at the keypoint prediction stage, we designe a robust 3D keypoints selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes. Additionally, we build an effective filtering algorithm on predicted keypoint to dynamically eliminate multiple ambiguity and outlier keypoint candidates. At the domain adaptation stage, we propose the self-training framework using a student-teacher training scheme. To carefully distinguish reliable predictions, we harnesses a tailored heuristics for 3D geometry pseudo labelling based on semi-chamfer distance. On public Sil'eane dataset, SD-Net achieves state-of-the-art results, obtaining an average precision of 96%. Testing learning and generalization abilities on public Parametric datasets, SD-Net is 8% higher than the state-of-the-art method. The code is available at https://github.com/dingthuang/SD-Net."
SparseGTN: Human Trajectory Forecasting with Sparsely Represented Scene and Incomplete Trajectories,"Liu, Jianbang; Li, Guangyang; Mao, Xinyu; Meng, Fei; Mei, Jie; Meng, Max Q.-H.",,
Improving behavior profile discovery for vehicles,"de Moura Martins Gomes, Nelson; Garrido Carpio, Fernando José; Nashashibi, Fawzi",,
BaSeNet: A Learning-based Mobile Manipulator Base Pose Sequence Planning for Pickup Tasks,"Naik, Lakshadeep; Kalkan, Sinan; Sørensen, Sune Lundø; Mikkel, Kjærgaard; Krüger, Norbert",https://arxiv.org/abs/2406.08653,"In many applications, a mobile manipulator robot is required to grasp a set of objects distributed in space. This may not be feasible from a single base pose and the robot must plan the sequence of base poses for grasping all objects, minimizing the total navigation and grasping time. This is a Combinatorial Optimization problem that can be solved using exact methods, which provide optimal solutions but are computationally expensive, or approximate methods, which offer computationally efficient but sub-optimal solutions. Recent studies have shown that learning-based methods can solve Combinatorial Optimization problems, providing near-optimal and computationally efficient solutions.   In this work, we present BASENET - a learning-based approach to plan the sequence of base poses for the robot to grasp all the objects in the scene. We propose a Reinforcement Learning based solution that learns the base poses for grasping individual objects and the sequence in which the objects should be grasped to minimize the total navigation and grasping costs using Layered Learning. As the problem has a varying number of states and actions, we represent states and actions as a graph and use Graph Neural Networks for learning. We show that the proposed method can produce comparable solutions to exact and approximate methods with significantly less computation time."
Ternary-Type Opacity and Hybrid Odometry for RGB NeRF-SLAM,"Lin, Junru; Nachkov, Asen; Peng, Songyou; Van Gool, Luc; Paudel, Danda Pani",https://arxiv.org/abs/2312.13332,"The opacity of rigid 3D scenes with opaque surfaces is considered to be of a binary type. However, we observed that this property is not followed by the existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization through the volumetric rendering function does not facilitate easy integration of the desired prior. Instead, we observed that the opacity of ternary-type (TT) is well supported. In this work, we study why ternary-type opacity is well-suited and desired for the task at hand. In particular, we provide theoretical insights into the process of jointly optimizing radiance and opacity through the volumetric rendering process. Through exhaustive experiments on benchmark datasets, we validate our claim and provide insights into the optimization process, which we believe will unleash the potential of RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple yet novel visual odometry scheme that uses a hybrid combination of volumetric and warping-based image renderings. More specifically, the proposed hybrid odometry (HO) additionally uses image warping-based coarse odometry, leading up to an order of magnitude final speed-up. Furthermore, we show that the proposed TT and HO well complement each other, offering state-of-the-art results on benchmark datasets in terms of both speed and accuracy."
CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point Cloud Registration,"Yao, Gongxin; Xuan, Yixin; Li, Xinyang; Pan, Yu",https://arxiv.org/abs/2408.02394,"Image-to-point cloud registration aims to determine the relative camera pose of an RGB image with respect to a point cloud. It plays an important role in camera localization within pre-built LiDAR maps. Despite the modality gaps, most learning-based methods establish 2D-3D point correspondences in feature space without any feedback mechanism for iterative optimization, resulting in poor accuracy and interpretability. In this paper, we propose to reformulate the registration procedure as an iterative Markov decision process, allowing for incremental adjustments to the camera pose based on each intermediate state. To achieve this, we employ reinforcement learning to develop a cross-modal registration agent (CMR-Agent), and use imitation learning to initialize its registration policy for stability and quick-start of the training. According to the cross-modal observations, we propose a 2D-3D hybrid state representation that fully exploits the fine-grained features of RGB images while reducing the useless neutral states caused by the spatial truncation of camera frustum. Additionally, the overall framework is well-designed to efficiently reuse one-shot cross-modal embeddings, avoiding repetitive and time-consuming feature extraction. Extensive experiments on the KITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves competitive accuracy and efficiency in registration. Once the one-shot embeddings are completed, each iteration only takes a few milliseconds."
Interactive Multi-Stiffness Mixed Reality Interface: Controlling and Visualizing Robot and Environment Stiffnes,"Díaz Rosales, Alejandro; Rodriguez-Nogueira, Jose; Matheson, Eloise; Abbink, David A.; Peternel, Luka",,
Design and Evaluation of a Prototype Tactile Radar for Active Sensing of Proximal Objects,"Dechaux, Amaury; Kitazaki, Michiteru; Lagarde, Julien; Ganesh, Gowrishankar",,
Refractive COLMAP: Refractive Structure-from-Motion Revisited,"She, Mengkun; Seegräber, Felix; Nakath, David; Koeser, Kevin",https://arxiv.org/abs/2403.08640,"In this paper, we present a complete refractive Structure-from-Motion (RSfM) framework for underwater 3D reconstruction using refractive camera setups (for both, flat- and dome-port underwater housings). Despite notable achievements in refractive multi-view geometry over the past decade, a robust, complete and publicly available solution for such tasks is not available at present, and often practical applications have to resort to approximating refraction effects by the intrinsic (distortion) parameters of a pinhole camera model. To fill this gap, we have integrated refraction considerations throughout the entire SfM process within the state-of-the-art, open-source SfM framework COLMAP. Numerical simulations and reconstruction results on synthetically generated but photo-realistic images with ground truth validate that enabling refraction does not compromise accuracy or robustness as compared to in-air reconstructions. Finally, we demonstrate the capability of our approach for large-scale refractive scenarios using a dataset consisting of nearly 6000 images. The implementation is released as open-source at: https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater."
OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation,"Jiang, Haochen; Xu, Yueming; Zeng, Yihan; XU, Hang; Zhang, Wei; Feng, Jianfeng; Zhang, Li",https://arxiv.org/abs/2403.11796,"3D reconstruction has been widely used in autonomous navigation fields of mobile robotics. However, the former research can only provide the basic geometry structure without the capability of open-world scene understanding, limiting advanced tasks like human interaction and visual navigation. Moreover, traditional 3D scene understanding approaches rely on expensive labeled 3D datasets to train a model for a single task with supervision. Thus, geometric reconstruction with zero-shot scene understanding i.e. Open vocabulary 3D Understanding and Reconstruction, is crucial for the future development of mobile robots. In this paper, we propose OpenOcc, a novel framework unifying the 3D scene reconstruction and open vocabulary understanding with neural radiance fields. We model the geometric structure of the scene with occupancy representation and distill the pre-trained open vocabulary model into a 3D language field via volume rendering for zero-shot inference. Furthermore, a novel semantic-aware confidence propagation (SCP) method has been proposed to relieve the issue of language field representation degeneracy caused by inconsistent measurements in distilled features. Experimental results show that our approach achieves competitive performance in 3D scene understanding tasks, especially for small and long-tail objects."
Continual Domain Randomization,"Josifovski, Josip; Auddy, Sayantan; Malmir, Mohammadhossein; Piater, Justus; Knoll, Alois; Navarro-Guerrero, Nicolás",https://arxiv.org/abs/2403.12193,"Domain Randomization (DR) is commonly used for sim2real transfer of reinforcement learning (RL) policies in robotics. Most DR approaches require a simulator with a fixed set of tunable parameters from the start of the training, from which the parameters are randomized simultaneously to train a robust model for use in the real world. However, the combined randomization of many parameters increases the task difficulty and might result in sub-optimal policies. To address this problem and to provide a more flexible training process, we propose Continual Domain Randomization (CDR) for RL that combines domain randomization with continual learning to enable sequential training in simulation on a subset of randomization parameters at a time. Starting from a model trained in a non-randomized simulation where the task is easier to solve, the model is trained on a sequence of randomizations, and continual learning is employed to remember the effects of previous randomizations. Our robotic reaching and grasping tasks experiments show that the model trained in this fashion learns effectively in simulation and performs robustly on the real robot while matching or outperforming baselines that employ combined randomization or sequential randomization without continual learning. Our code and videos are available at https://continual-dr.github.io/."
Multi-View 2D to 3D Lifting Video-Based Optimization: A Robust Approach for Human Pose Estimation with Occluded Joint Prediction,"Rato, Daniela; Oliveira, Miguel; Santos, Vitor; Sappa, Angel",,
A Framework for Neurosymbolic Goal-Conditioned Continual Learning for Open World Environments,"Lorang, Pierrick; Goel, Shivam; Shukla, Yash; Zips, Patrik; Scheutz, Matthias",,
AVM-SLAM: Semantic Visual SLAM with Multi-Sensor Fusion in a Bird's Eye View for Automated Valet Parking,"Li, Ye; Yang, Wenchao; Lin, Dekun; Wang, Qianlei; Cui, Zhe; Qin, Xiaolin",https://arxiv.org/abs/2309.08180,"Accurate localization in challenging garage environments -- marked by poor lighting, sparse textures, repetitive structures, dynamic scenes, and the absence of GPS -- is crucial for automated valet parking (AVP) tasks. Addressing these challenges, our research introduces AVM-SLAM, a cutting-edge semantic visual SLAM architecture with multi-sensor fusion in a bird's eye view (BEV). This novel framework synergizes the capabilities of four fisheye cameras, wheel encoders, and an inertial measurement unit (IMU) to construct a robust SLAM system. Unique to our approach is the implementation of a flare removal technique within the BEV imagery, significantly enhancing road marking detection and semantic feature extraction by convolutional neural networks for superior mapping and localization. Our work also pioneers a semantic pre-qualification (SPQ) module, designed to adeptly handle the challenges posed by environments with repetitive textures, thereby enhancing loop detection and system robustness. To demonstrate the effectiveness and resilience of AVM-SLAM, we have released a specialized multi-sensor and high-resolution dataset of an underground garage, accessible at https://yale-cv.github.io/avm-slam_dataset, encouraging further exploration and validation of our approach within similar settings."
Evaluation and Deployment of LiDAR-based Place Recognition in Dense Forests,"Oh, Haedam; Chebrolu, Nived; Mattamala, Matias; Freißmuth, Leonard; Fallon, Maurice",https://arxiv.org/abs/2403.14326,"Many LiDAR place recognition systems have been developed and tested specifically for urban driving scenarios. Their performance in natural environments such as forests and woodlands have been studied less closely. In this paper, we analyzed the capabilities of four different LiDAR place recognition systems, both handcrafted and learning-based methods, using LiDAR data collected with a handheld device and legged robot within dense forest environments. In particular, we focused on evaluating localization where there is significant translational and orientation difference between corresponding LiDAR scan pairs. This is particularly important for forest survey systems where the sensor or robot does not follow a defined road or path. Extending our analysis we then incorporated the best performing approach, Logg3dNet, into a full 6-DoF pose estimation system -- introducing several verification layers for precise registration. We demonstrated the performance of our methods in three operational modes: online SLAM, offline multi-mission SLAM map merging, and relocalization into a prior map. We evaluated these modes using data captured in forests from three different countries, achieving 80% of correct loop closures candidates with baseline distances up to 5m, and 60% up to 10m. Video at: https://youtu.be/86l-oxjwmjY"
Creating Discomfort Maps via Hand-held Human Feedback Interface for Robotic Shoulder Physiotherapy,"Ravenberg, Jevon Gianni; Belli, Italo; Prendergast, J. Micah; Seth, Ajay; Peternel, Luka",,
React to This! How Humans Challenge Interactive Agents using Nonverbal Behaviors,"Zhang, Chuxuan; Burkanova, Bermet; Kim, Lawrence H.; Yip, Lauren; Cupcic, Ugo; Lallée, Stéphane; Lim, Angelica",,
Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System,"Freißmuth, Leonard; Mattamala, Matias; Chebrolu, Nived; Schaefer, Simon; Leutenegger, Stefan; Fallon, Maurice",https://arxiv.org/abs/2403.17622,"Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete."
DMFuser: Distilled Multi-Task Learning for End-to-end Transformer-Based Sensor Fusion in Autonomous Driving,"Agand, Pedram; Mahdavian, Mohammad; Savva, Manolis; Chen, Mo",,
NLNS-MASPF for solving Multi-Agent scheduling and Path-Finding,"Park, Heemang; AHN, Kyuree; Park, Jinkyoo",,
Towards Enhanced Fairness and Sample Efficiency in Traffic Signal Control,"Huang, Xingshuai; Wu, Di; Jenkin, Michael; Boulet, Benoit",,
Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions,"Chanrungmaneekul, Podshara; Ren, Kejia; Grace, Joshua; Dollar, Aaron; Hang, Kaiyu",https://arxiv.org/abs/2403.13144,"Calibrating robots into their workspaces is crucial for manipulation tasks. Existing calibration techniques often rely on sensors external to the robot (cameras, laser scanners, etc.) or specialized tools. This reliance complicates the calibration process and increases the costs and time requirements. Furthermore, the associated setup and measurement procedures require significant human intervention, which makes them more challenging to operate. Using the built-in force-torque sensors, which are nowadays a default component in collaborative robots, this work proposes a self-calibration framework where robot-environmental spatial relations are automatically estimated through compliant exploratory actions by the robot itself. The self-calibration approach converges, verifies its own accuracy, and terminates upon completion, autonomously purely through interactive exploration of the environment's geometries. Extensive experiments validate the effectiveness of our self-calibration approach in accurately establishing the robot-environment spatial relationships without the need for additional sensing equipment or any human intervention."
Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction,"Chen, Changan; Ramos Chen, Jordi; Tomar, Anshul; Grauman, Kristen",https://arxiv.org/abs/2405.02821,"Sim2real transfer has received increasing attention lately due to the success of learning robotic tasks in simulation end-to-end. While there has been a lot of progress in transferring vision-based navigation policies, the existing sim2real strategy for audio-visual navigation performs data augmentation empirically without measuring the acoustic gap. The sound differs from light in that it spans across much wider frequencies and thus requires a different solution for sim2real. We propose the first treatment of sim2real for audio-visual navigation by disentangling it into acoustic field prediction (AFP) and waypoint navigation. We first validate our design choice in the SoundSpaces simulator and show improvement on the Continuous AudioGoal navigation benchmark. We then collect real-world data to measure the spectral difference between the simulation and the real world by training AFP models that only take a specific frequency subband as input. We further propose a frequency-adaptive strategy that intelligently selects the best frequency band for prediction based on both the measured spectral difference and the energy distribution of the received audio, which improves the performance on the real data. Lastly, we build a real robot platform and show that the transferred policy can successfully navigate to sounding objects. This work demonstrates the potential of building intelligent agents that can see, hear, and act entirely from simulation, and transferring them to the real world."
NARRATE: Versatile Language Architecture for Optimal Control in Robotics,"Ismail, Seif; Arbues, Antonio; Cotterell, Ryan; Zurbrügg, René; Amo Alonso, Carmen",https://arxiv.org/abs/2403.10762,"The impressive capabilities of Large Language Models (LLMs) have led to various efforts to enable robots to be controlled through natural language instructions, opening exciting possibilities for human-robot interaction The goal is for the motor-control task to be performed accurately, efficiently and safely while also enjoying the flexibility imparted by LLMs to specify and adjust the task through natural language. In this work, we demonstrate how a careful layering of an LLM in combination with a Model Predictive Control (MPC) formulation allows for accurate and flexible robotic control via natural language while taking into consideration safety constraints. In particular, we rely on the LLM to effectively frame constraints and objective functions as mathematical expressions, which are later used in the motor-control module via MPC. The transparency of the optimization formulation allows for interpretability of the task and enables adjustments through human feedback. We demonstrate the validity of our method through extensive experiments on long-horizon reasoning, contact-rich, and multi-object interaction tasks. Our evaluations show that NARRATE outperforms current existing methods on these benchmarks and effectively transfers to the real world on two different embodiments. Videos, Code and Prompts at narrate-mpc.github.io"
MaskingDepth: Masked Consistency Regularization for Semi-Supervised Monocular Depth Estimation,"Baek, Jongbeom; Kim, Gyeongnyeon; Park, Seonghoon; An, Honggyu; Poggi, Matteo; Kim, Seungryong",https://arxiv.org/abs/2212.10806,"We propose MaskingDepth, a novel semi-supervised learning framework for monocular depth estimation to mitigate the reliance on large ground-truth depth quantities. MaskingDepth is designed to enforce consistency between the strongly-augmented unlabeled data and the pseudo-labels derived from weakly-augmented unlabeled data, which enables learning depth without supervision. In this framework, a novel data augmentation is proposed to take the advantage of a naive masking strategy as an augmentation, while avoiding its scale ambiguity problem between depths from weakly- and strongly-augmented branches and risk of missing small-scale instances. To only retain high-confident depth predictions from the weakly-augmented branch as pseudo-labels, we also present an uncertainty estimation technique, which is used to define robust consistency regularization. Experiments on KITTI and NYU-Depth-v2 datasets demonstrate the effectiveness of each component, its robustness to the use of fewer depth-annotated images, and superior performance compared to other state-of-the-art semi-supervised methods for monocular depth estimation. Furthermore, we show our method can be easily extended to domain adaptation task. Our code is available at https://github.com/KU-CVLAB/MaskingDepth."
Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning,"Honari, Homayoun; Soufi Enayati, Amir Mehdi; Ghafarian Tamizi, Mehran; Najjaran, Homayoun",https://arxiv.org/abs/2408.07962,"Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints."
Contacts from Motion: Learning Discrete Features for Automatic Contact Detection and Estimation from Human Movements,"Miyake, Hibiki; Ayusawa, Ko; Sagawa, Ryusuke; Yoshida, Eiichi",,
A Multi-model Fusion of LiDAR-inertial Odometry via Localization and Mapping,"Nguyen, An; Le, Chuong; Walunj, Pratik; Do, Thanh Nho; Netchaev, Anton; La, Hung",,
Effects of fiber number and density on fiber jamming: Towards follow-the-leader deployment of a continuum robot,"Qian, Chen; Liu, Tangyou; Wu, Liao",https://arxiv.org/abs/2408.13529,"Fiber jamming modules (FJMs) offer flexibility and quick stiffness variation, making them suitable for follow-the-leader (FTL) motions in continuum robots, which is ideal for minimally invasive surgery (MIS). However, their potential has not been fully exploited, particularly in designing and manufacturing small-sized FJMs with high stiffness variation. Although existing research has focused on factors like fiber materials and geometry to maximize stiffness variation, the results often do not apply to FJMs for MIS due to size constraints. Meanwhile, other factors such as fiber number and packing density, less significant to large FJMs but critical to small-sized FJMs, have received insufficient investigation regarding their impact on the stiffness variation for FTL deployment. In this paper, we design and fabricate FJMs with a diameter of 4mm. Through theoretical and experimental analysis, we find that fiber number and packing density significantly affect both absolute stiffness and stiffness variation. Our experiments confirm the feasibility of using FJMs in a medical FTL robot design. The optimal configuration is a 4mm FJM with 0.4mm fibers at a 56% packing density, achieving up to 3400% stiffness variation. A video demonstration of a prototype robot using the suggested parameters for achieving FTL motions can be found at https://youtu.be/7pI5U0z7kcE."
Belief-Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots,"Kim, Jinyeob; DAEWON, KWAK; RIM, Hyunwoo; Kim, Donghan",https://arxiv.org/abs/2403.10105,"Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the dynamics between the robot, humans, and inferred beliefs to determine the navigation paths and embeds social norms within the reward function, thereby facilitating socially aware navigation. Through experiments in various risk laden scenarios, this study validates the effectiveness of BNBRL+ in navigating crowded environments with blind spots. The model's ability to navigate effectively in spaces with limited visibility and avoid obstacles dynamically can significantly improve the safety and reliability of autonomous vehicles."
Large-scale Indoor Mapping with Failure Detection and Recovery in SLAM,"Rahman, Sharmin; DiPietro, Robert; Kedarisetti, Dharanish; Kulathumani, Vinod",,
Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting,"Swann, Aiden; Strong, Matthew; Do, Won Kyung; Sznaier Camps, Gadiel; Schwager, Mac; Kennedy, Monroe",https://arxiv.org/abs/2403.09875,"In this work, we propose a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. We merge this model with a monocular depth estimation network, which is aligned in a two stage process, coarsely aligning with a depth camera and then finely adjusting to match our touch data. For every training image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information, we propose a new loss function, variance weighted depth supervised loss, for training the 3DGS scene model. We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in a few-view scene syntheses on opaque as well as on reflective and transparent objects. Please see our project page at http://armlabstanford.github.io/touch-gs"
OSM vs HD Maps: Map Representations for Trajectory Prediction,"Liao, Jing-Yan; Doshi, Parth Jaydip; Zhang, Zihan; Paz, David; Christensen, Henrik Iskov",https://arxiv.org/abs/2311.02305,"While High Definition (HD) Maps have long been favored for their precise depictions of static road elements, their accessibility constraints and susceptibility to rapid environmental changes impede the widespread deployment of autonomous driving, especially in the motion forecasting task. In this context, we propose to leverage OpenStreetMap (OSM) as a promising alternative to HD Maps for long-term motion forecasting. The contributions of this work are threefold: firstly, we extend the application of OSM to long-horizon forecasting, doubling the forecasting horizon compared to previous studies. Secondly, through an expanded receptive field and the integration of intersection priors, our OSM-based approach exhibits competitive performance, narrowing the gap with HD Map-based models. Lastly, we conduct an exhaustive context-aware analysis, providing deeper insights in motion forecasting across diverse scenarios as well as conducting class-aware comparisons. This research not only advances long-term motion forecasting with coarse map representations but additionally offers a potential scalable solution within the domain of autonomous driving."
FoveaCam++: Systems-Level Advances for Long Range Multi-Object High-Resolution Tracking,"Zhang, Yuxuan; Koppal, Sanjeev",,
Real-Time Path Generation and Alignment Control for Autonomous Curb Following,"Wang, Yuanzhe; Dai, Yunxiang; Wang, Danwei",,
Dynamically Modulating Visual Place Recognition Sequence Length For Minimum Acceptable Performance Scenarios,"Malone, Connor; Vora, Ankit; Peynot, Thierry; Milford, Michael J",https://arxiv.org/abs/2407.00863,"Mobile robots and autonomous vehicles are often required to function in environments where critical position estimates from sensors such as GPS become uncertain or unreliable. Single image visual place recognition (VPR) provides an alternative for localization but often requires techniques such as sequence matching to improve robustness, which incurs additional computation and latency costs. Even then, the sequence length required to localize at an acceptable performance level varies widely; and simply setting overly long fixed sequence lengths creates unnecessary latency, computational overhead, and can even degrade performance. In these scenarios it is often more desirable to meet or exceed a set target performance at minimal expense. In this paper we present an approach which uses a calibration set of data to fit a model that modulates sequence length for VPR as needed to exceed a target localization performance. We make use of a coarse position prior, which could be provided by any other localization system, and capture the variation in appearance across this region. We use the correlation between appearance variation and sequence length to curate VPR features and fit a multilayer perceptron (MLP) for selecting the optimal length. We demonstrate that this method is effective at modulating sequence length to maximize the number of sections in a dataset which meet or exceed a target performance whilst minimizing the median length used. We show applicability across several datasets and reveal key phenomena like generalization capabilities, the benefits of curating features and the utility of non-state-of-the-art feature extractors with nuanced properties."
JointLoc: A Real-time Visual Localization Framework for Planetary UAVs Based on Joint Relative and Absolute Pose Estimation,"Luo, Xubo; Wan, Xue; Gao, Yixing; Tian, Yaolin; Zhang, Wei; Shu, Leizheng",https://arxiv.org/abs/2405.07429,"Unmanned aerial vehicles (UAVs) visual localization in planetary aims to estimate the absolute pose of the UAV in the world coordinate system through satellite maps and images captured by on-board cameras. However, since planetary scenes often lack significant landmarks and there are modal differences between satellite maps and UAV images, the accuracy and real-time performance of UAV positioning will be reduced. In order to accurately determine the position of the UAV in a planetary scene in the absence of the global navigation satellite system (GNSS), this paper proposes JointLoc, which estimates the real-time UAV position in the world coordinate system by adaptively fusing the absolute 2-degree-of-freedom (2-DoF) pose and the relative 6-degree-of-freedom (6-DoF) pose. Extensive comparative experiments were conducted on a proposed planetary UAV image cross-modal localization dataset, which contains three types of typical Martian topography generated via a simulation engine as well as real Martian UAV images from the Ingenuity helicopter. JointLoc achieved a root-mean-square error of 0.237m in the trajectories of up to 1,000m, compared to 0.594m and 0.557m for ORB-SLAM2 and ORB-SLAM3 respectively. The source code will be available at https://github.com/LuoXubo/JointLoc."
CurricularVPR: Curricular Contrastive Loss for Visual Place Recognition,"Zhang, Dongshuo; Chen, Nanhua; WU, MEIQING; Lam, Siew Kei",,
Hybrid Continuum-Eversion Robot: Precise Navigation and Decontamination in Nuclear Environments using Vine Robot,"Al-Dubooni, Mohammed; Wong, Cuebong; Althoefer, Kaspar",https://arxiv.org/abs/2404.13135,"Soft growing vine robots show great potential for navigation and decontamination tasks in the nuclear industry. This paper introduces a novel hybrid continuum-eversion robot designed to address certain challenges in relation to navigating and operating within pipe networks and enclosed remote vessels. The hybrid robot combines the flexibility of a soft eversion robot with the precision of a continuum robot at its tip, allowing for controlled steering and movement in hard to access and/or complex environments. The design enables the delivery of sensors, liquids, and aerosols to remote areas, supporting remote decontamination activities. This paper outlines the design and construction of the robot and the methods by which it achieves selective steering. We also include a comprehensive review of current related work in eversion robotics, as well as other steering devices and actuators currently under research, which underpin this novel active steering approach. This is followed by an experimental evaluation that demonstrates the robot's real-world capabilities in delivering liquids and aerosols to remote locations. The experiments reveal successful outcomes, with over 95% success in precision spraying tests. The paper concludes by discussing future work alongside limitations in the current design, ultimately showcasing its potential as a solution for remote decontamination operations in the nuclear industry."
Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR Data,"Kumar, Aakash; Chen, Chen; Mian, Ajmal; Lobo, Niels; Shah, Mubarak",https://arxiv.org/abs/2404.06715,"3D detection is a critical task that enables machines to identify and locate objects in three-dimensional space. It has a broad range of applications in several fields, including autonomous driving, robotics and augmented reality. Monocular 3D detection is attractive as it requires only a single camera, however, it lacks the accuracy and robustness required for real world applications. High resolution LiDAR on the other hand, can be expensive and lead to interference problems in heavy traffic given their active transmissions. We propose a balanced approach that combines the advantages of monocular and point cloud-based 3D detection. Our method requires only a small number of 3D points, that can be obtained from a low-cost, low-resolution sensor. Specifically, we use only 512 points, which is just 1% of a full LiDAR frame in the KITTI dataset. Our method reconstructs a complete 3D point cloud from this limited 3D information combined with a single image. The reconstructed 3D point cloud and corresponding image can be used by any multi-modal off-the-shelf detector for 3D object detection. By using the proposed network architecture with an off-the-shelf multi-modal 3D detector, the accuracy of 3D detection improves by 20% compared to the state-of-the-art monocular detection methods and 6% to 9% compare to the baseline multi-modal methods on KITTI and JackRabbot datasets."
BaRiFlex: A Robotic Gripper with Versatility and Collision Robustness for Robot Learning,"Jeong, Gu-Cheol; Bahety, Arpit; Pedraza, Gabriel; Deshpande, Ashish; Martín-Martín, Roberto",https://arxiv.org/abs/2312.05323,"We present a new approach to robot hand design specifically suited for successfully implementing robot learning methods to accomplish tasks in daily human environments. We introduce BaRiFlex, an innovative gripper design that alleviates the issues caused by unexpected contact and collisions during robot learning, offering robustness, grasping versatility, task versatility, and simplicity to the learning processes. This achievement is enabled by the incorporation of low-inertia actuators, providing high Back-drivability, and the strategic combination of Rigid and Flexible materials which enhances versatility and the gripper's resilience against unpredicted collisions. Furthermore, the integration of flexible Fin-Ray linkages and rigid linkages allows the gripper to execute compliant grasping and precise pinching. We conducted rigorous performance tests to characterize the novel gripper's compliance, durability, grasping and task versatility, and precision. We also integrated the BaRiFlex with a 7 Degree of Freedom (DoF) Franka Emika's Panda robotic arm to evaluate its capacity to support a trial-and-error (reinforcement learning) training procedure. The results of our experimental study are then compared to those obtained using the original rigid Franka Hand and a reference Fin-Ray soft gripper, demonstrating the superior capabilities and advantages of our developed gripper system."
A Safe and Efficient Timed-Elastic-Band Planner for Unstructured Environments,"Xi, Haoyu; Li, Wei; Zhao, Fangzhou; Chen, Liang; Hu, Yu",,
A Biomimetic Robot Crawling Upstream using Adhesive Suckers Inspired by Net-winged Midge Larvae,"Xu, Haoyuan; Zhao, Shuyong; Zhi, Jiale; Bi, Chongze; Wen, Li",,
Supervised Articulation Angles Estimation for Multi-Articulated Vehicles Based on Panoramic Camera System,"Liu, Weimin; Wang, Wenjun; Sun, Zhaocong",,
SGOR: Outlier Removal by Leveraging Semantic and Geometric Information for Robust Point Cloud Registration,"Zhao, Guiyu; Guo, Zhentao; Ma, Hongbin",https://arxiv.org/abs/2407.06297,"In this paper, we introduce a new outlier removal method that fully leverages geometric and semantic information, to achieve robust registration. Current semantic-based registration methods only use semantics for point-to-point or instance semantic correspondence generation, which has two problems. First, these methods are highly dependent on the correctness of semantics. They perform poorly in scenarios with incorrect semantics and sparse semantics. Second, the use of semantics is limited only to the correspondence generation, resulting in bad performance in the weak geometry scene. To solve these problems, on the one hand, we propose secondary ground segmentation and loose semantic consistency based on regional voting. It improves the robustness to semantic correctness by reducing the dependence on single-point semantics. On the other hand, we propose semantic-geometric consistency for outlier removal, which makes full use of semantic information and significantly improves the quality of correspondences. In addition, a two-stage hypothesis verification is proposed, which solves the problem of incorrect transformation selection in the weak geometry scene. In the outdoor dataset, our method demonstrates superior performance, boosting a 22.5 percentage points improvement in registration recall and achieving better robustness under various conditions. Our code is available."
Fast Spatial Reasoning of Implicit 3D maps through Explicit Near-Far Sampling Range Prediction,"Min, Chaerin; Cha, sehyun; Won, Changhee; Lim, Jongwoo",,
Learning the Inverse Kinematics of Magnetic Continuum Robot for Teleoperated Navigation,"Xiang, Pingyu; Qiu, Ke; Sun, Danying; zhang, jingyu; Fang, Qin; Mi, Xiangyu; Chen, Mengxiao; Wang, Yue; Xiong, Rong; Lu, Haojian",,
Immersive Human-in-the-Loop Control: Real-Time 3D Surface Meshing and Physics Simulation,"Akturk, Sait; Valentine, Justin; Ahmad, Junaid; Jagersand, Martin",,
An Optimization-Based Planner with B-spline Parameterized Continuous-Time Reference Signals,"Tao, Chuyuan; Cheng, Sheng; Zhao, Yang; Wang, Fanxin; HOVAKIMYAN, NAIRA",https://arxiv.org/abs/2404.00133,"For the cascaded planning and control modules implemented for robot navigation, the frequency gap between the planner and controller has received limited attention. In this study, we introduce a novel B-spline parameterized optimization-based planner (BSPOP) designed to address the frequency gap challenge with limited onboard computational power in robots. The proposed planner generates continuous-time control inputs for low-level controllers running at arbitrary frequencies to track. Furthermore, when considering the convex control action sets, BSPOP uses the convex hull property to automatically constrain the continuous-time control inputs within the convex set. Consequently, compared with the discrete-time optimization-based planners, BSPOP reduces the number of decision variables and inequality constraints, which improves computational efficiency as a byproduct. Simulation results demonstrate that our approach can achieve a comparable planning performance to the high-frequency baseline optimization-based planners while demanding less computational power. Both simulation and experiment results show that the proposed method performs better in planning compared with baseline planners in the same frequency."
Voltage Regulation in Polymer Electrolyte Fuel Cell Systems Using Gaussian Process Model Predictive Control,"Li, Xiufei; Yang, Miao; Qi, Yuanxin; Li, Zhuowei; zhang, miao",https://arxiv.org/abs/2403.16170,"This study introduces a novel approach utilizing Gaussian process model predictive control (MPC) to stabilize the output voltage of a polymer electrolyte fuel cell (PEFC) system by simultaneously regulating hydrogen and airflow rates. Two Gaussian process models are developed to capture PEFC dynamics, taking into account constraints including hydrogen pressure and input change rates, thereby aiding in mitigating errors inherent to PEFC predictive control. The dynamic performance of the physical model and Gaussian process MPC in constraint handling and system inputs is compared and analyzed. Simulation outcomes demonstrate that the proposed Gaussian process MPC effectively maintains the voltage at the target 48 V while adhering to safety constraints, even amidst workload disturbances ranging from 110-120 A. In comparison to traditional MPC using detailed system models, Gaussian process MPC exhibits a 43\% higher overshoot and 25\% slower response time. Nonetheless, it offers the advantage of not requiring the underlying true system model and needing less system information."
Modeling and Gait Analysis of Passive Rimless Wheel with Compliant Feet,"Zheng, Yanqiu; Yan, Cong; HE, YUETONG; Asano, Fumihiko; Tokuda, Isao",,
Task Planning for Long-Horizon Cooking Tasks Based on Large Language Models,"Shin, Jungkyoo; Han, Jieun; Kim, Seungjun; Oh, Yoonseon; Kim, Eunwoo",,
Contrastive Mask Denoising Transformer for 3D Instance Segmentation,"Wang, He; Lin, Minshen; Zhang, Guofeng",,
Real-time Hazard Prediction in Connected Autonomous Vehicles: A Digital Twin Approach,"Barroso Ramírez, Sergio; Zapata Cornejo, Noé José; Pérez González, Gerardo; Bustos, Pablo; Núñez, Pedro",,
CompdVision: Combining Near-Field 3D Visual and Tactile Sensing Using a Compact Compound-Eye Imaging System,"Luo, Lifan; Zhang, Boyang; Peng, Zhijie; Cheung, Yik Kin; Zhang, Guanlan; Li, Zhigang; Wang, Michael Yu; Yu, Hongyu",https://arxiv.org/abs/2312.07146,"As automation technologies advance, the need for compact and multi-modal sensors in robotic applications is growing. To address this demand, we introduce CompdVision, a novel sensor that employs a compound-eye imaging system to combine near-field 3D visual and tactile sensing within a compact form factor. CompdVision utilizes two types of vision units to address diverse sensing needs, eliminating the need for complex modality conversion. Stereo units with far-focus lenses can see through the transparent elastomer for depth estimation beyond the contact surface. Simultaneously, tactile units with near-focus lenses track the movement of markers embedded in the elastomer to obtain contact deformation. Experimental results validate the sensor's superior performance in 3D visual and tactile sensing, proving its capability for reliable external object depth estimation and precise measurement of tangential and normal contact forces. The dual modalities and compact design make the sensor a versatile tool for robotic manipulation."
Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in Event Cameras,"B Nair, Gokul; Milford, Michael J; Fischer, Tobias",https://arxiv.org/abs/2403.16425,"Event cameras are increasingly popular in robotics due to beneficial features such as low latency, energy efficiency, and high dynamic range. Nevertheless, their downstream task performance is greatly influenced by the optimization of bias parameters. These parameters, for instance, regulate the necessary change in light intensity to trigger an event, which in turn depends on factors such as the environment lighting and camera motion. This paper introduces feedback control algorithms that automatically tune the bias parameters through two interacting methods: 1) An immediate, on-the-fly \textit{fast} adaptation of the refractory period, which sets the minimum interval between consecutive events, and 2) if the event rate exceeds the specified bounds even after changing the refractory period repeatedly, the controller adapts the pixel bandwidth and event thresholds, which stabilizes after a short period of noise events across all pixels (\textit{slow} adaptation). Our evaluation focuses on the visual place recognition task, where incoming query images are compared to a given reference database. We conducted comprehensive evaluations of our algorithms' adaptive feedback control in real-time. To do so, we collected the QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366 repeated traversals of a Scout Mini robot navigating through a 100 meter long indoor lab setting (totaling over 35km distance traveled) in varying brightness conditions with ground truth location information. Our proposed feedback controllers result in superior performance when compared to the standard bias settings and prior feedback control methods. Our findings also detail the impact of bias adjustments on task performance and feature ablation studies on the fast and slow adaptation mechanisms."
GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot,"Song, Wenxuan; Zhao, Han; Ding, Pengxiang; Cui, Can; Lyu, Shangke; Fan, YaNing; Wang, Donglin",https://arxiv.org/abs/2403.13358,"Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues and difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a transformer-based VLA network to process multi-modal inputs and output actions. By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also validating its efficiency in both training and inference processes. Additionally, we uncover its potential to acquire emergent skills. Additionally, we contribute the QUARD-Auto dataset, collected automatically to support our training approach and foster advancements in multi-task quadruped robot learning. This work presents a new paradigm for reducing the cost of collecting robot data and driving progress in the multi-task learning community. You can reach our project and video through the link: https://songwxuan.github.io/GeRM/ ."
3D Object Detection via Stereo Pyramid Transformers with Rich Semantic Feature Fusion,"Gu, Rongqi; Yang, Chu; Lu, Yaohan; Liu, Peigen; WU, FEI; Chen, Guang",,
Sequential Convex Programming for Time-optimal Quadrotor Waypoint Flight,"Shen, Zhipeng; Zhou, Guanzhong; Huang, Hailong",,
A Unified Interaction Control Framework for Safe Robotic Ultrasound Scanning with Human-Intention-Aware Compliance,"Yan, Xiangjie; Shaqi, Luo; Jiang, Yongpeng; Yu, Mingrui; Chen, Chen; Huang, Gao; LI, Xiang",,
Transformer-Based Relationship Inference Model for Household Object Organization by Integrating Graph Topology and Ontology,"Li, Xiaodong; Tian, Guohui; Cui, yongcheng; Gu, Yu",,
Safe and Efficient Auto-tuning to Cross Sim-to-real Gap for Bipedal Robot,"Du, Yidong; Chen, Xuechao; YU, Zhangguo; Zhang, YuanXi; zhou, zishun; Huang, Qiang",,
FlowTrack: Point-level Flow Network for 3D Single Object Tracking,"Li, Shuo; Cui, Yubo; Li, Zhiheng; Fang, Zheng",https://arxiv.org/abs/2407.01959,"3D single object tracking (SOT) is a crucial task in fields of mobile robotics and autonomous driving. Traditional motion-based approaches achieve target tracking by estimating the relative movement of target between two consecutive frames. However, they usually overlook local motion information of the target and fail to exploit historical frame information effectively. To overcome the above limitations, we propose a point-level flow method with multi-frame information for 3D SOT task, called FlowTrack. Specifically, by estimating the flow for each point in the target, our method could capture the local motion details of target, thereby improving the tracking performance. At the same time, to handle scenes with sparse points, we present a learnable target feature as the bridge to efficiently integrate target information from past frames. Moreover, we design a novel Instance Flow Head to transform dense point-level flow into instance-level motion, effectively aggregating local motion information to obtain global target motion. Finally, our method achieves competitive performance with improvements of 5.9% on the KITTI dataset and 2.9% on NuScenes. The code will be made publicly available soon."
Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern,"Yoon, Jeonghyeon; Park, Junhyun; Park, Hyojae; Lee, Hakyoon; Lee, Sang Won; Hwang, Minho",https://arxiv.org/abs/2402.16101,"In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot base is crucial for successful surgery. Improper placement can hinder performance because of manipulator limitations and inaccessible workspaces. Conventional base placement relies on the experience of trained medical staff. This study proposes a novel method for determining the optimal base pose based on the surgeon's working pattern. The proposed method analyzes recorded end-effector poses using a machine learning-based clustering technique to identify key positions and orientations preferred by the surgeon. We introduce two scoring metrics to address the joint limit and singularity issues: joint margin and manipulability scores. We then train a multi-layer perceptron regressor to predict the optimal base pose based on these scores. Evaluation in a simulated environment using the da Vinci Research Kit shows unique base pose score maps for four volunteers, highlighting the individuality of the working patterns. Results comparing with 20,000 randomly selected base poses suggest that the score obtained using the proposed method is 28.2% higher than that obtained by random base placement. These results emphasize the need for operator-specific optimization during base placement in RAMIS."
Reinforcement Learning with Generalizable Gaussian Splatting,"Wang, Jiaxu; Zhang, Qiang; SUN, Jingkai; Cao, Jiahang; Han, Gang; Zhao, Wen; ZHANG, Weining; Guo, Yijie; Shao, Yecheng; Xu, Renjing",https://arxiv.org/abs/2404.07950,"An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box"", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian Splatting framework to be the representation of RL tasks, called GSRL. Through validation in the RoboMimic environment, our method achieves better results than other baselines in multiple tasks, improving the performance by 10%, 44%, and 15% compared with baselines on the hardest task. This work is the first attempt to leverage generalizable 3DGS as a representation for RL."
Asynchronous Event-Inertial Odometry using a Unified Gaussian Process Regression Framework,"Li, Xudong; Wang, Zhixiang; Zhang, Yizhai; Zhang, Fan; Huang, Panfeng",,
Benchmarking Smoothness and Reducing High-Frequency Oscillations in Continuous Control Policies,"Galelli Christmann, Guilherme Henrique; Luo, Ying-Sheng; Mandala, Hanjaya; Chen, Wei-Chao",,
A Soft Robotic Finger Inspired by Biological Perception Models for Tactile Sensing,"mao, baijin; yuan, qiangjing; xiang, yuyaocen; zhou, kunyu; wang, weichen; chen, yaozhen; hao, hongwei; Qu, Juntian",,
I-ASM: Iterative Acoustic Scene Mapping for Enhanced Robot Auditory Perception in Complex Indoor Environments,"Fu, Linya; He, Yuanzheng; Wang, Jiang; Qiao, Xu; Kong, He",,
Environmental and Behavioral Imitation for Autonomous Navigation,"Aoki, Junki; Sasaki, Fumihiro; Matsumoto, Kohei; Yamashina, Ryota; Kurazume, Ryo",https://arxiv.org/abs/2005.01935,"All-day and all-weather navigation is a critical capability for autonomous driving, which requires proper reaction to varied environmental conditions and complex agent behaviors. Recently, with the rise of deep learning, end-to-end control for autonomous vehicles has been well studied. However, most works are solely based on visual information, which can be degraded by challenging illumination conditions such as dim light or total darkness. In addition, they usually generate and apply deterministic control commands without considering the uncertainties in the future. In this paper, based on imitation learning, we propose a probabilistic driving model with ultiperception capability utilizing the information from the camera, lidar and radar. We further evaluate its driving performance online on our new driving benchmark, which includes various environmental conditions (e.g., urban and rural areas, traffic densities, weather and times of the day) and dynamic obstacles (e.g., vehicles, pedestrians, motorcyclists and bicyclists). The results suggest that our proposed model outperforms baselines and achieves excellent generalization performance in unseen environments with heavy traffic and extreme weather."
Uncertainty-aware Deep Imitation Learning and Deployment for Autonomous Navigation through Crowded Intersections,"Zhu, Zeyu; Wang, Shuai; Zhao, Huijing",,
Predictive Coding for Decision Transformer,"Luu, Tung; Lee, Donghoon; Yoo, Chang D.",https://arxiv.org/abs/2107.08031,"The human driver is no longer the only one concerned with the complexity of the driving scenarios. Autonomous vehicles (AV) are similarly becoming involved in the process. Nowadays, the development of AVs in urban places raises essential safety concerns for vulnerable road users (VRUs) such as pedestrians. Therefore, to make the roads safer, it is critical to classify and predict the pedestrians' future behavior. In this paper, we present a framework based on multiple variations of the Transformer models able to infer predict the pedestrian street-crossing decision-making based on the dynamics of its initiated trajectory. We showed that using solely bounding boxes as input features can outperform the previous state-of-the-art results by reaching a prediction accuracy of 91\% and an F1-score of 0.83 on the PIE dataset. In addition, we introduced a large-size simulated dataset (CP2A) using CARLA for action prediction. Our model has similarly reached high accuracy (91\%) and F1-score (0.91) on this dataset. Interestingly, we showed that pre-training our Transformer model on the CP2A dataset and then fine-tuning it on the PIE dataset is beneficial for the action prediction task. Finally, our model's results are successfully supported by the ""human attention to bounding boxes"" experiment which we created to test humans ability for pedestrian action prediction without the need for environmental context. The code for the dataset and the models is available at: https://github.com/linaashaji/Action_Anticipation"
Time-Optimal TCP and Robot Base Placement for Pick-and-Place Tasks in Highly Constrained Environments,"Wachter, Alexander; Kugi, Andreas; Hartl-Nesic, Christian",,
FDNet: Feature Decoupling Framework for Trajectory Prediction,"Li, Yuhang; Li, Changsheng; Fan, Baoyu; Li, Rongqing; Zhang, Ziyue; Ren, Dongchun; Yuan, Ye; Wang, Guoren",,
Online Hand Movement Recognition System with EEG-EMG Fusion Using One-Dimensional Convolutional Neural Network,"Wang, Haozheng; Jia, Hao; Sun, Zhe; Duan, Feng",,
Energy-Optimized Planning in Non-Uniform Wind Fields with Fixed-Wing Aerial Vehicles,"Duan, Yufei; Achermann, Florian; Lim, Jaeyoung; Siegwart, Roland",https://arxiv.org/abs/2404.02077,"Fixed-wing small uncrewed aerial vehicles (sUAVs) possess the capability to remain airborne for extended durations and traverse vast distances. However, their operation is susceptible to wind conditions, particularly in regions of complex terrain where high wind speeds may push the aircraft beyond its operational limitations, potentially raising safety concerns. Moreover, wind impacts the energy required to follow a path, especially in locations where the wind direction and speed are not favorable. Incorporating wind information into mission planning is essential to ensure both safety and energy efficiency. In this paper, we propose a sampling-based planner using the kinematic Dubins aircraft paths with respect to the ground, to plan energy-efficient paths in non-uniform wind fields. We study the planner characteristics with synthetic and real-world wind data and compare its performance against baseline cost and path formulations. We demonstrate that the energy-optimized planner effectively utilizes updrafts to minimize energy consumption, albeit at the expense of increased travel time. The ground-relative path formulation facilitates the generation of safe trajectories onboard sUAVs within reasonable computational timeframes."
MLPER: Multi-Level Prompts for Adaptively Enhancing Vision-Language Emotion Recognition,"Gao, Yu; Ren, Weihong; Xu, Xinglong; wang, yan; Wang, Zhiyong; Liu, Honghai",,
VRExplorer: An Efficient View-Region based Autonomous Exploration Method in Unknown Environments for UAV,"Xu, Kai; Zheng, Lanxiang; Wei, Mingxin; Cheng, Hui",,
BonnBeetClouds3D: A Dataset Towards Point Cloud-Based Organ-Level Phenotyping of Sugar Beet Plants Under Real Field Conditions,"Marks, Elias Ariel; Bömer, Jonas; Magistri, Federico; Sah, Anurag; Behley, Jens; Stachniss, Cyrill",,
Tightly-Coupled Factor Graph Formulation For Radar-Inertial Odometry,"Michalczyk, Jan; Quell, Julius Karsten Oskar; Steidle, Florian; Müller, Marcus Gerhard; Weiss, Stephan",https://arxiv.org/abs/2403.05332,"Enabling autonomous robots to operate robustly in challenging environments is necessary in a future with increased autonomy. For many autonomous systems, estimation and odometry remains a single point of failure, from which it can often be difficult, if not impossible, to recover. As such robust odometry solutions are of key importance. In this work a method for tightly-coupled LiDAR-Radar-Inertial fusion for odometry is proposed, enabling the mitigation of the effects of LiDAR degeneracy by leveraging a complementary perception modality while preserving the accuracy of LiDAR in well-conditioned environments. The proposed approach combines modalities in a factor graph-based windowed smoother with sensor information-specific factor formulations which enable, in the case of degeneracy, partial information to be conveyed to the graph along the non-degenerate axes. The proposed method is evaluated in real-world tests on a flying robot experiencing degraded conditions including geometric self-similarity as well as obscurant occlusion. For the benefit of the community we release the datasets presented: https://github.com/ntnu-arl/lidar_degeneracy_datasets."
Domain Adaptation in Visual Reinforcement Learning via Self-Expert Imitation with Purifying Latent Feature,"Chen, Lin; Huang, Jianan; zhou, zhen; Mo, Yang; Wang, Yaonan; Miao, Zhiqiang; zeng, kia; Feng, Mingtao; Wang, Danwei",,
Development of a Super-thin and Fast Omnidirectional Treadmill through a Novel Helical Transmission Mechanism,"Pyo, Sanghun; Choi, Jinsun; Yoon, Jungwon",,
Three-Dimensional Vehicle Dynamics State Estimation for High-Speed Race Cars under varying Signal Quality,"Goblirsch, Sven; Weinmann, Marcel; Betz, Johannes",https://arxiv.org/abs/2408.14885,"This work aims to present a three-dimensional vehicle dynamics state estimation under varying signal quality. Few researchers have investigated the impact of three-dimensional road geometries on the state estimation and, thus, neglect road inclination and banking. Especially considering high velocities and accelerations, the literature does not address these effects. Therefore, we compare two- and three-dimensional state estimation schemes to outline the impact of road geometries. We use an Extended Kalman Filter with a point-mass motion model and extend it by an additional formulation of reference angles. Furthermore, virtual velocity measurements significantly improve the estimation of road angles and the vehicle's side slip angle. We highlight the importance of steady estimations for vehicle motion control algorithms and demonstrate the challenges of degraded signal quality and Global Navigation Satellite System dropouts. The proposed adaptive covariance facilitates a smooth estimation and enables stable controller behavior. The developed state estimation has been deployed on a high-speed autonomous race car at various racetracks. Our findings indicate that our approach outperforms state-of-the-art vehicle dynamics state estimators and an industry-grade Inertial Navigation System. Further studies are needed to investigate the performance under varying track conditions and on other vehicle types."
DuCAS: a knowledge-enhanced dual-hand compositional action segmentation method for human-robot collaborative assembly,"Zheng, Hao; Lee, Regina; Liang, Huachang; Lu, Yuqian; Xu, Xun",,
Compliant Blind Handover Control for Human-Robot Collaboration,"Ferrari, Davide; Pupa, Andrea; Secchi, Cristian",https://arxiv.org/abs/2409.07155,"This paper presents a Human-Robot Blind Handover architecture within the context of Human-Robot Collaboration (HRC). The focus lies on a blind handover scenario where the operator is intentionally faced away, focused in a task, and requires an object from the robot. In this context, it is imperative for the robot to autonomously manage the entire handover process. Key considerations include ensuring safety while handing the object to the operator's hand, and detect the proper timing to release the object. The article explores strategies to navigate these challenges, emphasizing the need for a robot to operate safely and independently in facilitating blind handovers, thereby contributing to the advancement of HRC protocols and fostering a natural and efficient collaboration between humans and robots."
A Hybrid Human Tracking System using UWB Sensors and Monocular Visual Data Fusion for Human Following Robots,"Zhang, Dingzhi; Birner, Lukas; Pancheri, Felix; Rehekampff, Christoph; Lueth, Tim C.",,
Skeleton-Based Human Action Recognition with Noisy Labels,"Xu, Yi; Peng, Kunyu; Wen, Di; LIU, Ruiping; Zheng, Junwei; Chen, Yufan; Zhang, Jiaming; Roitberg, Alina; Yang, Kailun; Stiefelhagen, Rainer",https://arxiv.org/abs/2403.09975,"Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model's training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial benchmark. Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodology, NoiseEraSAR, which integrates global sample selection, co-teaching, and Cross-Modal Mixture-of-Experts (CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise. Our proposed approach demonstrates better performance on the established benchmark, setting new state-of-the-art standards. The source code for this study is accessible at https://github.com/xuyizdby/NoiseEraSAR."
UWB-Based Localization System Considering Antenna Anisotropy and NLOS/Multipath Conditions,"Kim, Taekyun; Yoon, Byoungkwon; Lee, Dongjun",,
"VIRUS-NeRF - Vision, InfraRed and UltraSonic based Neural Radiance Fields","Schmid, Nicolaj; von Einem, Cornelius; Cadena Lerma, Cesar; Siegwart, Roland; Hruby, Lorenz; Tschopp, Florian",https://arxiv.org/abs/2403.09477,"Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations. Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, while in larger ones, it is bounded by the utilized ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic and infrared sensors is highly effective when dealing with sparse data and low view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the mapping capabilities and increases the training speed by 46% compared to Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for cost-effective local mapping in mobile robotics, with potential applications in safety and navigation tasks. The code can be found at https://github.com/ethz-asl/virus nerf."
Development and Functional Evaluation of The PrHand V2 Soft-Robotics Prosthetic Hand,"Ramos, Orion Yari Santiago; De Arco, Laura; Munera, Marcela; Robledo, Jorge; Moazen, Mehran; Wurdemann, Helge Arne; Cifuentes, Carlos A.",,
On performing non-prehensile rolling manipulations: Stabilizing synchronous motions of Butterfly robots,"Surov, Maksim; Pchelkin, Stepan; Shiriaev, Anton; Gusev, Sergei V.; Freidovich, Leonid",,
Assessing Monocular Depth Estimation Networks for UAS Deployment in Rainforest Environments,"Tangellapalli, Srisai Anirudh; Sangha, Harman Singh; Peschel, Joshua; Duncan, Brittany",,
Shadow Maintenance for Automatic Light-Probe Control in Ophthalmic Surgeries Using Only 2D information,"Yang, Junjie; Inagaki, Satoshi; Zhao, Zhihao; Zapp, Daniel; Maier, Mathias; Huang, Kai; Navab, Nassir; Nasseri, M. Ali",,
A Language-Driven Navigation Strategy Integrating Semantic Maps and Large Language Models,"Zhong, Zhengjun; He, Ying; Li, Pengteng; Yu, Fei; Ma, Fei",,
Collaborative Conversation in Safe Multimodal Human-Robot Collaboration,"Ferrari, Davide; Pupa, Andrea; Secchi, Cristian",https://arxiv.org/abs/2409.07158,"In the context of Human-Robot Collaboration (HRC), it is crucial that the two actors are able to communicate with each other in a natural and efficient manner. The absence of a communication interface is often a cause of undesired slowdowns. On one hand, this is because unforeseen events may occur, leading to errors. On the other hand, due to the close contact between humans and robots, the speed must be reduced significantly to comply with safety standard ISO/TS 15066. In this paper, we propose a novel architecture that enables operators and robots to communicate efficiently, emulating human-to-human dialogue, while addressing safety concerns. This approach aims to establish a communication framework that not only facilitates collaboration but also reduces undesired speed reduction. Through the use of a predictive simulator, we can anticipate safety-related limitations, ensuring smoother workflows, minimizing risks, and optimizing efficiency. The overall architecture has been validated with a UR10e and compared with a state of the art technique. The results show a significant improvement in user experience, with a corresponding 23% reduction in execution times and a 50% decrease in robot downtime."
Roaming with Robots: Utilizing Artificial Curiosity in Global Path Planning for Autonomous Mobile Robots,"Spielbauer, Niklas; Laube, Till Jasper; Oberacker, David; Roennau, Arne; Dillmann, Rüdiger",,
Automating ROS2 Security Policies Extraction through Static Analysis,"Zanatta, Giacomo; Caiazza, Gianluca; Ferrara, Pietro; Negrini, Luca; White, Ruffin",,
Modelling and Analysis of Joint-to-End Variable Stiffness for Cable-Driven Hyper-Redundant Manipulator,"Zhang, hongyang; Wang, Shuting; Li, Hu; Xie, Yuanlong",,
Adaptive Smith Predictor Fractional Control of a Tele-operated Flexible Link Robot,"Gharab, Saddam; Ben Ftima, Salma; Feliu, Vicente",,
Origami Actuator with Tunable Limiting Layer for Reconfigurable Soft Robotic Grasping,"Yang, Yang; Kejin, Zhu; Xie, Yuan; Yan, Shaoyang; Yi, Juan; Jiang, Pei; Li, Yunquan; Li, Yingtian",,
Research of calibration method for fusion of LDS sensor and ToF low-cost sensor,"Zhu, Jiahui; Yu, Guitao; He, Yang; Yang, Kui; Liang, Dongtai",,
Energy Sharing Mechanism for Freeform Robots Utilizing Conductive Spherical Sliding Surfaces,"Li, Xin-Zhuo; Tu, Yuxiao; Liang, Guanqi; Wu, Di; Lam, Tin Lun",,
Exploring 3D Human Pose Estimation and Forecasting from the Robots Perspective: The HARPER Dataset,"Avogaro, Andrea; Toaiari, Andrea; Cunico, Federico; Xu, Xiangmin; Dafas, Haralambos; Vinciarelli, Alessandro; Li, Liying Emma; Cristani, Marco",https://arxiv.org/abs/2403.14447,"We introduce HARPER, a novel dataset for 3D body pose estimation and forecast in dyadic interactions between users and Spot, the quadruped robot manufactured by Boston Dynamics. The key-novelty is the focus on the robot's perspective, i.e., on the data captured by the robot's sensors. These make 3D body pose analysis challenging because being close to the ground captures humans only partially. The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users. The Corpus contains not only the recordings of the built-in stereo cameras of Spot, but also those of a 6-camera OptiTrack system (all recordings are synchronized). This leads to ground-truth skeletal representations with a precision lower than a millimeter. In addition, the Corpus includes reproducible benchmarks on 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches. This enables future HARPER users to rigorously compare their results with those we provide in this work."
SCOML: Trajectory Planning Based on Self-Correcting Meta-Reinforcement Learning in Hybird Terrain for Mobile Robot,"Yang, Andong; Li, Wei; Hu, Yu",,
Efficient Path Planning for Modular Reconfigurable Robots,"Mayer, Matthias; Li, Zihao; Althoff, Matthias",,
Adaptive Feedforward Super-Twisting Sliding Mode Control of Parallel Kinematic Manipulators With Real-Time Experiments,"Saied, Hussein; Chemori, Ahmed; Bouri, Mohamed; EL RAFEI, Maher; Francis, Clovis",,
Inline Photometrically Calibrated Hybrid Visual SLAM,"Abboud, Nicolas; Sayour, Malak; Elhajj, Imad; Zelek, John S.; Asmar, Daniel",,
Self-supervised Monocular Depth Estimation in Challenging Environments Based On Illumination Compensation PoseNet,"Hou, Shengyu; Song, Wenjie; Wang, Rongchuan; Wang, Meiling; Yang, Yi; Fu, Mengyin",,
Data-Driven Modeling of Cable Slab Dynamics via Neural Networks,"al-rawashdeh, Yazan; Al Saaideh, Mohammad; Pumphrey, Michael Joseph; Alatawneh, Natheer; Al Janaideh, Mohammad",,
Robot Traversability Prediction: Towards Third-Person-View Extension of Walk2Map with Photometric and Physical Constraints,"Tay Yu Liang, Jonathan; Tanaka, Kanji",,
A Laser-Induced Graphene-Based Flexible Multimodal Sensor for Material and Texture Perception,"Duo, Youning; Duan, Jinxi; Chen, Xingyu; Liu, Wenbo; Wang, Shengxue; Wen, Li",,
BEV$^2$PR: BEV-Enhanced Visual Place Recognition with Structural Cues,"Ge, Fudong; Zhang, Yiwei; Shen, Shuhan; Hu, Weiming; Wang, Yue; Gao, Jin",https://arxiv.org/abs/2403.06600,"In this paper, we propose a new image-based visual place recognition (VPR) framework by exploiting the structural cues in bird's-eye view (BEV) from a single monocular camera. The motivation arises from two key observations about place recognition methods based on both appearance and structure: 1) For the methods relying on LiDAR sensors, the integration of LiDAR in robotic systems has led to increased expenses, while the alignment of data between different sensors is also a major challenge. 2) Other image-/camera-based methods, involving integrating RGB images and their derived variants (eg, pseudo depth images, pseudo 3D point clouds), exhibit several limitations, such as the failure to effectively exploit the explicit spatial relationships between different objects. To tackle the above issues, we design a new BEV-enhanced VPR framework, namely BEV$^2$PR, generating a composite descriptor with both visual cues and spatial awareness based on a single camera. The key points lie in: 1) We use BEV features as an explicit source of structural knowledge in constructing global features. 2) The lower layers of the pre-trained backbone from BEV generation are shared for visual and structural streams in VPR, facilitating the learning of fine-grained local features in the visual stream. 3) The complementary visual and structural features can jointly enhance VPR performance. Our BEV$^2$PR framework enables consistent performance improvements over several popular aggregation modules for RGB global features. The experiments on our collected VPR-NuScenes dataset demonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP baseline to achieve the best performance in our setting, and notably, a 18.06% gain on the hard set. The code and dataset will be available at https://github.com/FudongGe/BEV2PR."
Multi-Stage Monte Carlo Tree Search for Non-Monotone Object Rearrangement Planning in Narrow Confined Environments,"Ren, Hanwen; Qureshi, Ahmed H.",https://arxiv.org/abs/2305.17175,"Non-monotone object rearrangement planning in confined spaces such as cabinets and shelves is a widely occurring but challenging problem in robotics. Both the robot motion and the available regions for object relocation are highly constrained because of the limited space. This work proposes a Multi-Stage Monte Carlo Tree Search (MS-MCTS) method to solve non-monotone object rearrangement planning problems in confined spaces. Our approach decouples the complex problem into simpler subproblems using an object stage topology. A subgoal-focused tree expansion algorithm that jointly considers the high-level planning and the low-level robot motion is designed to reduce the search space and better guide the search process. By fitting the task into the MCTS paradigm, our method produces optimistic solutions by balancing exploration and exploitation. The experiments demonstrate that our method outperforms the existing methods in terms of the planning time, the number of steps, and the total move distance. Moreover, we deploy our MS-MCTS to a real-world robot system and verify its performance in different scenarios."
Multi-agent Traffic Prediction via Denoised Endpoint Distribution,"Liu, Yao; Wang, Ruoyu; Cao, Yuanjiang; Sheng, Quan Z.; Yao, Lina",https://arxiv.org/abs/2405.07041,"The exploration of high-speed movement by robots or road traffic agents is crucial for autonomous driving and navigation. Trajectory prediction at high speeds requires considering historical features and interactions with surrounding entities, a complexity not as pronounced in lower-speed environments. Prior methods have assessed the spatio-temporal dynamics of agents but often neglected intrinsic intent and uncertainty, thereby limiting their effectiveness. We present the Denoised Endpoint Distribution model for trajectory prediction, which distinctively models agents' spatio-temporal features alongside their intrinsic intentions and uncertainties. By employing Diffusion and Transformer models to focus on agent endpoints rather than entire trajectories, our approach significantly reduces model complexity and enhances performance through endpoint information. Our experiments on open datasets, coupled with comparison and ablation studies, demonstrate our model's efficacy and the importance of its components. This approach advances trajectory prediction in high-speed scenarios and lays groundwork for future developments."
Monocular Event-Inertial Odometry with Adaptive decay-based Time Surface and Polarity-aware Tracking,"Tang, Kai; Lang, Xiaolei; Ma, Yukai; Huang, Yuehao; Li, Laijian; Liu, Yong; Lv, Jiajun",,
Bridging the Gap to Natural Language-based Grasp Predictions through Semantic Information Extraction,"Kleer, Niko; Feick, Martin; Gomaa, Amr; Feld, Michael; Krüger, Antonio",,
Development of a Modular Robotic Finger for Gripping Various Shaped Objects,"Kim, Jisu; Cho, Jinman; Kang, Yeon; LEE, Changhwa; Yun, Dongwon",,
Differentiable Collision-Free Parametric Corridors,"Arrizabalaga, Jon; Manchester, Zachary; Ryll, Markus",https://arxiv.org/abs/2407.12283,"This paper presents a method to compute differentiable collision-free parametric corridors. In contrast to existing solutions that decompose the obstacle-free space into multiple convex sets, the continuous corridors computed by our method are smooth and differentiable, making them compatible with existing numerical techniques for learning and optimization. To achieve this, we represent the collision-free corridors as a path-parametric off-centered ellipse with a polynomial basis. We show that the problem of maximizing the volume of such corridors is convex, and can be efficiently solved. To assess the effectiveness of the proposed method, we examine its performance in a synthetic case study and subsequently evaluate its applicability in a real-world scenario from the KITTI dataset."
Attitude Control of the Hydrobatic Intervention AUV Cuttlefish Using Incremental Nonlinear Dynamic Inversion,"Slawik, Tom; Vyas, Shubham; Christensen, Leif; Kirchner, Frank",,
CoT-TL: Low-Resource Temporal Knowledge Representation of Planning Instructions using Chain-of-Thought Reasoning,"Manas, Kumar; Zwicklbauer, Stefan; Paschke, Adrian",,
DCSANet: Dual Cross-channel and Spatial Attention Make RGB-T Object Detection Better,"Lan, Xiaoxiong; Liu, Shenghao; Zhang, Zhiyong; Qiu, Changzhen",,
BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation,"Schramm, Jonas; Vödisch, Niclas; Petek, Kürsat; Ravi, Kiran; Yogamani, Senthil; Burgard, Wolfram; Valada, Abhinav",https://arxiv.org/abs/2403.11761,"Semantic scene segmentation from a bird's-eye-view (BEV) perspective plays a crucial role in facilitating planning and decision-making for mobile robots. Although recent vision-only methods have demonstrated notable advancements in performance, they often struggle under adverse illumination conditions such as rain or nighttime. While active sensors offer a solution to this challenge, the prohibitively high cost of LiDARs remains a limiting factor. Fusing camera data with automotive radars poses a more inexpensive alternative but has received less attention in prior research. In this work, we aim to advance this promising avenue by introducing BEVCar, a novel approach for joint BEV object and map segmentation. The core novelty of our approach lies in first learning a point-based encoding of raw radar data, which is then leveraged to efficiently initialize the lifting of image features into the BEV space. We perform extensive experiments on the nuScenes dataset and demonstrate that BEVCar outperforms the current state of the art. Moreover, we show that incorporating radar information significantly enhances robustness in challenging environmental conditions and improves segmentation performance for distant objects. To foster future research, we provide the weather split of the nuScenes dataset used in our experiments, along with our code and trained models at http://bevcar.cs.uni-freiburg.de."
DecAP : Decaying Action Priors for Accelerated Learning of Torque-Based Legged Locomotion Policies,"Sood, Shivam; SUN, GE; LI, Peizhuo; Sartoretti, Guillaume Adrien",https://arxiv.org/abs/2310.05714,"Optimal Control for legged robots has gone through a paradigm shift from position-based to torque-based control, owing to the latter's compliant and robust nature. In parallel to this shift, the community has also turned to Deep Reinforcement Learning (DRL) as a promising approach to directly learn locomotion policies for complex real-life tasks. However, most end-to-end DRL approaches still operate in position space, mainly because learning in torque space is often sample-inefficient and does not consistently converge to natural gaits. To address these challenges, we propose a two-stage framework. In the first stage, we generate our own imitation data by training a position-based policy, eliminating the need for expert knowledge to design optimal controllers. The second stage incorporates decaying action priors, a novel method to enhance the exploration of torque-based policies aided by imitation rewards. We show that our approach consistently outperforms imitation learning alone and is robust to scaling these rewards from 0.1x to 10x. We further validate the benefits of torque control by comparing the robustness of a position-based policy to a position-assisted torque-based policy on a quadruped (Unitree Go1) without any domain randomization in the form of external disturbances during training."
A Two-Stage Reinforcement Learning Approach for Robot Navigation in Long-range Indoor Dense Crowd Environments,"Jing, Xinghui; Xiong, Xin; Li, Fuhao; Zhang, Tao; Zeng, Long",,
UMAD: University of Macau Anomaly Detection Benchmark Dataset,"Li, Dong; Chen, Lineng; Xu, Chengzhong; Kong, Hui",https://arxiv.org/abs/2408.12527,"Anomaly detection is critical in surveillance systems and patrol robots by identifying anomalous regions in images for early warning. Depending on whether reference data are utilized, anomaly detection can be categorized into anomaly detection with reference and anomaly detection without reference. Currently, anomaly detection without reference, which is closely related to out-of-distribution (OoD) object detection, struggles with learning anomalous patterns due to the difficulty of collecting sufficiently large and diverse anomaly datasets with the inherent rarity and novelty of anomalies. Alternatively, anomaly detection with reference employs the scheme of change detection to identify anomalies by comparing semantic changes between a reference image and a query one. However, there are very few ADr works due to the scarcity of public datasets in this domain. In this paper, we aim to address this gap by introducing the UMAD Benchmark Dataset. To our best knowledge, this is the first benchmark dataset designed specifically for anomaly detection with reference in robotic patrolling scenarios, e.g., where an autonomous robot is employed to detect anomalous objects by comparing a reference and a query video sequences. The reference sequences can be taken by the robot along a specified route when there are no anomalous objects in the scene. The query sequences are captured online by the robot when it is patrolling in the same scene following the same route. Our benchmark dataset is elaborated such that each query image can find a corresponding reference based on accurate robot localization along the same route in the prebuilt 3D map, with which the reference and query images can be geometrically aligned using adaptive warping. Besides the proposed benchmark dataset, we evaluate the baseline models of ADr on this dataset."
MIXED-SENSE: A Mixed Reality Sensor Emulation Framework for Test and Evaluation of UAVs Against False Data Injection Attacks,"Pant, Kartik Anand; Lin, Li-Yu; Kim, Jaehyeok; Sribunma, Worawis; Goppert, James; Hwang, Inseok",https://arxiv.org/abs/2407.09342,"We present a high-fidelity Mixed Reality sensor emulation framework for testing and evaluating the resilience of Unmanned Aerial Vehicles (UAVs) against false data injection (FDI) attacks. The proposed approach can be utilized to assess the impact of FDI attacks, benchmark attack detector performance, and validate the effectiveness of mitigation/reconfiguration strategies in single-UAV and UAV swarm operations. Our Mixed Reality framework leverages high-fidelity simulations of Gazebo and a Motion Capture system to emulate proprioceptive (e.g., GNSS) and exteroceptive (e.g., camera) sensor measurements in real-time. We propose an empirical approach to faithfully recreate signal characteristics such as latency and noise in these measurements. Finally, we illustrate the efficacy of our proposed framework through a Mixed Reality experiment consisting of an emulated GNSS attack on an actual UAV, which (i) demonstrates the impact of false data injection attacks on GNSS measurements and (ii) validates a mitigation strategy utilizing a distributed camera network developed in our previous work. Our open-source implementation is available at \href{https://github.com/CogniPilot/mixed\_sense}{\texttt{https://github.com/CogniPilot/mixed\_sense}}"
Dynamic Manipulation of Deformable Objects using Imitation Learning with Adaptation to Hardware Constraints,"Hannus, Eric Mikael Leonard; Nguyen Le, Tran; Blanco-Mulero, David; Kyrki, Ville",https://arxiv.org/abs/2403.12685,"Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate simulations of such objects. However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot's physical limits. These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical. To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot's constraints using a constrained Dynamic Movement Primitive. Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics. This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements. We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening. Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag. See supplementary material at https://sites.google.com/view/bilbo-bag."
Multimodal Evolutionary Encoder for Continuous Vision-Language Navigation,"He, Zongtao; Wang, Liuyi; Chen, Lu; Li, Shu; Yan, Qingqing; Liu, Chengju; Chen, Qijun",,
Interactive Reward Tuning: A Visual Analytics Approach for Preference Elicitation with User Feedback,"Shi, Danqing; Zhu, Shibei; Weinkauf, Tino; Oulasvirta, Antti",,
Attainable Force Approximation and Full-Pose Tracking Control of an Over-Actuated Thrust-Vectoring Modular Team UAV,"Chu, Yen-Cheng; Lian, Feng-Li",,
PICaSo: A Collaborative Robotics System for Inpainting on Physical Canvas using Marker and Eraser,"Nasrat, Shady; Yi, Jae-Bong; Jo, Minseong; Yi, Seung-Joon",,
IMU-based Monitoring of Buoy-Ballast System through Cable Dynamics Simulation,"PERAUD, Charly; Filliung, Martin; Anthierens, Cedric; Dune, Claire; boizot, nicolas; HUGEL, Vincent",,
Model Predictive Path Integral Control for Agile Unmanned Aerial Vehicles,"Mina&#345;ík, Michal; Penicka, Robert; Vonasek, Vojtech; Saska, Martin",https://arxiv.org/abs/2407.09812,"This paper introduces a control architecture for real-time and onboard control of Unmanned Aerial Vehicles (UAVs) in environments with obstacles using the Model Predictive Path Integral (MPPI) methodology. MPPI allows the use of the full nonlinear model of UAV dynamics and a more general cost function at the cost of a high computational demand. To run the controller in real-time, the sampling-based optimization is performed in parallel on a graphics processing unit onboard the UAV. We propose an approach to the simulation of the nonlinear system which respects low-level constraints, while also able to dynamically handle obstacle avoidance, and prove that our methods are able to run in real-time without the need for external computers. The MPPI controller is compared to MPC and SE(3) controllers on the reference tracking task, showing a comparable performance. We demonstrate the viability of the proposed method in multiple simulation and real-world experiments, tracking a reference at up to 44 km/h and acceleration close to 20 m/s^2, while still being able to avoid obstacles. To the best of our knowledge, this is the first method to demonstrate an MPPI-based approach in real flight."
Dual-Branch Graph Transformer Network for 3D Human Mesh Reconstruction from Video,"Tang, Tao; Liu, Hong; You, Yingxuan; Wang, Ti; Li, Wenhao",,
Decentralized Collaborative Localization and Map Update with Buildings,"Escourrou, Maxime; Al Hage, Joelle; Bonnifait, Philippe",,
Raising Body Ownership in End-to-End Visuomotor Policy Learning via Robot-Centric Pooling,"Zhuang, Zheyu; Kyrki, Ville; Kragic, Danica",,
6-DoF Grasp Detection in Clutter with Enhanced Receptive Field and Graspable Balance Sampling,"Wang, Hanwen; Ying, Zhang; Wang, Yunlong; LI, Jian",https://arxiv.org/abs/2407.01209,"6-DoF grasp detection of small-scale grasps is crucial for robots to perform specific tasks. This paper focuses on enhancing the recognition capability of small-scale grasping, aiming to improve the overall accuracy of grasping prediction results and the generalization ability of the network. We propose an enhanced receptive field method that includes a multi-radii cylinder grouping module and a passive attention module. This method enhances the receptive field area within the graspable space and strengthens the learning of graspable features. Additionally, we design a graspable balance sampling module based on a segmentation network, which enables the network to focus on features of small objects, thereby improving the recognition capability of small-scale grasping. Our network achieves state-of-the-art performance on the GraspNet-1Billion dataset, with an overall improvement of approximately 10% in average precision@k (AP). Furthermore, we deployed our grasp detection model in pybullet grasping platform, which validates the effectiveness of our method."
Jointly Learning Cost and Constraints from Demonstrations for Safe Trajectory Generation,"Chaubey, Shivam; Verdoja, Francesco; Kyrki, Ville",https://arxiv.org/abs/2405.03491,"Learning from Demonstration allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost."
Solving Dynamic Cosserat Rods with Frictional Contact Using the Shooting Method and Implicit Surfaces,"Jilani, Radhouane; Villard, Pierre-Frederic; Kerrien, Erwan",,
A Point-Based Approach to Efficient LiDAR Multi-Task Perception,"Lang, Christopher; Braun, Alexander; Schillingmann, Lars; Valada, Abhinav",https://arxiv.org/abs/2404.12798,"Multi-task networks can potentially improve performance and computational efficiency compared to single-task networks, facilitating online deployment. However, current multi-task architectures in point cloud perception combine multiple task-specific point cloud representations, each requiring a separate feature encoder and making the network structures bulky and slow. We propose PAttFormer, an efficient multi-task architecture for joint semantic segmentation and object detection in point clouds that only relies on a point-based representation. The network builds on transformer-based feature encoders using neighborhood attention and grid-pooling and a query-based detection decoder using a novel 3D deformable-attention detection head design. Unlike other LiDAR-based multi-task architectures, our proposed PAttFormer does not require separate feature encoders for multiple task-specific point cloud representations, resulting in a network that is 3x smaller and 1.4x faster while achieving competitive performance on the nuScenes and KITTI benchmarks for autonomous driving perception. Our extensive evaluations show substantial gains from multi-task learning, improving LiDAR semantic segmentation by +1.7% in mIou and 3D object detection by +1.7% in mAP on the nuScenes benchmark compared to the single-task models."
BuzzRacer: A Palm-sized Autonomous Vehicle Platform for Testing Multi-Agent Adversarial Decision-Making,"Zhang, Zhiyuan; Tsiotras, Panagiotis",,
Boosting 3D Visual Grounding by Object-Centric Referring Network,"Ren, Ruilong; Cao, Jian; Xu, Weichen; Fu, Tianhao; Dong, Yilei; Xu, Xinxin; Hu, Zicong; Zhang, Xing",,
The Power of Input: Benchmarking Zero-Shot Sim-to-Real Transfer of Reinforcement Learning Control Policies for Quadrotor Control,"Dionigi, Alberto; Costante, Gabriele; Loianno, Giuseppe",,
Active Loop Closure for OSM-guided Robotic Mapping in Large-Scale Urban Environments,"Gao, Wei; Sun, Zezhou; Zhao, Mingle; Xu, Chengzhong; Kong, Hui",https://arxiv.org/abs/2407.17078,"The autonomous mapping of large-scale urban scenes presents significant challenges for autonomous robots. To mitigate the challenges, global planning, such as utilizing prior GPS trajectories from OpenStreetMap (OSM), is often used to guide the autonomous navigation of robots for mapping. However, due to factors like complex terrain, unexpected body movement, and sensor noise, the uncertainty of the robot's pose estimates inevitably increases over time, ultimately leading to the failure of robotic mapping. To address this issue, we propose a novel active loop closure procedure, enabling the robot to actively re-plan the previously planned GPS trajectory. The method can guide the robot to re-visit the previous places where the loop-closure detection can be performed to trigger the back-end optimization, effectively reducing errors and uncertainties in pose estimation. The proposed active loop closure mechanism is implemented and embedded into a real-time OSM-guided robot mapping framework. Empirical results on several large-scale outdoor scenarios demonstrate its effectiveness and promising performance."
A Novel Geometrical Structure Robot Hand for Linear-parallel Pinching and Coupled Self-adaptive Hybrid Grasping,"chen, shi; Zhang, Bihao; Feng, Kehan; Wang, Yizhou; Zhang, Wenzeng",,
Object Augmentation Algorithm: Computing virtual object motion and object induced interaction wrench from optical markers,"Herneth, Christopher; Li, Junnan; Fatoni, Muhammad Hilman; Ganguly, Amartya; Haddadin, Sami",https://arxiv.org/abs/2408.07434,"This study addresses the critical need for diverse and comprehensive data focused on human arm joint torques while performing activities of daily living (ADL). Previous studies have often overlooked the influence of objects on joint torques during ADL, resulting in limited datasets for analysis. To address this gap, we propose an Object Augmentation Algorithm (OAA) capable of augmenting existing marker-based databases with virtual object motions and object-induced joint torque estimations. The OAA consists of five phases: (1) computing hand coordinate systems from optical markers, (2) characterising object movements with virtual markers, (3) calculating object motions through inverse kinematics (IK), (4) determining the wrench necessary for prescribed object motion using inverse dynamics (ID), and (5) computing joint torques resulting from object manipulation. The algorithm's accuracy is validated through trajectory tracking and torque analysis on a 7+4 degree of freedom (DoF) robotic hand-arm system, manipulating three unique objects. The results show that the OAA can accurately and precisely estimate 6 DoF object motion and object-induced joint torques. Correlations between computed and measured quantities were > 0.99 for object trajectories and > 0.93 for joint torques. The OAA was further shown to be robust to variations in the number and placement of input markers, which are expected between databases. Differences between repeated experiments were minor but significant (p < 0.05). The algorithm expands the scope of available data and facilitates more comprehensive analyses of human-object interaction dynamics."
Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors,"Tsagkas, Nikolaos; Rome, Jack A; Ramamoorthy, Subramanian; Mac Aodha, Oisin; Lu, Chris Xiaoxuan",https://arxiv.org/abs/2403.14526,"Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the grounding of fine-grained part descriptors for precise manipulation in a zero-shot setting by utilizing web-trained text-to-image diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object geometry and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrating its potential for advancing semantic-aware robotics manipulation. Web page: https://tsagkas.github.io/click2grasp"
Fly by Book: How to Train a Humanoid Robot to Fly an Airplane using Large Language Models,"Kim, Hyungjoo; Min, Sungjae; Kang, Gyuree; Kim, Jihyeok; Shim, David Hyunchul",,
Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning,"Dong, Shaohua; Feng, Yunhe; Yang, Qing; Huang, Yan; Liu, Dongfang; Fan, Heng",https://arxiv.org/abs/2312.00360,"Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for improving semantic segmentation in complex scenes (e.g., indoor/low-light conditions). Existing approaches often fully fine-tune a dual-branch encoder-decoder framework with a complicated feature fusion strategy for achieving multimodal semantic segmentation, which is training-costly due to the massive parameter updates in feature extraction and fusion. To address this issue, we propose a surprisingly simple yet effective dual-prompt learning network (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D/T) semantic segmentation. The core of DPLNet is to directly adapt a frozen pre-trained RGB model to multimodal semantic segmentation, reducing parameter updates. For this purpose, we present two prompt learning modules, comprising multimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG works to fuse the features from different modalities in a compact manner and is inserted from shadow to deep stages to generate the multi-level multimodal prompts that are injected into the frozen backbone, while MPG adapts prompted multimodal features in the frozen backbone for better multimodal semantic segmentation. Since both the MPG and MFA are lightweight, only a few trainable parameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced for multimodal feature fusion and learning. Using a simple decoder (3.27M parameters), DPLNet achieves new state-of-the-art performance or is on a par with other complex approaches on four RGB-D/T semantic segmentation datasets while satisfying parameter efficiency. Moreover, we show that DPLNet is general and applicable to other multimodal tasks such as salient object detection and video semantic segmentation. Without special design, DPLNet outperforms many complicated models. Our code will be available at github.com/ShaohuaDong2021/DPLNet."
Real-time Localization and Dense Mapping of Low-texture Underwater Environments with a Low-cost Unmanned Underwater Vehicle,"Song, Jingyu; Bagoren, Onur; Venkatramanan Sethuraman, Advaith; Andigani, Razan; Skinner, Katherine",https://arxiv.org/abs/2408.01569,"Significant work has been done on advancing localization and mapping in underwater environments. Still, state-of-the-art methods are challenged by low-texture environments, which is common for underwater settings. This makes it difficult to use existing methods in diverse, real-world scenes. In this paper, we present TURTLMap, a novel solution that focuses on textureless underwater environments through a real-time localization and mapping method. We show that this method is low-cost, and capable of tracking the robot accurately, while constructing a dense map of a low-textured environment in real-time. We evaluate the proposed method using real-world data collected in an indoor water tank with a motion capture system and ground truth reference map. Qualitative and quantitative results validate the proposed system achieves accurate and robust localization and precise dense mapping, even when subject to wave conditions. The project page for TURTLMap is https://umfieldrobotics.github.io/TURTLMap."
Pedicle Drilling Planning Transfer for Spine Surgery Using Functional Map Correspondences,"Leblanc, Lilyan; Vialle, Raphael; de Farias, Cristiana; SAGHBINY, Elie; Marturi, Naresh; TAMADAZTE, Brahim",,
D3G: Learning Multi-robot Coordination from Demonstrations,"Zhou, Yizhi; JIN, WANXIN; Wang, Xuan",,
Exploration of Efficacy of Movable Palm in Caging Inspired Grasping using a Reinforcement Learning-based Approach,"Beddow, Luke Jonathan; Wurdemann, Helge Arne; Kanoulas, Dimitrios",,
Learning to Estimate the Pose of a Peer Robot in a Camera Image by Predicting the States of its LEDs,"Carlotti, Nicholas; Nava, Mirko; Giusti, Alessandro",https://arxiv.org/abs/2407.10661,"We consider the problem of training a fully convolutional network to estimate the relative 6D pose of a robot given a camera image, when the robot is equipped with independent controllable LEDs placed in different parts of its body. The training data is composed by few (or zero) images labeled with a ground truth relative pose and many images labeled only with the true state (\textsc{on} or \textsc{off}) of each of the peer LEDs. The former data is expensive to acquire, requiring external infrastructure for tracking the two robots; the latter is cheap as it can be acquired by two unsupervised robots moving randomly and toggling their LEDs while sharing the true LED states via radio. Training with the latter dataset on estimating the LEDs' state of the peer robot (\emph{pretext task}) promotes learning the relative localization task (\emph{end task}). Experiments on real-world data acquired by two autonomous wheeled robots show that a model trained only on the pretext task successfully learns to localize a peer robot on the image plane; fine-tuning such model on the end task with few labeled images yields statistically significant improvements in 6D relative pose estimation with respect to baselines that do not use pretext-task pre-training, and alternative approaches. Estimating the state of multiple independent LEDs promotes learning to estimate relative heading. The approach works even when a large fraction of training images do not include the peer robot and generalizes well to unseen environments."
Automatic 3D Road Surface Reconstruction via Cross-Section Modeling and Interpolation,"Bellusci, Matteo; Matteucci, Matteo",,
On Learning Scene-aware Generative State Abstractions for Task-level Mobile Manipulation Planning,"Förster, Julian; Chung, Jen Jen; Ott, Lionel; Siegwart, Roland",,
Learning incipient slip with GelSight sensors: Attention Classification with Video Vision Transformers,"Parag, Amit; Misimi, Ekrem; Adelson, Edward",,
Development of a Low Pressure Pouch Sensor for Force Measurement in Colonoscopy Procedures,"Borvorntanajanya, Korn; Ahmed, Jabed F; Runciman, Mark; Franco, Enrico; Patel, Nisha; Rodriguez y Baena, Ferdinando",,
Road Boundary Estimation Using Sparse Automotive Radar Inputs,"Kingery, Aaron; Song, Dezhen",https://arxiv.org/abs/2309.08341,"This paper presents a new approach to detecting road boundaries based on sparse radar signals. We model the roadway using a homogeneous model and derive its conditional predictive model under known radar motion. Using the conditional predictive model and model radar points using a Dirichlet Process Mixture Model (DPMM), we employ Mean Field Variational Inference (MFVI) to derive an unconditional road boundary model distribution. In order to generate initial candidate solutions for the MFVI, we develop a custom Random Sample and Consensus (RANSAC) variant to propose unseen model instances as candidate road boundaries. For each radar point cloud we alternate the MFVI and RANSAC proposal steps until convergence to generate the best estimate of all candidate models. We select the candidate model with the minimum lateral distance to the radar on each side as the estimates of the left and right boundaries. We have implemented the proposed algorithm in C++. We have tested the algorithm and it has shown satisfactory results. More specifically, the mean lane boundary estimation error is not more than 11.0 cm."
Evaluating the Impact of a Semi-Autonomous Interface on Configuration Space Accessibility for Multi-DOF Upper Limb Prostheses,"Greene, Rebecca J.; Hunt, Christopher; Acosta, Brooklyn Paige; Huang, Zihan; Kaliki, Rahul; Thakor, Nitish V.",,
Exploring Few-Beam LiDAR Assistance in Self-Supervised Multi-Frame Depth Estimation,"Fan, Rizhao; Poggi, Matteo; Mattoccia, Stefano",,
AO-Grasp: Articulated Object Grasp Generation,"Pares-Morlans, Carlota; Chen, Claire; Weng, Yijia; Yi, Michelle; Huang, Yuying; Heppert, Nick; Zhou, Linqi; Guibas, Leonidas; Bohg, Jeannette",https://arxiv.org/abs/2310.15928,"We introduce AO-Grasp, a grasp proposal method that generates 6 DoF grasps that enable robots to interact with articulated objects, such as opening and closing cabinets and appliances. AO-Grasp consists of two main contributions: the AO-Grasp Model and the AO-Grasp Dataset. Given a segmented partial point cloud of a single articulated object, the AO-Grasp Model predicts the best grasp points on the object with an Actionable Grasp Point Predictor. Then, it finds corresponding grasp orientations for each of these points, resulting in stable and actionable grasp proposals. We train the AO-Grasp Model on our new AO-Grasp Dataset, which contains 78K actionable parallel-jaw grasps on synthetic articulated objects. In simulation, AO-Grasp achieves a 45.0 % grasp success rate, whereas the highest performing baseline achieves a 35.0% success rate. Additionally, we evaluate AO-Grasp on 120 real-world scenes of objects with varied geometries, articulation axes, and joint states, where AO-Grasp produces successful grasps on 67.5% of scenes, while the baseline only produces successful grasps on 33.3% of scenes. To the best of our knowledge, AO-Grasp is the first method for generating 6 DoF grasps on articulated objects directly from partial point clouds without requiring part detection or hand-designed grasp heuristics. Project website: https://stanford-iprl-lab.github.io/ao-grasp"
Translating Agent-Environment Interactions from Humans to Robots,"Shankar, Tanmay; Chawla, Chaitanya; Hassan, Almutwakel Khalid; Oh, Jean",https://arxiv.org/abs/2301.13343,"We investigate policy transfer using image-to-semantics translation to mitigate learning difficulties in vision-based robotics control agents. This problem assumes two environments: a simulator environment with semantics, that is, low-dimensional and essential information, as the state space, and a real-world environment with images as the state space. By learning mapping from images to semantics, we can transfer a policy, pre-trained in the simulator, to the real world, thereby eliminating real-world on-policy agent interactions to learn, which are costly and risky. In addition, using image-to-semantics mapping is advantageous in terms of the computational efficiency to train the policy and the interpretability of the obtained policy over other types of sim-to-real transfer strategies. To tackle the main difficulty in learning image-to-semantics mapping, namely the human annotation cost for producing a training dataset, we propose two techniques: pair augmentation with the transition function in the simulator environment and active learning. We observed a reduction in the annotation cost without a decline in the performance of the transfer, and the proposed approach outperformed the existing approach without annotation."
Design Improvements to the Float Upper-Limb Exoskeleton Better Mimics the Glenohumeral Complex Kinematics,"Bodo, Giulia; Tessari, Federico; Capitta, Gianluca; De Guglielmo, Luca; Buccelli, Stefano; Laffranchi, Matteo",,
Robust Precision Landing of a Quadrotor with Online Temporal Scaling Adaptation of Dynamic Movement Primitives,"Rothomphiwat, Kongkiat; Manoonpong, Poramate",,
The Effectiveness of State Representation Model in Multi-Agent Proximal Policy Optimization for Multi-Agent Path Finding,"Chung, Jaehoon; Fayyad, Jamil; Najjaran, Homayoun",,
Sampling-based Motion Planning for Optimal Probability of Collision under Environment Uncertainty,"Lu, Hao; Kurniawati, Hanna; Shome, Rahul",https://arxiv.org/abs/2004.12317,"Safe autonomous navigation is an essential and challenging problem for robots operating in highly unstructured or completely unknown environments. Under these conditions, not only robotic systems must deal with limited localisation information, but also their manoeuvrability is constrained by their dynamics and often suffer from uncertainty. In order to cope with these constraints, this manuscript proposes an uncertainty-based framework for mapping and planning feasible motions online with probabilistic safety-guarantees. The proposed approach deals with the motion, probabilistic safety, and online computation constraints by: (i) incrementally mapping the surroundings to build an uncertainty-aware representation of the environment, and (ii) iteratively (re)planning trajectories to goal that are kinodynamically feasible and probabilistically safe through a multi-layered sampling-based planner in the belief space. In-depth empirical analyses illustrate some important properties of this approach, namely, (a) the multi-layered planning strategy enables rapid exploration of the high-dimensional belief space while preserving asymptotic optimality and completeness guarantees, and (b) the proposed routine for probabilistic collision checking results in tighter probability bounds in comparison to other uncertainty-aware planners in the literature. Furthermore, real-world in-water experimental evaluation on a non-holonomic torpedo-shaped autonomous underwater vehicle and simulated trials in the Stairwell scenario of the DARPA Subterranean Challenge 2019 on a quadrotor unmanned aerial vehicle demonstrate the efficacy of the method as well as its suitability for systems with limited on-board computational power."
Control of Unknown Quadrotors from a Single Throw,"Blaha, Till Martin; Smeur, Ewoud; Remes, Bart",https://arxiv.org/abs/2406.11723,"This paper presents a method to recover quadrotor UAV from a throw, when no control parameters are known before the throw. We leverage the availability of high-frequency rotor speed feedback available in racing drone hardware and software to find control effectiveness values and fit a motor model using recursive least squares (RLS) estimation. Furthermore, we propose an excitation sequence that provides large actuation commands while guaranteeing to stay within gyroscope sensing limits. After 450ms of excitation, an INDI attitude controller uses the 52 fitted parameters to arrest rotational motion and recover an upright attitude. Finally, a NDI position controller drives the craft to a position setpoint. The proposed algorithm runs efficiently on microcontrollers found in common UAV flight controllers, and was shown to recover an agile quadrotor every time in 57 live experiments with as low as 3.5m throw height, demonstrating robustness against initial rotations and noise. We also demonstrate control of randomized quadrotors in simulated throws, where the parameter fitting RMS error is typically within 10% of the true value.   This work has been submitted to IROS 2024 for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible."
Tension Feedback Control for Musculoskeletal Quadrupedal Locomotion over Uneven Terrain.,"Tanaka, Hiroaki; Matsumoto, Ojiro; Kawasetsu, Takumi; Hosoda, Koh",,
Social Navigation in Crowded Environments with Model Predictive Control and Deep Learning-Based Human Trajectory Prediction,"Le, Viet-Anh; Chalaki, Behdad; Tadiparthi, Vaishnav; Nourkhiz Mahjoub, Hossein; D'sa, Jovin; Moradi-Pari, Ehsan",https://arxiv.org/abs/2309.16838,"Crowd navigation has received increasing attention from researchers over the last few decades, resulting in the emergence of numerous approaches aimed at addressing this problem to date. Our proposed approach couples agent motion prediction and planning to avoid the freezing robot problem while simultaneously capturing multi-agent social interactions by utilizing a state-of-the-art trajectory prediction model i.e., social long short-term memory model (Social-LSTM). Leveraging the output of Social-LSTM for the prediction of future trajectories of pedestrians at each time-step given the robot's possible actions, our framework computes the optimal control action using Model Predictive Control (MPC) for the robot to navigate among pedestrians. We demonstrate the effectiveness of our proposed approach in multiple scenarios of simulated crowd navigation and compare it against several state-of-the-art reinforcement learning-based methods."
Unified Control Framework for Real-Time Interception and Obstacle Avoidance of Fast-Moving Objects with Diffusion Variational Autoencoder,"Dastider, Apan; Fang, Hao; Mingjie, Lin",https://arxiv.org/abs/2209.13628,"Real-time interception of fast-moving objects by robotic arms in dynamic environments poses a formidable challenge due to the need for rapid reaction times, often within milliseconds, amidst dynamic obstacles. This paper introduces a unified control framework to address the above challenge by simultaneously intercepting dynamic objects and avoiding moving obstacles. Central to our approach is using diffusion-based variational autoencoder for motion planning to perform both object interception and obstacle avoidance. We begin by encoding the high-dimensional temporal information from streaming events into a two-dimensional latent manifold, enabling the discrimination between safe and colliding trajectories, culminating in the construction of an offline densely connected trajectory graph. Subsequently, we employ an extended Kalman filter to achieve precise real-time tracking of the moving object. Leveraging a graph-traversing strategy on the established offline dense graph, we generate encoded robotic motor control commands. Finally, we decode these commands to enable real-time motion of robotic motors, ensuring effective obstacle avoidance and high interception accuracy of fast-moving objects. Experimental validation on both computer simulations and autonomous 7-DoF robotic arms demonstrates the efficacy of our proposed framework. Results indicate the capability of the robotic manipulator to navigate around multiple obstacles of varying sizes and shapes while successfully intercepting fast-moving objects thrown from different angles by hand. Complete video demonstrations of our experiments can be found in https://sites.google.com/view/multirobotskill/home."
Autonomous Power Line Tracking with mmWave Radar,"Malle, Nicolaj; Ebeid, Emad",,
Alternative Connection Radius for Asymptotic Optimality in RRT Star,"Shome, Rahul",,
Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks,"Mehta, Shaunak; Habibian, Soheil; Losey, Dylan",https://arxiv.org/abs/2403.13281,"Robot arms should be able to learn new tasks. One framework here is reinforcement learning, where the robot is given a reward function that encodes the task, and the robot autonomously learns actions to maximize its reward. Existing approaches to reinforcement learning often frame this problem as a Markov decision process, and learn a policy (or a hierarchy of policies) to complete the task. These policies reason over hundreds of fine-grained actions that the robot arm needs to take: e.g., moving slightly to the right or rotating the end-effector a few degrees. But the manipulation tasks that we want robots to perform can often be broken down into a small number of high-level motions: e.g., reaching an object or turning a handle. In this paper we therefore propose a waypoint-based approach for model-free reinforcement learning. Instead of learning a low-level policy, the robot now learns a trajectory of waypoints, and then interpolates between those waypoints using existing controllers. Our key novelty is framing this waypoint-based setting as a sequence of multi-armed bandits: each bandit problem corresponds to one waypoint along the robot's motion. We theoretically show that an ideal solution to this reformulation has lower regret bounds than standard frameworks. We also introduce an approximate posterior sampling solution that builds the robot's motion one waypoint at a time. Results across benchmark simulations and two real-world experiments suggest that this proposed approach learns new tasks more quickly than state-of-the-art baselines. See videos here: https://youtu.be/MMEd-lYfq4Y"
GenChIP: Generating Robot PolicyCode forHigh-Precision and Contact-Rich Manipulation Tasks,"Burns, Kaylee; Jain, Ajinkya; Go, Keegan; Xia, Fei; Stark, Michael; Schaal, Stefan; Hausman, Karol",,
DiPPeST: Diffusion-based Path Planner for Synthesizing Trajectories applied on Quadruped Robots,"Stamatopoulou, Maria; Liu, Jianwei; Kanoulas, Dimitrios",https://arxiv.org/abs/2405.19232,"We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning. DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR). The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques. DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR. A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments."
A parallel-actuated robot with two end-effector degrees-of-freedom: application as a novel wearable head-neck traction brace,"Zhou, Jingzong; Kulkarni, Priya; Agrawal, Sunil",,
Barely-Visible Surface Crack Detection for Wind Turbine Sustainability,"Agrawal, Sourav; Corley, Isaac; Wallace, Conor; Vaughn, Clovis; Lwowski, Jonathan",https://arxiv.org/abs/2407.07186,"The production of wind energy is a crucial part of sustainable development and reducing the reliance on fossil fuels. Maintaining the integrity of wind turbines to produce this energy is a costly and time-consuming task requiring repeated inspection and maintenance. While autonomous drones have proven to make this process more efficient, the algorithms for detecting anomalies to prevent catastrophic damage to turbine blades have fallen behind due to some dangerous defects, such as hairline cracks, being barely-visible. Existing datasets and literature are lacking and tend towards detecting obvious and visible defects in addition to not being geographically diverse. In this paper we introduce a novel and diverse dataset of barely-visible hairline cracks collected from numerous wind turbine inspections. To prove the efficacy of our dataset, we detail our end-to-end deployed turbine crack detection pipeline from the image acquisition stage to the use of predictions in providing automated maintenance recommendations to extend the life and efficiency of wind turbines."
Evaluation of the Design of a Tool for the Automated Assembly of Preconfigured Wires,"Bartelt, Stefanie; Kuhlenkötter, Bernd",,
Optimal Robotic Assembly Sequence Planning (ORASP): A Sequential Decision-Making Approach,"Nagpal, Kartik; Mehr, Negar",https://arxiv.org/abs/2310.17115,"The optimal robot assembly planning problem is challenging due to the necessity of finding the optimal solution amongst an exponentially vast number of possible plans, all while satisfying a selection of constraints. Traditionally, robotic assembly planning problems have been solved using heuristics, but these methods are specific to a given objective structure or set of problem parameters. In this paper, we propose a novel approach to robotic assembly planning that poses assembly sequencing as a sequential decision making problem, enabling us to harness methods that far outperform the state-of-the-art. We formulate the problem as a Markov Decision Process (MDP) and utilize Dynamic Programming (DP) to find optimal assembly policies for moderately sized strictures. We further expand our framework to exploit the deterministic nature of assembly planning and introduce a class of optimal Graph Exploration Assembly Planners (GEAPs). For larger structures, we show how Reinforcement Learning (RL) enables us to learn policies that generate high reward assembly sequences. We evaluate our approach on a variety of robotic assembly problems, such as the assembly of the Hubble Space Telescope, the International Space Station, and the James Webb Space Telescope. We further showcase how our DP, GEAP, and RL implementations are capable of finding optimal solutions under a variety of different objective functions and how our formulation allows us to translate precedence constraints to branch pruning and thus further improve performance. We have published our code at https://github.com/labicon/ORASP-Code."
A Comprehensive Modeling and Scheduling Approach for Allocating Distributed Multi-Robot Software to the Edge/Cloud,"Zhang, Yongzhou; Mirus, Florian; Pasch, Frederik; Scholl, Kay-Ulrich; Wurll, Christian; Hein, Björn",,
Active Learning-augmented Intent-aware Obstacle Avoidance of Autonomous Surface Vehicles in High-traffic Waters,"Jeong, Mingi; Chadda, Arihant; Quattrini Li, Alberto",,
IN-Sight: Interactive Navigation through Sight,"Schoch, Philipp; Yang, Fan; Ma, Yuntao; Leutenegger, Stefan; Hutter, Marco; Leboutet, Quentin",https://arxiv.org/abs/2408.00343,"Current visual navigation systems often treat the environment as static, lacking the ability to adaptively interact with obstacles. This limitation leads to navigation failure when encountering unavoidable obstructions. In response, we introduce IN-Sight, a novel approach to self-supervised path planning, enabling more effective navigation strategies through interaction with obstacles. Utilizing RGB-D observations, IN-Sight calculates traversability scores and incorporates them into a semantic map, facilitating long-range path planning in complex, maze-like environments. To precisely navigate around obstacles, IN-Sight employs a local planner, trained imperatively on a differentiable costmap using representation learning techniques. The entire framework undergoes end-to-end training within the state-of-the-art photorealistic Intel SPEAR Simulator. We validate the effectiveness of IN-Sight through extensive benchmarking in a variety of simulated scenarios and ablation studies. Moreover, we demonstrate the system's real-world applicability with zero-shot sim-to-real transfer, deploying our planner on the legged robot platform ANYmal, showcasing its practical potential for interactive navigation in real environments."
Interruptive Language Control of Bipedal Locomotion,"Malik, Ashish; Lee, Stefan; Fern, Alan",,
Recurrent Non-Rigid Point Cloud Registration,"Cao, Yue; Cheng, Ziang; Li, Hongdong",,
MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation,"Wu, Jiayi; Lin, Xiaomin; Negahdaripour, Shahriar; Fermuller, Cornelia; Aloimonos, Yiannis",https://arxiv.org/abs/2403.09850,"Tasks such as autonomous navigation, 3D reconstruction, and object recognition near the water surfaces are crucial in marine robotics applications. However, challenges arise due to dynamic disturbances, e.g., light reflections and refraction from the random air-water interface, irregular liquid flow, and similar factors, which can lead to potential failures in perception and navigation systems. Traditional computer vision algorithms struggle to differentiate between real and virtual image regions, significantly complicating tasks. A virtual image region is an apparent representation formed by the redirection of light rays, typically through reflection or refraction, creating the illusion of an object's presence without its actual physical location. This work proposes a novel approach for segmentation on real and virtual image regions, exploiting synthetic images combined with domain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric Consistency. Our segmentation network does not need to be re-trained if the domain changes. We show this by deploying the same segmentation network in two different domains: simulation and the real world. By creating realistic synthetic images that mimic the complexities of the water surface, we provide fine-grained training data for our network (MARVIS) to discern between real and virtual images effectively. By motion & geometry-aware design choices and through comprehensive experimental analysis, we achieve state-of-the-art real-virtual image segmentation performance in unseen real world domain, achieving an IoU over 78% and a F1-Score over 86% while ensuring a small computational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a single GPU (CPU core). Our code and dataset are available here https://github.com/jiayi-wu-umd/MARVIS."
An Actor-Critic Reinforcement Learning Scheme for Reactive 3D Optimal Motion Planning Based on Fluid Dynamics,"Malliaropoulos, Marios; Rousseas, Panagiotis; Bechlioulis, Charalampos; Kyriakopoulos, Kostas",,
NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields,"Erich, Floris Marc Arden; Chiba, Naoya; Mustafa, Abdullah; Yoshiyasu, Yusuke; Ando, Noriaki; Hanai, Ryo; Domae, Yukiyasu",https://arxiv.org/abs/2309.11966,"We present NeuralLabeling, a labeling approach and toolset for annotating 3D scenes using either bounding boxes or meshes and generating segmentation masks, affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth maps, and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as a renderer, allowing labeling to be performed using 3D spatial tools while incorporating geometric clues such as occlusions, relying only on images captured from multiple viewpoints as input. To demonstrate the applicability of NeuralLabeling to a practical problem in robotics, we added ground truth depth maps to 30000 frames of transparent object RGB and noisy depth maps of glasses placed in a dishwasher captured using an RGBD sensor, yielding the Dishwasher30k dataset. We show that training a simple deep neural network with supervision using the annotated depth maps yields a higher reconstruction performance than training with the previously applied weakly supervised approach. We also show how instance segmentation and depth completion datasets generated using NeuralLabeling can be incorporated into a robot application for grasping transparent objects placed in a dishwasher with an accuracy of 83.3%, compared to 16.3% without depth completion."
Deep Domain Adaptation Regression for Force Calibration of Optical Tactile Sensors,"Chen, Zhuo; Ou, Ni; Jiang, Jiaqi; LUO, SHAN",https://arxiv.org/abs/2407.14380,"Optical tactile sensors provide robots with rich force information for robot grasping in unstructured environments. The fast and accurate calibration of three-dimensional contact forces holds significance for new sensors and existing tactile sensors which may have incurred damage or aging. However, the conventional neural-network-based force calibration method necessitates a large volume of force-labeled tactile images to minimize force prediction errors, with the need for accurate Force/Torque measurement tools as well as a time-consuming data collection process. To address this challenge, we propose a novel deep domain-adaptation force calibration method, designed to transfer the force prediction ability from a calibrated optical tactile sensor to uncalibrated ones with various combinations of domain gaps, including marker presence, illumination condition, and elastomer modulus. Experimental results show the effectiveness of the proposed unsupervised force calibration method, with lowest force prediction errors of 0.102N (3.4\% in full force range) for normal force, and 0.095N (6.3\%) and 0.062N (4.1\%) for shear forces along the x-axis and y-axis, respectively. This study presents a promising, general force calibration methodology for optical tactile sensors."
Efficient Trajectory Forecasting and Generation with Conditional Flow Matching,"Ye, Sean; Gombolay, Matthew",https://arxiv.org/abs/2403.10809,"Trajectory prediction and generation are vital for autonomous robots navigating dynamic environments. While prior research has typically focused on either prediction or generation, our approach unifies these tasks to provide a versatile framework and achieve state-of-the-art performance. Diffusion models, which are currently state-of-the-art for learned trajectory generation in long-horizon planning and offline reinforcement learning tasks, rely on a computationally intensive iterative sampling process. This slow process impedes the dynamic capabilities of robotic systems. In contrast, we introduce Trajectory Conditional Flow Matching (T-CFM), a novel data-driven approach that utilizes flow matching techniques to learn a solver time-varying vector field for efficient and fast trajectory generation. We demonstrate the effectiveness of T-CFM on three separate tasks: adversarial tracking, real-world aircraft trajectory forecasting, and long-horizon planning. Our model outperforms state-of-the-art baselines with an increase of 35% in predictive accuracy and 142% increase in planning performance. Notably, T-CFM achieves up to 100$\times$ speed-up compared to diffusion-based models without sacrificing accuracy, which is crucial for real-time decision making in robotics."
Refining Airway Segmentation Through Breakage Filling and Leakage Reduction Using Point Clouds,"Hu, Yan; Meijering, Erik; Song, Yang",,
Fixing symbolic plans with reinforcement learning in object-based action spaces,"Thierauf, Christopher; Scheutz, Matthias",,
SDFT: Structural Discrete Fourier Transform for Place Recognition and Traversability Analysis,"Umemura, Ayumi; Sakurada, Ken; Onishi, Masaki; Yoshida, Kazuya",,
DTG : Diffusion-based Trajectory Generation for Mapless Global Navigation,"Liang, Jing; payandeh, amirreza; Song, Daeun; Xiao, Xuesu; Manocha, Dinesh",https://arxiv.org/abs/2403.09900,"We present a novel end-to-end diffusion-based trajectory generation method, DTG, for mapless global navigation in challenging outdoor scenarios with occlusions and unstructured off-road features like grass, buildings, bushes, etc. Given a distant goal, our approach computes a trajectory that satisfies the following goals: (1) minimize the travel distance to the goal; (2) maximize the traversability by choosing paths that do not lie in undesirable areas. Specifically, we present a novel Conditional RNN(CRNN) for diffusion models to efficiently generate trajectories. Furthermore, we propose an adaptive training method that ensures that the diffusion model generates more traversable trajectories. We evaluate our methods in various outdoor scenes and compare the performance with other global navigation algorithms on a Husky robot. In practice, we observe at least a 15% improvement in traveling distance and around a 7% improvement in traversability."
Domain Randomization-free Sim-to-Real : An Attention-Augmented Memory Approach for Robotic Tasks,"Qu, Jia; Otsubo, Shun; Yamanokuchi, Tomoya; Matsubara, Takamitsu; Miwa, Shotaro",,
GripFlexer: Development of hybrid gripper with a novel shape that can perform in narrow spaces,"Kim, Donghyun; Choi, Sunghyun; Song, Bongsub; Song, Jinhyeok; Yoon, Jingon; Yun, Dongwon",,
ESO-SLAM: Tightly-Coupled and Simultaneous Estimation of Self and Multi-Object Pose via Sensor Fusion,"Li, Wu; Zhang, Yunzhou; Lv, Yuezhang; Wang, TingTing; Wang, Sizhan; Wang, Guiyuan",,
ToolEENet: Tool Affordance 6D Pose Estimation,"Wang, Yunlong; Zhang, Lei; Tu, Yuyang; Zhang, Hui; Bai, Kaixin; Chen, Zhaopeng; Zhang, Jianwei",https://arxiv.org/abs/2404.04193,"The exploration of robotic dexterous hands utilizing tools has recently attracted considerable attention. A significant challenge in this field is the precise awareness of a tool's pose when grasped, as occlusion by the hand often degrades the quality of the estimation. Additionally, the tool's overall pose often fails to accurately represent the contact interaction, thereby limiting the effectiveness of vision-guided, contact-dependent activities. To overcome this limitation, we present the innovative TOOLEE dataset, which, to the best of our knowledge, is the first to feature affordance segmentation of a tool's end-effector (EE) along with its defined 6D pose based on its usage. Furthermore, we propose the ToolEENet framework for accurate 6D pose estimation of the tool's EE. This framework begins by segmenting the tool's EE from raw RGBD data, then uses a diffusion model-based pose estimator for 6D pose estimation at a category-specific level. Addressing the issue of symmetry in pose estimation, we introduce a symmetry-aware pose representation that enhances the consistency of pose estimation. Our approach excels in this field, demonstrating high levels of precision and generalization. Furthermore, it shows great promise for application in contact-based manipulation scenarios. All data and codes are available on the project website: https://tooleenet-iros2024.github.io/"
Exploring How Non-Prehensile Manipulation Expands Capability in Robots Experiencing Multi-Joint Failure,"Briscoe-Martinez, Gilberto; Pasricha, Anuj; Abderezaei, Ava; Chaganti, Rama Durga Santosh Kumar; Vajrala, Sarath Chandra; Popuri, Srikanth; Roncone, Alessandro",,
Data-Driven Predictive Control for Robust Exoskeleton Locomotion,"Li, Kejun; Kim, Jeeseop; Xiong, Xiaobin; Akbari Hamed, Kaveh; Yue, Yisong; Ames, Aaron",https://arxiv.org/abs/2403.15658,"Exoskeleton locomotion must be robust while being adaptive to different users with and without payloads. To address these challenges, this work introduces a data-driven predictive control (DDPC) framework to synthesize walking gaits for lower-body exoskeletons, employing Hankel matrices and a state transition matrix for its data-driven model. The proposed approach leverages DDPC through a multi-layer architecture. At the top layer, DDPC serves as a planner employing Hankel matrices and a state transition matrix to generate a data-driven model that can learn and adapt to varying users and payloads. At the lower layer, our method incorporates inverse kinematics and passivity-based control to map the planned trajectory from DDPC into the full-order states of the lower-body exoskeleton. We validate the effectiveness of this approach through numerical simulations and hardware experiments conducted on the Atalante lower-body exoskeleton with different payloads. Moreover, we conducted a comparative analysis against the model predictive control (MPC) framework based on the reduced-order linear inverted pendulum (LIP) model. Through this comparison, the paper demonstrates that DDPC enables robust bipedal walking at various velocities while accounting for model uncertainties and unknown perturbations."
"A Direct Semi-Exhaustive Search Method for Robust, Partial-to-Full Point Cloud Registration","Cheng, Richard; Papazov, Chavdar; Helmick, Daniel; Tjersland, Mark",,
BE-SLAM: BEV-Enhanced Dynamic Semantic SLAM with Static Object Reconstruction,"Luo, Jun; Wang, Gang; Liu, Hongliang; wu, lang; Huang, Tao; Xiao, Dengyu; Pu, Huayan; Luo, Jun",,
A Lightweight De-confounding Transformer for Image Captioning in Wearable Assistive Navigation Device,"Cao, Zhengcai; Xia, Ji; Shi, Yinbin; Zhou, MengChu",,
A Cooperative Recovery Framework for Resilient Multi-Robot Swarm Operations Under Loss of Localization in Unknown Environments,"Bonczek, Paul; Bezzo, Nicola",,
An Active and Dexterous Bionic Torso for A Quadruped Robot,"Li, Ruyue; Zhu, Yaguang; Wang, Yuntong; He, Zhimin; Zhou, Mengnan",,
TacLink-Integrated Robot Arm toward Safe Human-Robot Interaction,"Luu, Quan; Albini, Alessandro; Maiolino, Perla; Ho, Van",,
Safe Offline-to-Online Multi-Agent Decision Transformer: A Safety Conscious Sequence Modeling Approach,"Shah, Aamir Bader; WEN, Yu; Chen, Jiefu; Wu, Xuqing; Fu, Xin",,
Text2Map: From Navigational Instructions to Graph-Based Indoor Map Representations Using LLMs,"Karkour, Ammar; Harras, Khaled; FEO, EDUARDO",,
Expansion-GRR: Efficient Generation of Smooth Global Redundancy Resolution Roadmaps,"Zhong, Zhuoyun; Li, Zhi; Chamzas, Constantinos",https://arxiv.org/abs/2405.13770,"Global redundancy resolution (GRR) roadmaps is a novel concept in robotics that facilitates the mapping from task space paths to configuration space paths in a legible, predictable, and repeatable way. Such roadmaps could find widespread utility in applications such as safe teleoperation, consistent path planning, and motion primitives generation. However, previous methods to compute GRR roadmaps often necessitate a lengthy computation time and produce non-smooth paths, limiting their practical efficacy. To address this challenge, we introduce a novel method Expansion-GRR that leverages efficient configuration space projections and enables rapid generation of smooth roadmaps that satisfy the task constraints. Additionally, we propose a simple multi-seed strategy that further enhances the final quality. We conducted experiments in simulation with a 5-link planar manipulator and a Kinova arm. We were able to generate the Expansion-GRR roadmaps up to 2 orders of magnitude faster while achieving higher smoothness. We also demonstrate the utility of the GRR roadmaps in teleoperation tasks where our method outperformed prior methods and reactive IK solvers in terms of success rate and solution quality."
GestRight: Understanding the Feasibility of Gesture-driven Tele-Operation in Human-Robot Teams,"Rippy, Kevin; Gangopadhyay, Aryya; Jayarajah, Kasthuri",,
Multi-agent Path Finding for Mixed Autonomy Traffic Coordination,"Zheng, Han; Yan, Zhongxia; Wu, Cathy",https://arxiv.org/abs/2409.03881,"In the evolving landscape of urban mobility, the prospective integration of Connected and Automated Vehicles (CAVs) with Human-Driven Vehicles (HDVs) presents a complex array of challenges and opportunities for autonomous driving systems. While recent advancements in robotics have yielded Multi-Agent Path Finding (MAPF) algorithms tailored for agent coordination task characterized by simplified kinematics and complete control over agent behaviors, these solutions are inapplicable in mixed-traffic environments where uncontrollable HDVs must coexist and interact with CAVs. Addressing this gap, we propose the Behavior Prediction Kinematic Priority Based Search (BK-PBS), which leverages an offline-trained conditional prediction model to forecast HDV responses to CAV maneuvers, integrating these insights into a Priority Based Search (PBS) where the A* search proceeds over motion primitives to accommodate kinematic constraints. We compare BK-PBS with CAV planning algorithms derived by rule-based car-following models, and reinforcement learning. Through comprehensive simulation on a highway merging scenario across diverse scenarios of CAV penetration rate and traffic density, BK-PBS outperforms these baselines in reducing collision rates and enhancing system-level travel delay. Our work is directly applicable to many scenarios of multi-human multi-robot coordination."
A Low-Texture Robust Hybrid Feature Based Visual Odometry,"Wang, He; Zhang, Qi; Zheng, Zhiwen; Li, Xiaoli; Li, Ru",,
Calibration-Free Vision-Assisted Container Loading of RTG Cranes,"Yang, Jianbing; Wang, Yuanzhe; Jiang, Hao; Zhao, Bin; Li, Yiming; Wang, Danwei",,
LiDAR-based HD Map Localization using Semantic Generalized ICP with Road Marking Detection,"gong, yansong; zhang, xinglian; FENG, JINGYI; He, Xiao; Zhang, Dan",https://arxiv.org/abs/2407.02061,"In GPS-denied scenarios, a robust environmental perception and localization system becomes crucial for autonomous driving. In this paper, a LiDAR-based online localization system is developed, incorporating road marking detection and registration on a high-definition (HD) map. Within our system, a road marking detection approach is proposed with real-time performance, in which an adaptive segmentation technique is first introduced to isolate high-reflectance points correlated with road markings, enhancing real-time efficiency. Then, a spatio-temporal probabilistic local map is formed by aggregating historical LiDAR scans, providing a dense point cloud. Finally, a LiDAR bird's-eye view (LiBEV) image is generated, and an instance segmentation network is applied to accurately label the road markings. For road marking registration, a semantic generalized iterative closest point (SG-ICP) algorithm is designed. Linear road markings are modeled as 1-manifolds embedded in 2D space, mitigating the influence of constraints along the linear direction, addressing the under-constrained problem and achieving a higher localization accuracy on HD maps than ICP. Extensive experiments are conducted in real-world scenarios, demonstrating the effectiveness and robustness of our system."
LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and Computable Prior,"Wang, Jiahui; Deng, Yinan; Yang, Yi; Yue, Yufeng",,
Toward Universal and Scalable Road Graph Partitioning for Efficient Multi-Robot Path Planning,"Han, Xingyao; Cao, Bo; Liu, Zhe; Zhou, Shunbo; Zhang, Heng; Wang, Hesheng",,
APEX: Ambidextrous Dual-Arm Robotic Manipulation Using Collision-Free Generative Diffusion Models,"Dastider, Apan; Fang, Hao; Mingjie, Lin",https://arxiv.org/abs/2404.02284,"Dexterous manipulation, particularly adept coordinating and grasping, constitutes a fundamental and indispensable capability for robots, facilitating the emulation of human-like behaviors. Integrating this capability into robots empowers them to supplement and even supplant humans in undertaking increasingly intricate tasks in both daily life and industrial settings. Unfortunately, contemporary methodologies encounter serious challenges in devising manipulation trajectories owing to the intricacies of tasks, the expansive robotic manipulation space, and dynamic obstacles. We propose a novel approach, APEX, to address all these difficulties by introducing a collision-free latent diffusion model for both robotic motion planning and manipulation. Firstly, we simplify the complexity of real-life ambidextrous dual-arm robotic manipulation tasks by abstracting them as aligning two vectors. Secondly, we devise latent diffusion models to produce a variety of robotic manipulation trajectories. Furthermore, we integrate obstacle information utilizing a classifier-guidance technique, thereby guaranteeing both the feasibility and safety of the generated manipulation trajectories. Lastly, we validate our proposed algorithm through extensive experiments conducted on the hardware platform of ambidextrous dual-arm robots. Our algorithm consistently generates successful and seamless trajectories across diverse tasks, surpassing conventional robotic motion planning algorithms. These results carry significant implications for the future design of diffusion robots, enhancing their capability to tackle more intricate robotic manipulation tasks with increased efficiency and safety. Complete video demonstrations of our experiments can be found in https://sites.google.com/view/apex-dual-arm/home."
MonoPlane:Exploiting Monocular Geometric Cues for Generalizable 3D Plane Reconstruction,"Zhao, Wang; Liu, Jiachen; Zhang, Sheng; Li, Yishu; Chen, Sili; Huang, Sharon X.; Liu, Yong-Jin; Guo, Hengkai",,
Active Human Pose Estimation via an Autonomous UAV Agent,"Chen, Jingxi; He, Botao; Singh, Chahat Deep; Fermuller, Cornelia; Aloimonos, Yiannis",https://arxiv.org/abs/2407.01811,"One of the core activities of an active observer involves moving to secure a ""better"" view of the scene, where the definition of ""better"" is task-dependent. This paper focuses on the task of human pose estimation from videos capturing a person's activity. Self-occlusions within the scene can complicate or even prevent accurate human pose estimation. To address this, relocating the camera to a new vantage point is necessary to clarify the view, thereby improving 2D human pose estimation. This paper formalizes the process of achieving an improved viewpoint. Our proposed solution to this challenge comprises three main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone Network for Camera View Error Estimation, and a Combined Planner for devising a feasible motion plan to reposition the camera based on the predicted errors for camera views. The Data Generation Framework utilizes NeRF-based methods to generate a comprehensive dataset of human poses and activities, enhancing the drone's adaptability in various scenarios. The Camera View Error Estimation Network is designed to evaluate the current human pose and identify the most promising next viewing angles for the drone, ensuring a reliable and precise pose estimation from those angles. Finally, the combined planner incorporates these angles while considering the drone's physical and environmental limitations, employing efficient algorithms to navigate safe and effective flight paths. This system represents a significant advancement in active 2D human pose estimation for an autonomous UAV agent, offering substantial potential for applications in aerial cinematography by improving the performance of autonomous human pose estimation and maintaining the operational safety and efficiency of UAVs."
Working Backwards: Learning to Place by Picking,"Limoyo, Oliver; Konar, Abhisek; Ablett, Trevor; Kelly, Jonathan; Hogan, Francois; Dudek, Gregory",https://arxiv.org/abs/2312.02352,"We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific, contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention using two modules: compliant control for grasping and tactile regrasping. We train a policy directly from visual observations through behavioural cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the training environment without privileged information (e.g., placing a plate picked up from a table). We validate our approach in home robot scenarios that include dishwasher loading and table setting. Our approach yields robotic placing policies that outperform policies trained with kinesthetic teaching, both in terms of success rate and data efficiency, while requiring no human supervision."
Towards Designing a Low-Cost Humanoid Robot with Flex Sensors-Based Movement,"Al Omoush, Muhammad H.; Kishore, Sameer; Mehigan, Tracey",,
Discover2Walk: A cable-driven robotic platform to promote gait in pediatric population,"Romero Sorozabal, Pablo; Delgado-Oleas, Gabriel; Laudanski, Annemarie F; Gutierrez, Alvaro; Rocon, Eduardo",,
Robotic Measurement for Electrical Property of Polymers by Force-Sensing Robot toward Materials Lab-Automation,"Asano, Yuki; Okada, Kei; Shiomi, Junichiro",,
JUICER: Data-Efficient Imitation Learning for Robotic Assembly,"Ankile, Lars; Simeonov, Anthony; Shenfeld, Idan; Agrawal, Pulkit",https://arxiv.org/abs/2404.03729,"While learning from demonstrations is powerful for acquiring visuomotor policies, high-performance imitation without large demonstration datasets remains challenging for tasks requiring precise, long-horizon manipulation. This paper proposes a pipeline for improving imitation learning performance with a small human demonstration budget. We apply our approach to assembly tasks that require precisely grasping, reorienting, and inserting multiple parts over long horizons and multiple task phases. Our pipeline combines expressive policy architectures and various techniques for dataset expansion and simulation-based data augmentation. These help expand dataset support and supervise the model with locally corrective actions near bottleneck regions requiring high precision. We demonstrate our pipeline on four furniture assembly tasks in simulation, enabling a manipulator to assemble up to five parts over nearly 2500 time steps directly from RGB images, outperforming imitation and data augmentation baselines. Project website: https://imitation-juicer.github.io/."
Energy Minimization using Custom-Designed Magnetic-Spring Actuators,"Fu, Yue Yang; Kilic, Ali Umut; Braun, David",,
SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles,"Qu, Deyuan; Chen, Qi; Bai, Tianyu; Lu, Hongsheng; Fan, Heng; Zhang, Hao; Fu, Song; Yang, Qing",https://arxiv.org/abs/2312.04822,"Cooperative perception for connected and automated vehicles is traditionally achieved through the fusion of feature maps from two or more vehicles. However, the absence of feature maps shared from other vehicles can lead to a significant decline in 3D object detection performance for cooperative perception models compared to standalone 3D detection models. This drawback impedes the adoption of cooperative perception as vehicle resources are often insufficient to concurrently employ two perception models. To tackle this issue, we present Simultaneous Individual and Cooperative Perception (SiCP), a generic framework that supports a wide range of the state-of-the-art standalone perception backbones and enhances them with a novel Dual-Perception Network (DP-Net) designed to facilitate both individual and cooperative perception. In addition to its lightweight nature with only 0.13M parameters, DP-Net is robust and retains crucial gradient information during feature map fusion. As demonstrated in a comprehensive evaluation on the V2V4Real and OPV2V datasets, thanks to DP-Net, SiCP surpasses state-of-the-art cooperative perception solutions while preserving the performance of standalone perception solutions."
A Fast Online Omnidirectional Quadrupedal Jumping Framework Via Virtual-Model Control and Minimum Jerk Trajectory Generation,"Yue, Linzhu; Zhang, Lingwei; Song, Zhitao; Zhang, Hongbo; Dong, Jinhu; Zeng, Xuanqi; Liu, Yunhui",https://arxiv.org/abs/2407.00658,"Exploring the limits of quadruped robot agility, particularly in the context of rapid and real-time planning and execution of omnidirectional jump trajectories, presents significant challenges due to the complex dynamics involved, especially when considering significant impulse contacts. This paper introduces a new framework to enable fast, omnidirectional jumping capabilities for quadruped robots. Utilizing minimum jerk technology, the proposed framework efficiently generates jump trajectories that exploit its analytical solutions, ensuring numerical stability and dynamic compatibility with minimal computational resources. The virtual model control is employed to formulate a Quadratic Programming (QP) optimization problem to accurately track the Center of Mass (CoM) trajectories during the jump phase. The whole-body control strategies facilitate precise and compliant landing motion. Moreover, the different jumping phase is triggered by time-schedule. The framework's efficacy is demonstrated through its implementation on an enhanced version of the open-source Mini Cheetah robot. Omnidirectional jumps-including forward, backward, and other directional-were successfully executed, showcasing the robot's capability to perform rapid and consecutive jumps with an average trajectory generation and tracking solution time of merely 50 microseconds."
Real-time Coupled Centroidal Motion and Footstep Planning for Biped Robots,"Bartlett, Tara; Manchester, Ian",,
Modernising Delivery: A Low-Energy Tethered Package System using Fixed-Wing Drones,"ORD, Samuel; Marino, Matthew; Wiley, Timothy Colin",,
FlexLoc: Conditional Neural Networks for Zero-Shot Sensor Perspective Invariance in Object Localization with Distributed Multimodal Sensors,"Wu, Jason; Wang, Ziqi; Ouyang, Xiaomin; Jeong, Ho Lyun; Samplawski, Colin; Kaplan, Lance; Marlin, Benjamin; Srivastava, Mani",https://arxiv.org/abs/2406.06796,"Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living. Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy. Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities. However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations). During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies. To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline. Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead. Our evaluations on a multimodal, multiview indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines. The source code of FlexLoc is available at https://github.com/nesl/FlexLoc."
Trans-Rotor: An Active Omnidirectional Aerial-Ground Vehicle With Differential Gear Joint Transformation Mechanism,"Wu, Xuankang; Sun, Haoxiang; Xiao, Tong; Pan, Yanzhang; Fang, Zheng",,
Active Semantic Mapping and Pose Graph Spectral Analysis for Robot Exploration,"Zhang, Rongge; Bong, Haechan Mark; Beltrame, Giovanni",https://arxiv.org/abs/2408.14726,"Exploration in unknown and unstructured environments is a pivotal requirement for robotic applications. A robot's exploration behavior can be inherently affected by the performance of its Simultaneous Localization and Mapping (SLAM) subsystem, although SLAM and exploration are generally studied separately. In this paper, we formulate exploration as an active mapping problem and extend it with semantic information. We introduce a novel active metric-semantic SLAM approach, leveraging recent research advances in information theory and spectral graph theory: we combine semantic mutual information and the connectivity metrics of the underlying pose graph of the SLAM subsystem. We use the resulting utility function to evaluate different trajectories to select the most favorable strategy during exploration. Exploration and SLAM metrics are analyzed in experiments. Running our algorithm on the Habitat dataset, we show that, while maintaining efficiency close to the state-of-the-art exploration methods, our approach effectively increases the performance of metric-semantic SLAM with a 21% reduction in average map error and a 9% improvement in average semantic classification accuracy."
Task-Space Riccati Feedback based Whole Body Control for Underactuated Legged Locomotion,"Yang, Shunpeng; Hong, Zejun; Li, sen; Wensing, Patrick M.; Zhang, Wei; Chen, Hua",https://arxiv.org/abs/2404.00591,"This manuscript primarily aims to enhance the performance of whole-body controllers(WBC) for underactuated legged locomotion. We introduce a systematic parameter design mechanism for the floating-base feedback control within the WBC. The proposed approach involves utilizing the linearized model of unactuated dynamics to formulate a Linear Quadratic Regulator(LQR) and solving a Riccati gain while accounting for potential physical constraints through a second-order approximation of the log-barrier function. And then the user-tuned feedback gain for the floating base task is replaced by a new one constructed from the solved Riccati gain. Extensive simulations conducted in MuJoCo with a point bipedal robot, as well as real-world experiments performed on a quadruped robot, demonstrate the effectiveness of the proposed method. In the different bipedal locomotion tasks, compared with the user-tuned method, the proposed approach is at least 12% better and up to 50% better at linear velocity tracking, and at least 7% better and up to 47% better at angular velocity tracking. In the quadruped experiment, linear velocity tracking is improved by at least 3% and angular velocity tracking is improved by at least 23% using the proposed method."
Ultra Tightly Coupled Passive UWB Localization for Low-density Anchor Networks,"Senevirathna, Nushen M; De Silva, Oscar; Mann, George K. I.; Gosine, Raymond G.",,
CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models,"Huang, Haoxu; Lin, Fanqi; Hu, Yingdong; Wang, Shengjie; Gao, Yang",https://arxiv.org/abs/2403.08248,"Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: https://copa-2024.github.io/"
Embedded Valves for Distributed Control of Soft Pneumatic Actuators,"Zuo, Runze; Mehta, Mayank; Han, Dong Heon; Bruder, Daniel",,
Control-Oriented Reinforcement Active Modeling Scheme for Hysteresis Compensation of Flexible Endoscopic Robot,"Ren, Fan; Wang, Xiangyu; Fang, Yongchun; Qin, Yanding; Wang, Hongpeng; Yu, Ningbo; Han, Jianda",,
Efficient Extrinsic Self-Calibration of Multiple IMUs using Measurement Subset Selection,"Lee, Jongwon; Hanley, David; Bretl, Timothy",https://arxiv.org/abs/2407.02232,"This paper addresses the problem of choosing a sparse subset of measurements for quick calibration parameter estimation. A standard solution to this is selecting a measurement only if its utility -- the difference between posterior (with the measurement) and prior information (without the measurement) -- exceeds some threshold. Theoretically, utility, a function of the parameter estimate, should be evaluated at the estimate obtained with all measurements selected so far, hence necessitating a recalibration with each new measurement. However, we hypothesize that utility is insensitive to changes in the parameter estimate for many systems of interest, suggesting that evaluating utility at some initial parameter guess would yield equivalent results in practice. We provide evidence supporting this hypothesis for extrinsic calibration of multiple inertial measurement units (IMUs), showing the reduction in calibration time by two orders of magnitude by forgoing recalibration for each measurement."
Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation,"He, Tairan; Luo, Zhengyi; Xiao, Wenli; Zhang, Chong; Kitani, Kris; Liu, Changliu; Shi, Guanya",https://arxiv.org/abs/2403.04436,"We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable ""sim-to-data"" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation."
SurrealDriver: Designing LLM-powered Generative Driver Agent Framework based on Human Drivers' Driving-thinking Data,"Jin, Ye; Yang, Ruoxuan; Yi, Zhijie; SHEN, Xiaoxi; Huiling, Peng; Liu, Xiaoan; Qin, Jingli; Jiayang, Li; Gao, Peizhong; Zhou, Guyue; Gong, Jiangtao",https://arxiv.org/abs/2309.13193,"Leveraging advanced reasoning capabilities and extensive world knowledge of large language models (LLMs) to construct generative agents for solving complex real-world problems is a major trend. However, LLMs inherently lack embodiment as humans, resulting in suboptimal performance in many embodied decision-making tasks. In this paper, we introduce a framework for building human-like generative driving agents using post-driving self-report driving-thinking data from human drivers as both demonstration and feedback. To capture high-quality, natural language data from drivers, we conducted urban driving experiments, recording drivers' verbalized thoughts under various conditions to serve as chain-of-thought prompts and demonstration examples for the LLM-Agent. The framework's effectiveness was evaluated through simulations and human assessments. Results indicate that incorporating expert demonstration data significantly reduced collision rates by 81.04\% and increased human likeness by 50\% compared to a baseline LLM-based agent. Our study provides insights into using natural language-based human demonstration data for embodied tasks. The driving-thinking dataset is available at \url{https://github.com/AIR-DISCOVER/Driving-Thinking-Dataset}."
Asymptotically Optimal Lazy Lifelong Sampling-based Algorithm for Efficient Motion Planning in Dynamic Environments,"HUANG, LU; Jing, Xingjian",https://arxiv.org/abs/2409.06521,"The paper introduces an asymptotically optimal lifelong sampling-based path planning algorithm that combines the merits of lifelong planning algorithms and lazy search algorithms for rapid replanning in dynamic environments where edge evaluation is expensive. By evaluating only sub-path candidates for the optimal solution, the algorithm saves considerable evaluation time and thereby reduces the overall planning cost. It employs a novel informed rewiring cascade to efficiently repair the search tree when the underlying search graph changes. Simulation results demonstrate that the algorithm outperforms various state-of-the-art sampling-based planners in addressing both static and dynamic motion planning problems."
Evaluating Gait Symmetry with a Smart Robotic Walker: A Novel Approach to Mobility Assessment,"Abdollah Chalaki, Mahdi; Soleymani, Abed; Li, Xingyu; Mushahwar, Vivian K.; Tavakoli, Mahdi",https://arxiv.org/abs/2408.12005,"Gait asymmetry, a consequence of various neurological or physical conditions such as aging and stroke, detrimentally impacts bipedal locomotion, causing biomechanical alterations, increasing the risk of falls and reducing quality of life. Addressing this critical issue, this paper introduces a novel diagnostic method for gait symmetry analysis through the use of an assistive robotic Smart Walker equipped with an innovative asymmetry detection scheme. This method analyzes sensor measurements capturing the interaction torque between user and walker. By applying a seasonal-trend decomposition tool, we isolate gait-specific patterns within these data, allowing for the estimation of stride durations and calculation of a symmetry index. Through experiments involving 5 experimenters, we demonstrate the Smart Walker's capability in detecting and quantifying gait asymmetry by achieving an accuracy of 84.9% in identifying asymmetric cases in a controlled testing environment. Further analysis explores the classification of these asymmetries based on their underlying causes, providing valuable insights for gait assessment. The results underscore the potential of the device as a precise, ready-to-use monitoring tool for personalized rehabilitation, facilitating targeted interventions for enhanced patient outcomes."
Flexible and Topological Consistent Local Replanning for Multirotors,"Wang, Dong; Ye, Hongkai; Pan, Neng; Huang, Jinxin; Zhang, Bangyan; Mao, Yinian; Huang, Guoquan; Xu, Chao; Gao, Fei",,
Design and Modeling of a Thin-walled Multi-segment Continuum Robotic Bronchoscope,"Bian, Gui-Bin; Zhang, Mingyang; Ye, Qiang; Ren, Han; Zhai, Yu-Peng; Li, Zhen",,
Safe multi-agent reinforcement learning for bimanual dexterous manipulation,"Zhan, Weishu; Chin, Peter",,
Small Multi-Rotor UAV Oriented Direct Thrust Sensor Based on Lightweight Barometers,"Jiang, Han; Chang, Yanchun; yang, liying; He, Yuqing",,
A Closed-loop Control for Lower Limb Exoskeleton Considering Overall Deformations: A Simple and Direct Application Method,"Li, Feng; Yang, Ming; Chen, ziqiang; Luan, Mengbo; Tian, Dingkui; Wu, Xinyu",,
Development of a Throwbot with Shock Absorption Structure,"Keum, Jaeyeong; Kim, Jaemin; Lee, Changgi; Lim, Seunghyun; Ju, Insung; Yun, Dongwon",,
Answerability Fields: Answerable Location Estimation via Diffusion Models,"Azuma, Daichi; Miyanishi, Taiki; Kurita, Shuhei; Sakamoto, Koya; Kawanabe, Motoaki",https://arxiv.org/abs/2407.18497,"In an era characterized by advancements in artificial intelligence and robotics, enabling machines to interact with and understand their environment is a critical research endeavor. In this paper, we propose Answerability Fields, a novel approach to predicting answerability within complex indoor environments. Leveraging a 3D question answering dataset, we construct a comprehensive Answerability Fields dataset, encompassing diverse scenes and questions from ScanNet. Using a diffusion model, we successfully infer and evaluate these Answerability Fields, demonstrating the importance of objects and their locations in answering questions within a scene. Our results showcase the efficacy of Answerability Fields in guiding scene-understanding tasks, laying the foundation for their application in enhancing interactions between intelligent agents and their environments."
Path-Parameterised RRTs for Underactuated Systems,"Abood, Damian; Manchester, Ian",https://arxiv.org/abs/2409.05278,"We present a sample-based motion planning algorithm specialised to a class of underactuated systems using path parameterisation. The structure this class presents under a path parameterisation enables the trivial computation of dynamic feasibility along a path. Using this, a specialised state-based steering mechanism within an RRT motion planning algorithm is developed, enabling the generation of both geometric paths and their time parameterisations without introducing excessive computational overhead. We find with two systems that our algorithm computes feasible trajectories with higher rates of success and lower mean computation times compared to existing approaches."
Grid-based Submap Joining: An Efficient Algorithm for Simultaneously Optimizing Global Occupancy Map and Local Submap Frames,"Wang, Yingyu; Zhao, Liang; Huang, Shoudong",,
3D Affordance Keypoint Detection for Robotic Manipulation,"Liu, Zhiyang; Zhao, Ruiteng; Zhou, Lei; Yuan, Chengran; Wu, Yuwei; Guo, Sheng; Zhang, Zhengshen; Liu, Chenchen; Ang Jr, Marcelo H",,
CATOA:Cooperative Calibration of Timestamp Measurements for Distributed Multi-Robot Localization,"Wen, Feiyang; Zhao, Hanying; Jincheng, Yu; Cui, Shulin; Shen, Yuan",,
DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for Robotic Exploration in the Dark,"Zhang, Tianyi; Huang, Kaining; Zhi, Weiming; Johnson-Roberson, Matthew",https://arxiv.org/abs/2403.10814,"Humans have the remarkable ability to construct consistent mental models of an environment, even under limited or varying levels of illumination. We wish to endow robots with this same capability. In this paper, we tackle the challenge of constructing a photorealistic scene representation under poorly illuminated conditions and with a moving light source. We approach the task of modeling illumination as a learning problem, and utilize the developed illumination model to aid in scene reconstruction. We introduce an innovative framework that uses a data-driven approach, Neural Light Simulators (NeLiS), to model and calibrate the camera-light system. Furthermore, we present DarkGS, a method that applies NeLiS to create a relightable 3D Gaussian scene model capable of real-time, photorealistic rendering from novel viewpoints. We show the applicability and robustness of our proposed simulator and system in a variety of real-world environments."
FI-SLAM: Feature Fusion and Instance Reconstruction for Neural Implicit SLAM,"Wang, Xingshuo; Zhang, Yunzhou; Zhang, Zhiyao; Wang, Mengting; Li, Zhiteng; Chen, Xuanhua",,
Multi-Fov-Constrained Trajectory Planning for Multirotor Safe Landing,"Wang, Dong; Wang, Jingping; He, Suqin; Huang, Jinxin; Zhang, Bangyan; Mao, Yinian; Huang, Guoquan; Xu, Chao; Gao, Fei",,
ViSaRL: Visual Reinforcement Learning Guided by Human Saliency,"Liang, Anthony; B&#305;y&#305;k, Erdem; Thomason, Jesse",https://arxiv.org/abs/2403.10940,"Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-robot tasks compared to the baseline which does not use saliency."
Enhancing Exploratory Capability of Visual Navigation Using Uncertainty of Implicit Scene Representation,"Wang, Yichen; Liu, Qiming; Liu, Zhe; Wang, Hesheng",,
Novel Multi-port Output Twisted String Actuator with Self-differential Mechanism: Hand Glove Application,"Wei, Dunwen; Cui, Chenguang; Yu, Haitao; Gao, Tao; Hussain, Sajjad; Ficuciello, Fanny",,
Geometry-aided Underwater 3D Mapping Using Side-scan Sonar,"Yang, Yiqiao; Pang, Chenglin; Wu, Chengdong; Fang, Zheng",,
A High-Performance Anthropomorphic Robotic Arm for Household Applications,"Liu, Tianliang; Yang, Sicheng; Li, Jingchen; Chen, Xiangchi; WANG, Shuai; Teng, Xiao; Lee, Wang Wei; LI, XIONG; Zheng, Yu",,
Pseudo-Domain Adversarial Networks with Electrical Impedance Tomography for Electrode Offset Error,"Xu, Gengchen; Chen, Haofeng; Yang, Xuanxuan; ma, gang; Wang, Xiaojie",,
Temporal Attention for Cross-View Sequential Image Localization,"Yuan, Dong; Maire, Frederic; Dayoub, Feras",https://arxiv.org/abs/2408.15569,"This paper introduces a novel approach to enhancing cross-view localization, focusing on the fine-grained, sequential localization of street-view images within a single known satellite image patch, a significant departure from traditional one-to-one image retrieval methods. By expanding to sequential image fine-grained localization, our model, equipped with a novel Temporal Attention Module (TAM), leverages contextual information to significantly improve sequential image localization accuracy. Our method shows substantial reductions in both mean and median localization errors on the Cross-View Image Sequence (CVIS) dataset, outperforming current state-of-the-art single-image localization techniques. Additionally, by adapting the KITTI-CVL dataset into sequential image sets, we not only offer a more realistic dataset for future research but also demonstrate our model's robust generalization capabilities across varying times and areas, evidenced by a 75.3% reduction in mean distance error in cross-view sequential image localization."
Multi-modal Motion Prediction using Temporal Ensembling with Learning-based Aggregation,"Hong, Kai-Yin; Wang, Chieh-Chih; Lin, Wen-Chieh",,
DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment,"Guo, Yanjiang; Wang, Yen-Jen; Zha, Lihan; Chen, Jianyu",https://arxiv.org/abs/2307.00329,"Large language models (LLMs) encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous work has explored how to ground LLMs in robotic tasks to generate feasible and executable textual plans. However, low-level execution in the physical world may deviate from the high-level textual plan due to environmental perturbations or imperfect controller design. In this paper, we propose \textbf{DoReMi}, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, we leverage LLMs to play a dual role, aiding not only in high-level planning but also generating constraints that can indicate misalignment during execution. Then vision language models (VLMs) are utilized to detect constraint violations continuously. Our pipeline can monitor the low-level execution and enable timely recovery if certain plan-execution misalignment occurs. Experiments on various complex tasks including robot arms and humanoid robots demonstrate that our method can lead to higher task success rates and shorter task completion times. Videos of DoReMi are available at \url{https://sites.google.com/view/doremi-paper}."
SURESTEP: An Uncertainty-Aware Trajectory Optimization Framework to Enhance Visual Tool Tracking for Robust Surgical Automation,"Shinde, Nikhil; Chiu, Zih-Yun; Richter, Florian; Lim, Jason; Zhi, Yuheng; Herbert, Sylvia; Yip, Michael C.",https://arxiv.org/abs/2404.00123,"Inaccurate tool localization is one of the main reasons for failures in automating surgical tasks. Imprecise robot kinematics and noisy observations caused by the poor visual acuity of an endoscopic camera make tool tracking challenging. Previous works in surgical automation adopt environment-specific setups or hard-coded strategies instead of explicitly considering motion and observation uncertainty of tool tracking in their policies. In this work, we present SURESTEP, an uncertainty-aware trajectory optimization framework for robust surgical automation. We model the uncertainty of tool tracking with the components motivated by the sources of noise in typical surgical scenes. Using a Gaussian assumption to propagate our uncertainty models through a given tool trajectory, SURESTEP provides a general framework that minimizes the upper bound on the entropy of the final estimated tool distribution. We compare SURESTEP with a baseline method on a real-world suture needle regrasping task under challenging environmental conditions, such as poor lighting and a moving endoscopic camera. The results over 60 regrasps on the da Vinci Research Kit (dVRK) demonstrate that our optimized trajectories significantly outperform the un-optimized baseline."
TempBEV: Improving Learned BEV Encoders with Combined Image and BEV Space Temporal Aggregation,"Monninger, Thomas; Dokkadi, Vandana; Anwar, Md Zafar; Staab, Steffen",https://arxiv.org/abs/2404.11803,"Autonomous driving requires an accurate representation of the environment. A strategy toward high accuracy is to fuse data from several sensors. Learned Bird's-Eye View (BEV) encoders can achieve this by mapping data from individual sensors into one joint latent space. For cost-efficient camera-only systems, this provides an effective mechanism to fuse data from multiple cameras with different views. Accuracy can further be improved by aggregating sensor information over time. This is especially important in monocular camera systems to account for the lack of explicit depth and velocity measurements. Thereby, the effectiveness of developed BEV encoders crucially depends on the operators used to aggregate temporal information and on the used latent representation spaces. We analyze BEV encoders proposed in the literature and compare their effectiveness, quantifying the effects of aggregation operators and latent representations. While most existing approaches aggregate temporal information either in image or in BEV latent space, our analyses and performance comparisons suggest that these latent representations exhibit complementary strengths. Therefore, we develop a novel temporal BEV encoder, TempBEV, which integrates aggregated temporal information from both latent spaces. We consider subsequent image frames as stereo through time and leverage methods from optical flow estimation for temporal stereo encoding. Empirical evaluation on the NuScenes dataset shows a significant improvement by TempBEV over the baseline for 3D object detection and BEV segmentation. The ablation uncovers a strong synergy of joint temporal aggregation in the image and BEV latent space. These results indicate the overall effectiveness of our approach and make a strong case for aggregating temporal information in both image and BEV latent spaces."
Indoor Position Estimation Using NLoS Reflect Path by Wireless Distance Sensors,"Itsuka, Tomoya; Kurazume, Ryo",,
Robot Guided Evacuation with Viewpoint Constraints,"Gong, Chen; Meghjani, Malika; Prasetyo, Marcel Bartholomeus",,
Switching Sampling Space of Model Predictive Path-Integral Controller to Balance Efficiency and Safety in 4WIDS Vehicle Navigation,"Aoki, Mizuho; Honda, Kohei; Okuda, Hiroyuki; Suzuki, Tatsuya",https://arxiv.org/abs/2409.08648,"Four-wheel independent drive and steering vehicle (4WIDS Vehicle, Swerve Drive Robot) has the ability to move in any direction by its eight degrees of freedom (DoF) control inputs. Although the high maneuverability enables efficient navigation in narrow spaces, obtaining the optimal command is challenging due to the high dimension of the solution space. This paper presents a navigation architecture using the Model Predictive Path Integral (MPPI) control algorithm to avoid collisions with obstacles of any shape and reach a goal point. The key idea to make the problem easier is to explore the optimal control input in a reasonably reduced dimension that is adequate for navigation. Through evaluation in simulation, we found that selecting the sampling space of MPPI greatly affects navigation performance. In addition, our proposed controller which switches multiple sampling spaces according to the real-time situation can achieve balanced behavior between efficiency and safety. Source code is available at https://github.com/MizuhoAOKI/mppi_swerve_drive_ros"
SLIP Nature Embodied Robust Quadruped Robot Control,"Hong, Jin song; Yeo, Changmin; Bae, Sangjin; Hong, Jeongwoo; Oh, Sehoon",,
Advanced Liquid and Dust Detection Sensor Setup and Algorithm Based on YOLO and Feature Extraction for Commercial Autonomous Cleaning Robots,"Jung, Dae-Hwan; Hong, Hyun Seok; Park, Sahng-Gyu; Lee, Yeongrok; Lee, Woosub",,
InverseMatrixVT3D: An Efficient Projection Matrix-Based Approach for 3D Occupancy Prediction,"Ming, Zhenxing; Berrio Perez, Julie Stephany; Shan, Mao; Worrall, Stewart",https://arxiv.org/abs/2401.12422,"This paper introduces InverseMatrixVT3D, an efficient method for transforming multi-view image features into 3D feature volumes for 3D semantic occupancy prediction. Existing methods for constructing 3D volumes often rely on depth estimation, device-specific operators, or transformer queries, which hinders the widespread adoption of 3D occupancy models. In contrast, our approach leverages two projection matrices to store the static mapping relationships and matrix multiplications to efficiently generate global Bird's Eye View (BEV) features and local 3D feature volumes. Specifically, we achieve this by performing matrix multiplications between multi-view image feature maps and two sparse projection matrices. We introduce a sparse matrix handling technique for the projection matrices to optimize GPU memory usage. Moreover, a global-local attention fusion module is proposed to integrate the global BEV features with the local 3D feature volumes to obtain the final 3D volume. We also employ a multi-scale supervision mechanism to enhance performance further. Extensive experiments performed on the nuScenes and SemanticKITTI datasets reveal that our approach not only stands out for its simplicity and effectiveness but also achieves the top performance in detecting vulnerable road users (VRU), crucial for autonomous driving and road safety. The code has been made available at: https://github.com/DanielMing123/InverseMatrixVT3D"
Deeper Introspective SLAM: How to Avoid Tracking Failures Over Longer Routes?,"Naveed, Kanwal; Anjum, Muhammad Latif; Hussain, Wajahat; LEE, DONGHWAN",,
TriLoc-NetVLAD: Enhancing Long-term Place Recognition in Orchards with a Novel LiDAR-Based Approach,"Sun, Na; Fan, Zhengqiang; Qiu, Quan; Li, Tao; Feng, Qingchun; Ji, Chao; Zhao, Chunjiang",,
Dynamic SpectraFormer for Ultra-High Resolution Underwater Image Enhancement,"HU, ZHIQIANG; Yu, Tao; Huang, Shouren",,
"Design, Modelling, and Experimental Validation of a Soft Continuum Wrist Section developed for a Prosthetic Hand","Sulaiman, Shifa; MENON, MEHUL; Schetter, Francesco; Ficuciello, Fanny",,
SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects,"Ummadisingu, Avinash; Choi, Jongkeum; Yamane, Koki; Masuda, Shimpei; Fukaya, Naoki; Takahashi, Kuniyuki",https://arxiv.org/abs/2403.19607,"Acquiring accurate depth information of transparent objects using off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and Robotics. Depth estimation/completion methods are typically employed and trained on datasets with quality depth labels acquired from either simulation, additional sensors or specialized data collection setups and known 3d models. However, acquiring reliable depth information for datasets at scale is not straightforward, limiting training scalability and generalization. Neural Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide success in novel view synthesis and shape recovery. However, heuristics and controlled environments (lights, backgrounds, etc) are often required to accurately capture specular surfaces. In this paper, we propose using Visual Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to guide the NeRF reconstruction process for these objects via the simultaneous reconstruction of semantic fields and extensions to increase robustness. Our proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant performance on depth completion datasets for transparent objects and robotic grasping."
Coalition Formation Game Approach for Task Allocation in Heterogeneous Multi-Robot Systems under Resource Constraints,"Zhang, Liwang; Liang, Dong; Li, Minglong; YANG, WENJING; Yang, Shaowu",,
FusionTrack: An Online 3D Multi-object Tracking Framework Based on Camera-LiDAR Fusion,"Zeng, Weizhen; Fan, Jiaqi; Tian, Xuelin; Chu, Hongqing; Gao, Bingzhao",,
Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression,"Bui, Huy Hoang; Bui, Bach-Thuan; Tran, Dinh Tuan; Lee, Joo-Ho",https://arxiv.org/abs/2403.10297,"Classical structural-based visual localization methods offer high accuracy but face trade-offs in terms of storage, speed, and privacy. A recent innovation, keypoint scene coordinate regression (KSCR) named D2S addresses these issues by leveraging graph attention networks to enhance keypoint relationships and predict their 3D coordinates using a simple multilayer perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using established 2D-3D correspondences. While KSCR achieves competitive results, rivaling state-of-the-art image-retrieval methods like HLoc across multiple benchmarks, its performance is hindered when data samples are limited due to the deep learning model's reliance on extensive data. This paper proposes a solution to this challenge by introducing a pipeline for keypoint descriptor synthesis using Neural Radiance Field (NeRF). By generating novel poses and feeding them into a trained NeRF model to create new views, our approach enhances the KSCR's generalization capabilities in data-scarce environments. The proposed system could significantly improve localization accuracy by up to 50% and cost only a fraction of time for data synthesis. Furthermore, its modular design allows for the integration of multiple NeRFs, offering a versatile and efficient solution for visual localization. The implementation is publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map."
VIVO: A Visual-Inertial-Velocity Odometry with Online Calibration in Challenging Condition,"Han, Fuzhang; Jia, Shenhan; Huang, Wenjun; Wang, Yue; Xiong, Rong",,
Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation,"D'Amely di Melendugno, Guido Maria; Flaborea, Alessandro; Mettes, Pascal; Galasso, Fabio",https://arxiv.org/abs/2407.13567,"Autonomous robots are increasingly becoming a strong fixture in social environments. Effective crowd navigation requires not only safe yet fast planning, but should also enable interpretability and computational efficiency for working in real-time on embedded devices. In this work, we advocate for hyperbolic learning to enable crowd navigation and we introduce Hyp2Nav. Different from conventional reinforcement learning-based crowd navigation methods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to better encode the hierarchical nature of decision-making processes in navigation tasks. We propose a hyperbolic policy model and a hyperbolic curiosity module that results in effective social navigation, best success rates, and returns across multiple simulation settings, using up to 6 times fewer parameters than competitor state-of-the-art models. With our approach, it becomes even possible to obtain policies that work in 2-dimensional embedding spaces, opening up new possibilities for low-resource crowd navigation and model interpretability. Insightfully, the internal hyperbolic representation of Hyp2Nav correlates with how much attention the robot pays to the surrounding crowds, e.g. due to multiple people occluding its pathway or to a few of them showing colliding plans, rather than to its own planned route. The code is available at https://github.com/GDam90/hyp2nav."
Visual Perception System for Autonomous Driving,"Zhang, Qi; Gou, Siyuan; Li, Wenbin",https://arxiv.org/abs/2303.02257,"The recent surge in interest in autonomous driving stems from its rapidly developing capacity to enhance safety, efficiency, and convenience. A pivotal aspect of autonomous driving technology is its perceptual systems, where core algorithms have yielded more precise algorithms applicable to autonomous driving, including vision-based Simultaneous Localization and Mapping (SLAMs), object detection, and tracking algorithms. This work introduces a visual-based perception system for autonomous driving that integrates trajectory tracking and prediction of moving objects to prevent collisions, while addressing autonomous driving's localization and mapping requirements. The system leverages motion cues from pedestrians to monitor and forecast their movements and simultaneously maps the environment. This integrated approach resolves camera localization and the tracking of other moving objects in the scene, subsequently generating a sparse map to facilitate vehicle navigation. The performance, efficiency, and resilience of this approach are substantiated through comprehensive evaluations of both simulated and real-world datasets."
Density-aware Domain Generalization for LiDAR Semantic Segmentation,"Kim, Jaeyeul; Woo, Jungwan; Shin, Ukcheol; Oh, Jean; Im, Sunghoon",,
Thermal Ablation Therapy Control with Tissue Necrosis-driven Temperature Feedback Enabled by Neural State Space Model with Extended Kalman Filter,"Murakami, Ryo; Mori, Satoshi; Zhang, Haichong",,
Saturation in the Null-Space (SNS) for Tele-operated Surgery: Prioritized Motion Control for RCM and Joint Limit Constraints,"KANA, SREEKANTH; Pérez Arias, Antonia; Kahlau, Robert; Kanajar, Pavan; Sharma, Shashank",,
Event-Free Moving Object Segmentation from Moving Ego Vehicle,"Zhou, Zhuyun; Wu, Zongwei; Paudel, Danda Pani; Boutteau, Rémi; Yang, Fan; Van Gool, Luc; Timofte, Radu; Ginhac, Dominique",https://arxiv.org/abs/2305.00126,"Moving object segmentation (MOS) in dynamic scenes is challenging for autonomous driving, especially for sequences obtained from moving ego vehicles. Most state-of-the-art methods leverage motion cues obtained from optical flow maps. However, since these methods are often based on optical flows that are pre-computed from successive RGB frames, this neglects the temporal consideration of events occurring within inter-frame and limits the practicality of these methods in real-life situations. To address these limitations, we propose to exploit event cameras for better video understanding, which provide rich motion cues without relying on optical flow. To foster research in this area, we first introduce a novel large-scale dataset called DSEC-MOS for moving object segmentation from moving ego vehicles. Subsequently, we devise EmoFormer, a novel network able to exploit the event data. For this purpose, we fuse the event prior with spatial semantic maps to distinguish moving objects from the static background, adding another level of dense supervision around our object of interest - moving ones. Our proposed network relies only on event data for training but does not require event input during inference, making it directly comparable to frame-only methods in terms of efficiency and more widely usable in many application cases. An exhaustive comparison with 8 state-of-the-art video object segmentation methods highlights a significant performance improvement of our method over all other methods. Project Page: https://github.com/ZZY-Zhou/DSEC-MOS."
HeLiMOS: A Dataset for Moving Object Segmentation in 3D Point Clouds From Heterogeneous LiDAR Sensors,"LIM, HYUNGTAE; Jang, Seoyeon; Mersch, Benedikt; Behley, Jens; Myung, Hyun; Stachniss, Cyrill",https://arxiv.org/abs/2408.06328,"Moving object segmentation (MOS) using a 3D light detection and ranging (LiDAR) sensor is crucial for scene understanding and identification of moving objects. Despite the availability of various types of 3D LiDAR sensors in the market, MOS research still predominantly focuses on 3D point clouds from mechanically spinning omnidirectional LiDAR sensors. Thus, we are, for example, lacking a dataset with MOS labels for point clouds from solid-state LiDAR sensors which have irregular scanning patterns. In this paper, we present a labeled dataset, called \textit{HeLiMOS}, that enables to test MOS approaches on four heterogeneous LiDAR sensors, including two solid-state LiDAR sensors. Furthermore, we introduce a novel automatic labeling method to substantially reduce the labeling effort required from human annotators. To this end, our framework exploits an instance-aware static map building approach and tracking-based false label filtering. Finally, we provide experimental results regarding the performance of commonly used state-of-the-art MOS approaches on HeLiMOS that suggest a new direction for a sensor-agnostic MOS, which generally works regardless of the type of LiDAR sensors used to capture 3D point clouds. Our dataset is available at https://sites.google.com/view/helimos."
Learning dynamics models for velocity estimation in autonomous racing,"W&#281;grzynowski, Jan; Czechmanowski, Grzegorz; Kicki, Piotr; Walas, Krzysztof, Tadeusz",https://arxiv.org/abs/2408.15610,"Velocity estimation is of great importance in autonomous racing. Still, existing solutions are characterized by limited accuracy, especially in the case of aggressive driving or poor generalization to unseen road conditions. To address these issues, we propose to utilize Unscented Kalman Filter (UKF) with a learned dynamics model that is optimized directly for the state estimation task. Moreover, we propose to aid this model with the online-estimated friction coefficient, which increases the estimation accuracy and enables zero-shot adaptation to the new road conditions. To evaluate the UKF-based velocity estimator with the proposed dynamics model, we introduced a publicly available dataset of aggressive manoeuvres performed by an F1TENTH car, with sideslip angles reaching 40°. Using this dataset, we show that learning the dynamics model through UKF leads to improved estimation performance and that the proposed solution outperforms state-of-the-art learning-based state estimators by 17% in the nominal scenario. Moreover, we present unseen zero-shot adaptation abilities of the proposed method to the new road surface thanks to the use of the proposed learning-based tire dynamics model with online friction estimation."
An Adaptive Coordination System based on Functional Expressions of Robots and Environment Understanding,"Kato, Yuki; Yoshida, Takahiro; Sueoka, Yuichiro; Osuka, Koichi; Yajima, Ryosuke; Nagatani, Keiji; Asama, Hajime",,
TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural Radiance Fields,"Duan, Chengyao; Yang, Zhiliu",https://arxiv.org/abs/2310.18917,"Previous attempts to integrate Neural Radiance Fields (NeRF) into the Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or require the ground truth camera poses, which impedes their application in real-world scenarios. This paper proposes a time-varying representation to track and reconstruct the dynamic scenes. Firstly, two processes, a tracking process and a mapping process, are maintained simultaneously in our framework. In the tracking process, all input images are uniformly sampled and then progressively trained in a self-supervised paradigm. In the mapping process, we leverage motion masks to distinguish dynamic objects from the static background, and sample more pixels from dynamic areas. Secondly, the parameter optimization for both processes is comprised of two stages: the first stage associates time with 3D positions to convert the deformation field to the canonical field. The second stage associates time with the embeddings of the canonical field to obtain colors and a Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection strategy based on the overlapping rate. Our approach is evaluated on two synthetic datasets and one real-world dataset, and the experiments validate that our method achieves competitive results in both tracking and mapping when compared to existing state-of-the-art NeRF-based dynamic SLAM systems."
Hierarchical Search-Based Cooperative Motion Planning,"Wu, Yuchen; Yang, Yifan; Xu, Gang; Cao, Junjie; Chen, Yansong; Wen, Licheng; Liu, Yong",,
Thermal-NeRF: Neural Radiance Fields from an Infrared Camera,"YE, Tianxiang; Wu, Qi; DENG, Junyuan; Liu, Guoqing; Liu, Liu; Xia, Songpengcheng; Pang, Liang; Yu, Wenxian; Pei, Ling",https://arxiv.org/abs/2403.10340,"In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant potential in encoding highly-detailed 3D geometry and environmental appearance, positioning themselves as a promising alternative to traditional explicit representation for 3D scene reconstruction. However, the predominant reliance on RGB imaging presupposes ideal lighting conditions: a premise frequently unmet in robotic applications plagued by poor lighting or visual obstructions. This limitation overlooks the capabilities of infrared (IR) cameras, which excel in low-light detection and present a robust alternative under such adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first method that estimates a volumetric scene representation in the form of a NeRF solely from IR imaging. By leveraging a thermal mapping and structural thermal constraint derived from the thermal characteristics of IR imaging, our method showcasing unparalleled proficiency in recovering NeRFs in visually degraded scenes where RGB-based methods fall short. We conduct extensive experiments to demonstrate that Thermal-NeRF can achieve superior quality compared to existing methods. Furthermore, we contribute a dataset for IR-based NeRF applications, paving the way for future research in IR NeRF reconstruction."
A Multi-DoF Anthropomorphic Hand with Integrated Tactile Feedback for Grasping and Manipulation in Human Environments,"Yang, Sicheng; Lee, Wang Wei; Zhang, Zhong; Xiong, youda; Liang, Jiaming; Lu, Peng; ZHU, YONGHUI; Liu, Tianliang; Li, Jingchen; Wang, Rui; LI, XIONG; Zheng, Yu",,
Learning a Shape-Conditioned Agent for Purely Tactile In-Hand Manipulation of Various Objects,"Pitz, Johannes; Röstel, Lennart; Sievers, Leon; Burschka, Darius; Bäuml, Berthold",https://arxiv.org/abs/2407.18834,"Reorienting diverse objects with a multi-fingered hand is a challenging task. Current methods in robotic in-hand manipulation are either object-specific or require permanent supervision of the object state from visual sensors. This is far from human capabilities and from what is needed in real-world applications. In this work, we address this gap by training shape-conditioned agents to reorient diverse objects in hand, relying purely on tactile feedback (via torque and position measurements of the fingers' joints). To achieve this, we propose a learning framework that exploits shape information in a reinforcement learning policy and a learned state estimator. We find that representing 3D shapes by vectors from a fixed set of basis points to the shape's surface, transformed by its predicted 3D pose, is especially helpful for learning dexterous in-hand manipulation. In simulation and real-world experiments, we show the reorientation of many objects with high success rates, on par with state-of-the-art results obtained with specialized single-object agents. Moreover, we show generalization to novel objects, achieving success rates of $\sim$90% even for non-convex shapes."
Perception-Driven Shared Control Architecture for Agricultural Robots Performing Harvesting Tasks,"Palmieri, Jozsef; Di Lillo, Paolo; Sanfeliu, Alberto; Marino, Alessandro",,
VRSO: Visual-Centric Reconstruction for Static Object Annotation,"Yu, Chenyao; Cai, Yingfeng; Zhang, Jiaxin; Sui, Wei; Kong, Hui; Yang, Cong",https://arxiv.org/abs/2403.15026,"As a part of the perception results of intelligent driving systems, static object detection (SOD) in 3D space provides crucial cues for driving environment understanding. With the rapid deployment of deep neural networks for SOD tasks, the demand for high-quality training samples soars. The traditional, also reliable, way is manual labelling over the dense LiDAR point clouds and reference images. Though most public driving datasets adopt this strategy to provide SOD ground truth (GT), it is still expensive and time-consuming in practice. This paper introduces VRSO, a visual-centric approach for static object annotation. Experiments on the Waymo Open Dataset show that the mean reprojection error from VRSO annotation is only 2.6 pixels, around four times lower than the Waymo Open Dataset labels (10.6 pixels). VRSO is distinguished in low cost, high efficiency, and high quality: (1) It recovers static objects in 3D space with only camera images as input, and (2) manual annotation is barely involved since GT for SOD tasks is generated based on an automatic reconstruction and annotation pipeline."
Soft finger rotational stability for precision grasps,"Jang, Hun; Petrichenko, Valentyn; Bae, Joonbum; Haninger, Kevin",https://arxiv.org/abs/2310.04846,"Soft robotic fingers can safely grasp fragile or variable form objects, but their force capacity is limited, especially with less contact area: precision grasps and when objects are smaller or not spherical. Current research is improving force capacity through mechanical design by increasing contact area or stiffness, typically without models which explain soft finger force limitations. To address this, this paper considers two types of soft grip failure, slip and dynamic rotational stability. For slip, the validity of a Coulomb model investigated, identifying the effect of contact area, pressure, and relative pose. For rotational stability, bulk linear stiffness of the fingers is used to develop conditions for dynamic stability and identify when rotation leads to slip. Together, these models suggest contact area improves force capacity by increasing transverse stiffness and normal force. The models are validated on pneumatic fingers, both custom PneuNets-based and commercially available. The models are used to find grip parameters which increase force capacity without failure."
CGA: Corridor Generating Algorithm for Multi-Agent Environments,"Pertzovsky, Arseniy; Stern, Roni; Zivan, Roie",,
Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation,"Timoneda, Xavier; Herb, Markus; Duerr, Fabian; Goehring, Daniel; Yu, Fisher",,
Combining Sampling- and Gradient-based Planning for Contact-rich Manipulation,"Rozzi, Filippo; Roveda, Loris; Haninger, Kevin",https://arxiv.org/abs/2310.04822,"Planning over discontinuous dynamics is needed for robotics tasks like contact-rich manipulation, which presents challenges in the numerical stability and speed of planning methods when either neural network or analytical models are used. On the one hand, sampling-based planners require higher sample complexity in high-dimensional problems and cannot describe safety constraints such as force limits. On the other hand, gradient-based solvers can suffer from local optima and convergence issues when the Hessian is poorly conditioned. We propose a planning method with both sampling- and gradient-based elements, using the Cross-entropy Method to initialize a gradient-based solver, providing better search over local minima and the ability to handle explicit constraints. We show the approach allows smooth, stable contact-rich planning for an impedance-controlled robot making contact with a stiff environment, benchmarking against gradient-only MPC and CEM."
Fine-tuning the Diffusion Model and Distilling Informative Priors for Sparse-view 3D Reconstruction,"Tang, Jiadong; Gao, Yu; Jiang, Tianji; Yang, Yi; Fu, Mengyin",https://arxiv.org/abs/2405.16517,"We aim to tackle sparse-view reconstruction of a 360 3D scene using priors from latent diffusion models (LDM). The sparse-view setting is ill-posed and underconstrained, especially for scenes where the camera rotates 360 degrees around a point, as no visual information is available beyond some frontal views focused on the central object(s) of interest. In this work, we show that pretrained 2D diffusion models can strongly improve the reconstruction of a scene with low-cost fine-tuning. Specifically, we present SparseSplat360 (Sp2360), a method that employs a cascade of in-painting and artifact removal models to fill in missing details and clean novel views. Due to superior training and rendering speeds, we use an explicit scene representation in the form of 3D Gaussians over NeRF-based implicit representations. We propose an iterative update strategy to fuse generated pseudo novel views with existing 3D Gaussians fitted to the initial sparse inputs. As a result, we obtain a multi-view consistent scene representation with details coherent with the observed inputs. Our evaluation on the challenging Mip-NeRF360 dataset shows that our proposed 2D to 3D distillation algorithm considerably improves the performance of a regularized version of 3DGS adapted to a sparse-view setting and outperforms existing sparse-view reconstruction methods in 360 scene reconstruction. Qualitatively, our method generates entire 360 scenes from as few as 9 input views, with a high degree of foreground and background detail."
Reinforcement Learning for Active Search and Grasp in Clutter,"Pitcher, Thomas; Förster, Julian; Chung, Jen Jen",https://arxiv.org/abs/2004.13358,"Grasping in cluttered scenes is challenging for robot vision systems, as detection accuracy can be hindered by partial occlusion of objects. We adopt a reinforcement learning (RL) framework and 3D vision architectures to search for feasible viewpoints for grasping by the use of hand-mounted RGB-D cameras. To overcome the disadvantages of photo-realistic environment simulation, we propose a large-scale dataset called Real Embodied Dataset (RED), which includes full-viewpoint real samples on the upper hemisphere with amodal annotation and enables a simulator that has real visual feedback. Based on this dataset, a practical 3-stage transferable active grasping pipeline is developed, that is adaptive to unseen clutter scenes. In our pipeline, we propose a novel mask-guided reward to overcome the sparse reward issue in grasping and ensure category-irrelevant behavior. The grasping pipeline and its possible variants are evaluated with extensive experiments both in simulation and on a real-world UR-5 robotic arm."
PanopticRecon: Leverage Open-vocabulary Instance Segmentation for Zero-shot Panoptic Reconstruction,"Yu, Xuan; Liu, Yili; Han, Chenrui; Mao, Sitong; ZHOU, Shunbo; Xiong, Rong; Liao, Yiyi; Wang, Yue",https://arxiv.org/abs/2407.01349,"Panoptic reconstruction is a challenging task in 3D scene understanding. However, most existing methods heavily rely on pre-trained semantic segmentation models and known 3D object bounding boxes for 3D panoptic segmentation, which is not available for in-the-wild scenes. In this paper, we propose a novel zero-shot panoptic reconstruction method from RGB-D images of scenes. For zero-shot segmentation, we leverage open-vocabulary instance segmentation, but it has to face partial labeling and instance association challenges. We tackle both challenges by propagating partial labels with the aid of dense generalized features and building a 3D instance graph for associating 2D instance IDs. Specifically, we exploit partial labels to learn a classifier for generalized semantic features to provide complete labels for scenes with dense distilled features. Moreover, we formulate instance association as a 3D instance graph segmentation problem, allowing us to fully utilize the scene geometry prior and all 2D instance masks to infer global unique pseudo 3D instance ID. Our method outperforms state-of-the-art methods on the indoor dataset ScanNet V2 and the outdoor dataset KITTI-360, demonstrating the effectiveness of our graph segmentation method and reconstruction network."
SwiftBase: A dataset based on high-frequency visual measurement for Visual-Inertial Localization in high-speed motion scenes,"Zou, Zhenghao; Zhao, Chunhui; Kao, XiRui; Liu, jiang bo; Chai, Haochen; Lyu, Yang",,
Four-Axis Adaptive Fingers Hand for Object Insertion: FAAF Hand,"Fukaya, Naoki; Yamane, Koki; Masuda, Shimpei; Ummadisingu, Avinash; Maeda, Shin-ichi; Takahashi, Kuniyuki",https://arxiv.org/abs/2407.21245,"Robots operating in the real world face significant but unavoidable issues in object localization that must be dealt with. A typical approach to address this is the addition of compliance mechanisms to hardware to absorb and compensate for some of these errors. However, for fine-grained manipulation tasks, the location and choice of appropriate compliance mechanisms are critical for success. For objects to be inserted in a target site on a flat surface, the object must first be successfully aligned with the opening of the slot, as well as correctly oriented along its central axis, before it can be inserted. We developed the Four-Axis Adaptive Finger Hand (FAAF hand) that is equipped with fingers that can passively adapt in four axes (x, y, z, yaw) enabling it to perform insertion tasks including lid fitting in the presence of significant localization errors. Furthermore, this adaptivity allows the use of simple control methods without requiring contact sensors or other devices. Our results confirm the ability of the FAAF hand on challenging insertion tasks of square and triangle-shaped pegs (or prisms) and placing of container lids in the presence of position errors in all directions and rotational error along the object's central axis, using a simple control scheme."
MuTT: A Multimodal Trajectory Transformer for Robot Skills,"Kienle, Claudius; Alt, Benjamin; Celik, Onur; Becker, Philipp; Katic, Darko; Jäkel, Rainer; Neumann, Gerhard",https://arxiv.org/abs/2407.15660,"High-level robot skills represent an increasingly popular paradigm in robot programming. However, configuring the skills' parameters for a specific task remains a manual and time-consuming endeavor. Existing approaches for learning or optimizing these parameters often require numerous real-world executions or do not work in dynamic environments. To address these challenges, we propose MuTT, a novel encoder-decoder transformer architecture designed to predict environment-aware executions of robot skills by integrating vision, trajectory, and robot skill parameters. Notably, we pioneer the fusion of vision and trajectory, introducing a novel trajectory projection. Furthermore, we illustrate MuTT's efficacy as a predictor when combined with a model-based robot skill optimizer. This approach facilitates the optimization of robot skill parameters for the current environment, without the need for real-world executions during optimization. Designed for compatibility with any representation of robot skills, MuTT demonstrates its versatility across three comprehensive experiments, showcasing superior performance across two different skill representations."
Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning,"Saito, Namiko; Moura, Joao; Uchida, Hiroki; Vijayakumar, Sethu",https://arxiv.org/abs/2403.10689,"Recognising the characteristics of objects while a robot handles them is crucial for adjusting motions that ensure stable and efficient interactions with containers. Ahead of realising stable and efficient robot motions for handling/transferring the containers, this work aims to recognise the latent unobservable object characteristics. While vision is commonly used for object recognition by robots, it is ineffective for detecting hidden objects. However, recognising objects indirectly using other sensors is a challenging task. To address this challenge, we propose a cross-modal transfer learning approach from vision to haptic-audio. We initially train the model with vision, directly observing the target object. Subsequently, we transfer the latent space learned from vision to a second module, trained only with haptic-audio and motor data. This transfer learning framework facilitates the representation of object characteristics using indirect sensor data, thereby improving recognition accuracy. For evaluating the recognition accuracy of our proposed learning framework we selected shape, position, and orientation as the object characteristics. Finally, we demonstrate online recognition of both trained and untrained objects using the humanoid robot Nextage Open."
Fast Global Point Cloud Registration using Semantic NDT,"Schirmer, Robert; Vaskevicius, Narunas; Biber, Peter; Stachniss, Cyrill",,
Miscommunication between robots can improve group accuracy in best-of-n decision-making,"Zakir, Raina; Dorigo, Marco; Reina, Andreagiovanni",,
A Perceptive Pneumatic Artificial Muscle Empowered by Double Helix Fiber Reinforcement,"Wang, Yufeng; wu, houping; Li, Chenchen; Peng, Yu Lian; Wang, Hongbo",,
Learning to Imitate Spatial Organization in Multi-robot Systems,"Agunloye, Ayomide Oluwaseyi; Ramchurn, Sarvapali; Soorati, Mohammad D.",https://arxiv.org/abs/2407.11592,"Understanding collective behavior and how it evolves is important to ensure that robot swarms can be trusted in a shared environment. One way to understand the behavior of the swarm is through collective behavior reconstruction using prior demonstrations. Existing approaches often require access to the swarm controller which may not be available. We reconstruct collective behaviors in distinct swarm scenarios involving shared environments without using swarm controller information. We achieve this by transforming prior demonstrations into features that describe multi-agent interactions before behavior reconstruction with multi-agent generative adversarial imitation learning (MA-GAIL). We show that our approach outperforms existing algorithms in spatial organization, and can be used to observe and reconstruct a swarm's behavior for further analysis and testing, which might be impractical or undesirable on the original robot swarm."
An Online Automatic Calibration Method for Infrastructure-Based LiDAR-Camera via Cross-modal Object Matching,"wang, tao; He, Yuesheng; Zhuang, Hanyang; Yang, Ming",,
MPGNet: Learning Move-Push-Grasping Synergy for Target-Oriented Grasping in Occluded Scenes,"Li, Dayou; Zhao, Chenkun; Yang, Shuo; Song, Ran; Li, Xiaolei; Zhang, Wei",https://arxiv.org/abs/2408.10525,"This paper focuses on target-oriented grasping in occluded scenes, where the target object is specified by a binary mask and the goal is to grasp the target object with as few robotic manipulations as possible. Most existing methods rely on a push-grasping synergy to complete this task. To deliver a more powerful target-oriented grasping pipeline, we present MPGNet, a three-branch network for learning a synergy between moving, pushing, and grasping actions. We also propose a multi-stage training strategy to train the MPGNet which contains three policy networks corresponding to the three actions. The effectiveness of our method is demonstrated via both simulated and real-world experiments."
Subtle-Diff: A Dataset for Precise Recognition of Subtle Differences Among Visually Similar Objects,"Matsuzawa, Fumiya; QIU, YUE; Sun, Yanjun; Iwata, Kenji; Kataoka, Hirokatsu; Satoh, Yutaka",,
A Context-Enhanced Full-Resolution Floor Plan Segmentation Network for Topological Semantic Mapping,"Cao, Zhengcai; Sun, Yiyang; Ma, Zhe; Zhou, MengChu",,
DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories,"El Hariry, Matteo; Richard, Antoine; Muralidharan, Vivek; Geist, Matthieu; Olivares-Mendez, Miguel A.",https://arxiv.org/abs/2310.04266,"This investigation introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments. Floating platforms serve as versatile test-beds to emulate microgravity environments on Earth. Our approach addresses the system and environmental uncertainties in controlling such platforms by training policies capable of precise maneuvers amid dynamic and unpredictable conditions. Leveraging state-of-the-art deep reinforcement learning techniques, our suite achieves robustness, adaptability, and good transferability from simulation to reality. Our Deep Reinforcement Learning (DRL) framework provides advantages such as fast training times, large-scale testing capabilities, rich visualization options, and ROS bindings for integration with real-world robotic systems. Beyond policy development, our suite provides a comprehensive platform for researchers, offering open-access at https://github.com/elharirymatteo/RANS/tree/ICRA24."
SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications,"Guesmi, Amira; Hanif, Muhammad Abdullah; Alouani, Ihsen; OUNI, Bassem; Shafique, Muhammad",https://arxiv.org/abs/2403.11515,"Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective. Notably, our patch is shape-sensitive, meaning it considers the specific shape and scale of the target object, thereby extending its influence beyond immediate proximity. Furthermore, our patch is trained to effectively address different scales and distances from the camera. Experimental results demonstrate that our approach induces a mean depth estimation error surpassing 0.5, impacting up to 99% of the targeted region for CNN-based MDE models. Additionally, we investigate the vulnerability of Transformer-based MDE models to patch-based attacks, revealing that SSAP yields a significant error of 0.59 and exerts substantial influence over 99% of the target region on these models."
MapLocNet: Coarse-to-Fine Visual Neural Re-Localization in Navigation Maps,"Wu, Hang; Zhang, Zhenghao; Lin, Siyuan; Mu, Xiangru; Zhao, Qiang; Yang, Ming; Qin, Tong",,
Cross-Modal Visual Relocalization in Prior LiDAR Maps Utilizing Intensity Textures,"Shen, Qiyuan; Zhao, Hengwang; Yan, Weihao; Qin, Tong; Yang, Ming",,
Learning a Pre-Grasp Manipulation Policy to Effectively Retrieve a Target in Dense Clutter,"Kiatos, Marios; Koutras, Leonidas; Sarantopoulos, Iason; Doulgeri, Zoe",,
"ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning","Li, Changze; Ji, Ziheng; Qin, Tong; Yang, Ming",https://arxiv.org/abs/2408.02061,"Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conducted extensive experiments in real-world scenarios, and the results demonstrate that the proposed method achieved an average parking success rate of 87.8% across four different real-world garages. Real-vehicle experiments further validate the feasibility and effectiveness of the method proposed in this paper."
Safe CoR: A Dual-Expert Approach to Integrating Imitation Learning and Safe Reinforcement Learning through Constraint Reward,"Kwon, Hyeokjin; Lee, Gunmin; Lee, Junseo; Oh, Songhwai",,
Geolocation on Cartographic Maps with Multi-Modal Fusion,"Zhou, Mengjie; Liu, Liu; Zhong, Yiran; Calway, Andrew",https://arxiv.org/abs/2308.05993,"We study the image-based geolocalization problem, aiming to localize ground-view query images on cartographic maps. Current methods often utilize cross-view localization techniques to match ground-view query images with 2D maps. However, the performance of these methods is unsatisfactory due to significant cross-view appearance differences. In this paper, we lift cross-view matching to a 2.5D space, where heights of structures (e.g., trees and buildings) provide geometric information to guide the cross-view matching. We propose a new approach to learning representative embeddings from multi-modal data. Specifically, we establish a projection relationship between 2.5D space and 2D aerial-view space. The projection is further used to combine multi-modal features from the 2.5D and 2D maps using an effective pixel-to-point fusion method. By encoding crucial geometric cues, our method learns discriminative location embeddings for matching panoramic images and maps. Additionally, we construct the first large-scale ground-to-2.5D map geolocalization dataset to validate our method and facilitate future research. Both single-image based and route based localization experiments are conducted to test our method. Extensive experiments demonstrate that the proposed method achieves significantly higher localization accuracy and faster convergence than previous 2D map-based approaches."
Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR Semantic Segmentation,"Lee, Seungho; Lee, Hwijeong; Shim, Hyunjung",,
Retargeting Human Facial Expression to Human-like Robotic Face through Neural Network Surrogate-based Optimization,"Wu, Bowen; Liu, Chaoran; Ishi, Carlos Toshinori; Minato, Takashi; Ishiguro, Hiroshi",,
Fingertip Tactile Sensor for Detecting Rope Slip,"Koga, Takayuki; Sato, Junya; Daigo, Takuya; Kimura, Kohei; Kudoh, Shunsuke",,
Differentiable Fluid Physics Parameter Identification By Stirring and For Stirring,"Xu, Wenqiang; Zheng, Dongzhe; Li, Yutong; Ren, Jieji; Lu, Cewu",https://arxiv.org/abs/2311.05137,"Fluid interactions permeate daily human activities, with properties like density and viscosity playing pivotal roles in household tasks. While density estimation is straightforward through Archimedes' principle, viscosity poses a more intricate challenge, especially given the varied behaviors of Newtonian and non-Newtonian fluids. These fluids, which differ in their stress-strain relationships, are delineated by specific constitutive models such as the Carreau, Cross, and Herschel-Bulkley models, each possessing unique viscosity parameters. This study introduces a novel differentiable fitting framework, DiffStir, tailored to identify key physics parameters via the common daily operation of stirring. By employing a robotic arm for stirring and harnessing a differentiable Material Point Method (diffMPM)-based simulator, the framework can determine fluid parameters by matching observations from both the simulator and the real world. Recognizing the distinct preferences of the aforementioned constitutive models for specific fluids, an online strategy was adopted to adaptively select the most fitting model based on real-world data. Additionally, we propose a refining neural network to bridge the sim-to-real gap and mitigate sensor noise-induced inaccuracies. Comprehensive experiments were conducted to validate the efficacy of DiffStir, showcasing its precision in parameter estimation when benchmarked against reported literature values. More experiments and videos can be found in the supplementary materials and on the website: https://sites.google.com/view/diffstir."
D-MARL: A Dynamic Communication-Based Action Space Enhancement for Multi Agent Reinforcement Learning Exploration of Large Scale Unknown Environments,"Calzolari, Gabriele; Sumathy, Vidya; Kanellakis, Christoforos; Nikolakopoulos, George",,
MOSFormer: A Transformer-based Multi-Modal Fusion Network for Moving Object Segmentation,"Cheng, Zike; Zhao, Hengwang; Shen, Qiyuan; Yan, Weihao; Wang, Chunxiang; Yang, Ming",,
AirCrab: A Hybrid Aerial-Ground Manipulator with An Active Wheel,"Cao, Muqing; Zhao, Jiayan; Xu, Xinhang; Xie, Lihua",https://arxiv.org/abs/2403.15805,"Inspired by the behavior of birds, we present AirCrab, a hybrid aerial ground manipulator (HAGM) with a single active wheel and a 3-degree of freedom (3-DoF) manipulator. AirCrab leverages a single point of contact with the ground to reduce position drift and improve manipulation accuracy. The single active wheel enables locomotion on narrow surfaces without adding significant weight to the robot. To realize accurate attitude maintenance using propellers on the ground, we design a control allocation method for AirCrab that prioritizes attitude control and dynamically adjusts the thrust input to reduce energy consumption. Experiments verify the effectiveness of the proposed control method and the gain in manipulation accuracy with ground contact. A series of operations to complete the letters 'NTU' demonstrates the capability of the robot to perform challenging hybrid aerial-ground manipulation missions."
Generating Force Vectors from Projective Truncated Signed Distance Fields for Collision Avoidance and Haptic Feedback,"Bien, Seongjin; Naceri, Abdeldjallil; Figueredo, Luis; Haddadin, Sami",,
Exploratory Motion Guided Tactile Learning for Shape-Consistent Robotic Insertion,"Yan, Gang; HE, Jinsong; Funabashi, Satoshi; Schmitz, Alexander; Sugano, Shigeki",,
Side-scan sonar based landmark detection for underwater vehicles,"Hoff, Simon Andreas; Haraldstad, Vegard; Reitan Hogstad, Bjørnar; Varagnolo, Damiano",,
Large Language Models Powered Context-aware Motion Prediction,"Zheng, Xiaoji; Wu, Lixiu; Yan, Zhijie; Tang, Yuanrong; Zhao, Hao; Zhong, Chen; Chen, Bokui; Gong, Jiangtao",https://arxiv.org/abs/2403.11057,"Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts -- Transportation Context Map (TC-Map), accompanied by corresponding text prompts. Through this approach, we obtained rich traffic context information from the LLM. By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7\% LLM-augmented datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving. The source code is available at \url{https://github.com/AIR-DISCOVER/LLM-Augmented-MTR} and \url{https://aistudio.baidu.com/projectdetail/7809548}."
Patterned Structure Muscle : Arbitrary Shaped Wire-driven Artificial Muscle Utilizing Anisotropic Flexible Structure for Musculoskeletal Robots,"Yoshimura, Shunnosuke; Miki, Akihiro; Miyama, Kazuhiro; Sahara, Yuta; Kawaharazuka, Kento; Okada, Kei; Inaba, Masayuki",,
Learning Social Cost Functions for Human-Aware Path Planning,"Eirale, Andrea; Leonetti, Matteo; Chiaberge, Marcello",https://arxiv.org/abs/2407.10547,"Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner's cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion."
LLaKey: Follow My Basic Action Instructions to Your Next Key State,"Zhao, Zheyi; He, Ying; Yu, Fei; Li, Pengteng; Zhuo, Fan; Sun, Xilong",,
Learning from Visual Demonstrations through Differentiable Nonlinear MPC for Personalized Autonomous Driving,"Acerbo, Flavia Sofia; Swevers, Jan; Tuytelaars, Tinne; Tong, Son",,
Kosmos-E: Learning to Follow Instruction for Robotic Grasping,"Wang, Zhi; Wu, Xun; Wu, Xun; Dong, Li; Wenhui, Wang; Ma, Shuming; Wei, Furu",,
ASML-VDIO: Visual-Depth-Inertial Odometry using Selected Multi-Modal Landmarks in Structural Environments,"Luo, Xingjian; Pang, Chenglin; Wu, Xuankang; Fang, Zheng",,
Probabilistic Inference of Human Capabilities from Passive Observations,"Tisnikar, Peter; Canal, Gerard; Leonetti, Matteo",,
Combining Ontological Knowledge and Large Language Model for User-Friendly Service Robots,"Nakajima, Haru; Miura, Jun",,
Real-time terrain assessment and Bayesian-based path planning for off-road navigation,"Niu, Tianwei; Shuwei, Yu; Wang, Liang; Yuan, Haoyu; Wang, Shoukun; Wang, Junzheng",,
CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking,"Baumann, Nicolas; Baumgartner, Michael; Ghignone, Edoardo; Kühne, Jonas; Fischer, Tobias; Yang, Yung-Hsu; Pollefeys, Marc; Magno, Michele",https://arxiv.org/abs/2403.15313,"To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor. Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications. The code is available at: https://github.com/ETH-PBL/CR3DT."
SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network,"Wan, Yuxuan; zhou, kaichen; Chen, Jinhong; Dong, Hao",https://arxiv.org/abs/2403.18195,"Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, determining their correctness in manual images and providing corrections when necessary. Finally, we utilize SCANet to correct the assembly results of MEPNet. Experimental results demonstrate that SCANet can identify and correct MEPNet's misassembled results, significantly improving the correctness of assembly. Our code and dataset are available at https://github.com/Yaser-wyx/SCANet."
ParametricNet+: A 6DoF Pose Estimation Network with Sparse Keypoint Recovery for Parametric Shapes in Stacked Scenarios,"Xie, Yihan; Lv, Weijie; Zhang, Xinyu; Chen, YiHong; Zeng, Long",,
ProSIP: Probabilistic Surface Interaction Primitives for Learning of Robotic Cleaning of Edges,"Unger, Christoph; Hartl-Nesic, Christian; Vu, Minh Nhat; Kugi, Andreas",,
6D Variable Virtual Fixtures for Telemanipulated Insertion Tasks,"Schwarz, Stephan Andreas; Thomas, Ulrike",,
PolyFit: A Peg-in-hole Assembly Framework for Unseen Polygon Shapes via Sim-to-real Adaptation,"Lee, Geonhyup; Lee, Joosoon; Noh, Sangjun; Ko, Minhwan; Kim, Kangmin; Lee, Kyoobin",https://arxiv.org/abs/2312.02531,"The study addresses the foundational and challenging task of peg-in-hole assembly in robotics, where misalignments caused by sensor inaccuracies and mechanical errors often result in insertion failures or jamming. This research introduces PolyFit, representing a paradigm shift by transitioning from a reinforcement learning approach to a supervised learning methodology. PolyFit is a Force/Torque (F/T)-based supervised learning framework designed for 5-DoF peg-in-hole assembly. It utilizes F/T data for accurate extrinsic pose estimation and adjusts the peg pose to rectify misalignments. Extensive training in a simulated environment involves a dataset encompassing a diverse range of peg-hole shapes, extrinsic poses, and their corresponding contact F/T readings. To enhance extrinsic pose estimation, a multi-point contact strategy is integrated into the model input, recognizing that identical F/T readings can indicate different poses. The study proposes a sim-to-real adaptation method for real-world application, using a sim-real paired dataset to enable effective generalization to complex and unseen polygon shapes. PolyFit achieves impressive peg-in-hole success rates of 97.3% and 96.3% for seen and unseen shapes in simulations, respectively. Real-world evaluations further demonstrate substantial success rates of 86.7% and 85.0%, highlighting the robustness and adaptability of the proposed method."
Mathematical characterization of the convergence domain for Direct Visual Servoing,"Naamani, Meriem Belinda; Caron, Guillaume; Morisawa, Mitsuharu; Mouaddib, El Mustapha",,
Transparency evaluation for the Kinematic Design of the Harnesses through Human-Exoskeleton Interaction Modeling,"Bezzini, Riccardo; Avizzano, Carlo Alberto; Porcini, Francesco; Filippeschi, Alessandro",,
Theoretical Modeling and Bio-inspired Trajectory Optimization of A Multiple-locomotion Origami Robot,"Zhu, Keqi; Guo, Haotian; Yu, Wei; Sirag, Hassen Nigatu; Li, Tong; Dong, Huixu",https://arxiv.org/abs/2403.12471,"Recent research on mobile robots has focused on increasing their adaptability to unpredictable and unstructured environments using soft materials and structures. However, the determination of key design parameters and control over these compliant robots are predominantly iterated through experiments, lacking a solid theoretical foundation. To improve their efficiency, this paper aims to provide mathematics modeling over two locomotion, crawling and swimming. Specifically, a dynamic model is first devised to reveal the influence of the contact surfaces' frictional coefficients on displacements in different motion phases. Besides, a swimming kinematics model is provided using coordinate transformation, based on which, we further develop an algorithm that systematically plans human-like swimming gaits, with maximum thrust obtained. The proposed algorithm is highly generalizable and has the potential to be applied in other soft robots with multiple joints. Simulation experiments have been conducted to illustrate the effectiveness of the proposed modeling."
A framework for Reproducible Benchmarking and Performance Diagnosis of SLAM Systems,"Radulov, Nikola; Zhang, Yuhao; Bujanca, Mihai; Ye, Ruiqi; Luján, Mikel",,
Toward Micro Eye Movement Detection in Practice: Stand-alone Eye Tracker with High Resolution and Wide Measurement Range,"Yokoyama, Keiko; Sueishi, Tomohiro; Inoue, Michiaki; Jin, YingJie; Hosoi, Toshinori; Ishikawa, Masatoshi",,
"M3-GMN: A Multi-environment, Multi-LiDAR, Multi-task dataset for Grid Map based Navigation","Xie, Guanglei; Fu, Hao; Xue, Hanzhang; Liu, Bokai; Xu, Xin; Li, Xiaohui; Sun, Zhenping",,
LF-3PM: a LiDAR-based Framework for Perception-aware Planning with Perturbation-induced Metric,"Chai, Kaixin; Xu, Long; Wang, Qianhao; Xu, Chao; Gao, Fei",https://arxiv.org/abs/2408.01649,"Just as humans can become disoriented in featureless deserts or thick fogs, not all environments are conducive to the Localization Accuracy and Stability (LAS) of autonomous robots. This paper introduces an efficient framework designed to enhance LiDAR-based LAS through strategic trajectory generation, known as Perception-aware Planning. Unlike vision-based frameworks, the LiDAR-based requires different considerations due to unique sensor attributes. Our approach focuses on two main aspects: firstly, assessing the impact of LiDAR observations on LAS. We introduce a perturbation-induced metric to provide a comprehensive and reliable evaluation of LiDAR observations. Secondly, we aim to improve motion planning efficiency. By creating a Static Observation Loss Map (SOLM) as an intermediary, we logically separate the time-intensive evaluation and motion planning phases, significantly boosting the planning process. In the experimental section, we demonstrate the effectiveness of the proposed metrics across various scenes and the feature of trajectories guided by different metrics. Ultimately, our framework is tested in a real-world scenario, enabling the robot to actively choose topologies and orientations preferable for localization. The source code is accessible at https://github.com/ZJU-FAST-Lab/LF-3PM."
Visuo-Tactile Zero-Shot Object Recognition with Vision-Language Model,"Ueda, Shiori; Hashimoto, Atsushi; Hamaya, Masashi; Tanaka, Kazutoshi; Saito, Hideo",,
HSS-SLAM: Human-in-the-Loop Semantic SLAM Represented by Superquadrics,"Li, Yulong; Zhang, Yunzhou; Zhao, Bin; Zhang, Zhiyao; Shen, You; Zhang, Tengda; Chen, Guolu",,
A Robotic-centric Paradigm for 3D Human Tracking Under Complex Environments Using Multi-modal Adaptation,"Xin, Shuo; Zhang, Zhen; Liu, Liang; Hou, Xiaojun; Zhu, Deye; Wang, Mengmeng; Liu, Yong",,
Rethinking 3D Geometric Object Features for Enhancing Skeleton-based Action Recognition,"Wu, Yuankai; Wang, Chi; Salihu, Driton; Patsch, Constantin; Zakour, Marsil; Steinbach, Eckehard",,
MFCalib: Single-shot and Automatic Extrinsic Calibration for LiDAR and Camera in Targetless Environments Based on Multi-Feature Edge,"Ye, Tianyong; Xu, Wei; Cui, Yukang",https://arxiv.org/abs/2409.00992,"This paper presents MFCalib, an innovative extrinsic calibration technique for LiDAR and RGB camera that operates automatically in targetless environments with a single data capture. At the heart of this method is using a rich set of edge information, significantly enhancing calibration accuracy and robustness. Specifically, we extract both depth-continuous and depth-discontinuous edges, along with intensity-discontinuous edges on planes. This comprehensive edge extraction strategy ensures our ability to achieve accurate calibration with just one round of data collection, even in complex and varied settings. Addressing the uncertainty of depth-discontinuous edges, we delve into the physical measurement principles of LiDAR and develop a beam model, effectively mitigating the issue of edge inflation caused by the LiDAR beam. Extensive experiment results demonstrate that MFCalib outperforms the state-of-the-art targetless calibration methods across various scenes, achieving and often surpassing the precision of multi-scene calibrations in a single-shot collection. To support community development, we make our code available open-source on GitHub."
FF-SRL: High Performance GPU-Based Surgical Simulation For Robot Learning,"Dall'Alba, Diego; Naskr&#281;t, Micha&#322;; Korzeniowski, Przemyslaw; Kami&#324;ska, Sabina",,
A Fast Motion and Foothold Planning Framework for Legged Robot on Discrete Terrain,"Yu, Jiyu; Wang, Dongqi; Chen, Zhenghan; Chen, Ci; Wu, Shuangpeng; Wang, Yue; Xiong, Rong",,
Accurate power consumption estimation method makes walking robots energy efficient and quiet,"Valsecchi, Giorgio; Vicari, Andrea; Tischhauser, Fabian; Garabini, Manolo; Hutter, Marco",,
DIABLO: A 6-DoF Wheeled Bipedal Robot Composed Entirely of Direct-Drive Joints,"Liu, Dingchuan; Fangfang, Yang; LIAO, Xuanhong; Lyu, Ximin",https://arxiv.org/abs/2407.21500,"Wheeled bipedal robots offer the advantages of both wheeled and legged robots, combining the ability to traverse a wide range of terrains and environments with high efficiency. However, the conventional approach in existing wheeled bipedal robots involves motor-driven joints with high-ratio gearboxes. While this approach provides specific benefits, it also presents several challenges, including increased mechanical complexity, efficiency losses, noise, vibrations, and higher maintenance and lubrication requirements. Addressing the aforementioned concerns, we developed a direct-drive wheeled bipedal robot called DIABLO, which eliminates the use of gearboxes entirely. Our robotic system is simplified as a second-order inverted pendulum, and we have designed an LQR-based balance controller to ensure stability. Additionally, we implemented comprehensive motion controller, including yaw, split-angle, height, and roll controllers. Through expriments in simulations and real-world prototype, we have demonstrated that our platform achieves satisfactory performance."
U-BEV: Height-aware Bird's-Eye-View Segmentation and Neural Map-based Relocalization,"Boscolo Camiletto, Andrea; Bochicchio, Alfredo; Liniger, Alexander; Dai, Dengxin; Gawel, Abel Roman",https://arxiv.org/abs/2310.13766,"Efficient relocalization is essential for intelligent vehicles when GPS reception is insufficient or sensor-based localization fails. Recent advances in Bird's-Eye-View (BEV) segmentation allow for accurate estimation of local scene appearance and in turn, can benefit the relocalization of the vehicle. However, one downside of BEV methods is the heavy computation required to leverage the geometric constraints. This paper presents U-BEV, a U-Net inspired architecture that extends the current state-of-the-art by allowing the BEV to reason about the scene on multiple height layers before flattening the BEV features. We show that this extension boosts the performance of the U-BEV by up to 4.11 IoU. Additionally, we combine the encoded neural BEV with a differentiable template matcher to perform relocalization on neural SD-map data. The model is fully end-to-end trainable and outperforms transformer-based BEV methods of similar computational complexity by 1.7 to 2.8 mIoU and BEV-based relocalization by over 26% Recall Accuracy on the nuScenes dataset."
QO-Net: Query Optimization Underwater Object Detection Network,"Sun, Hongyang; fan, baojie; Xia, Caixia; Xu, Hongxin",,
Enhancing leg odometry in legged robots with learned contact bias: an LSTM recurrent neural network approach,"Gu, Yaru; Liu, Ze; Zou, Ting",,
Motion Primitives Planning For Center-Articulated Vehicles,"Hu, Jiangpeng; Yang, Fan; Nan, Fang; Hutter, Marco",https://arxiv.org/abs/2405.17127,"Autonomous navigation across unstructured terrains, including forests and construction areas, faces unique challenges due to intricate obstacles and the element of the unknown. Lacking pre-existing maps, these scenarios necessitate a motion planning approach that combines agility with efficiency. Critically, it must also incorporate the robot's kinematic constraints to navigate more effectively through complex environments. This work introduces a novel planning method for center-articulated vehicles (CAV), leveraging motion primitives within a receding horizon planning framework using onboard sensing. The approach commences with the offline creation of motion primitives, generated through forward simulations that reflect the distinct kinematic model of center-articulated vehicles. These primitives undergo evaluation through a heuristic-based scoring function, facilitating the selection of the most suitable path for real-time navigation. To augment this planning process, we develop a pose-stabilizing controller, tailored to the kinematic specifications of center-articulated vehicles. During experiments, our method demonstrates a $67\%$ improvement in SPL (Success Rate weighted by Path Length) performance over existing strategies. Furthermore, its efficacy was validated through real-world experiments conducted with a tree harvester vehicle - SAHA."
Under-actuated Robotic Gripper with Multiple Grasping Modes Inspired by Human Finger,"Li, Jihao; Liao, Tingbo; Sirag, Hassen Nigatu; Guo, Haotian; Lu, GuoDong; Dong, Huixu",https://arxiv.org/abs/2403.12502,"Under-actuated robot grippers as a pervasive tool of robots have become a considerable research focus. Despite their simplicity of mechanical design and control strategy, they suffer from poor versatility and weak adaptability, making widespread applications limited. To better relieve relevant research gaps, we present a novel 3-finger linkage-based gripper that realizes retractable and reconfigurable multi-mode grasps driven by a single motor. Firstly, inspired by the changes that occurred in the contact surface with a human finger moving, we artfully design a slider-slide rail mechanism as the phalanx to achieve retraction of each finger, allowing for better performance in the enveloping grasping mode. Secondly, a reconfigurable structure is constructed to broaden the grasping range of objects' dimensions for the proposed gripper. By adjusting the configuration and gesture of each finger, the gripper can achieve five grasping modes. Thirdly, the proposed gripper is just actuated by a single motor, yet it can be capable of grasping and reconfiguring simultaneously. Finally, various experiments on grasps of slender, thin, and large-volume objects are implemented to evaluate the performance of the proposed gripper in practical scenarios, which demonstrates the excellent grasping capabilities of the gripper."
Development of a peristaltic flexible transfer system for&#12288;transporting feces under microgravity: Construction and validation of transport models,"Kawano, Masaki; Uzawa, Shogo; Yamazaki, Chiaki; Nakamura, Taro",,
Error-State Kalman Filter based Visual-Inertial Odometry Using Orientation Measurement on Unit Quaternion Group,"Chang, Chao-Wei; Lian, Feng-Li",,
Hardware-Based Time Synchronization for a Multi-Sensor System,"Wang, Yueqi; Liu, Tangyou; FENG, LICHENG; Wang, Jinze; Yang, Yang; Bao, Jianjun; Li, Binghao; Wu, Liao",https://arxiv.org/abs/1912.02469,"Robust and accurate pose estimation is crucial for many applications in mobile robotics. Extending visual Simultaneous Localization and Mapping (SLAM) with other modalities such as an inertial measurement unit (IMU) can boost robustness and accuracy. However, for a tight sensor fusion, accurate time synchronization of the sensors is often crucial. Changing exposure times, internal sensor filtering, multiple clock sources and unpredictable delays from operation system scheduling and data transfer can make sensor synchronization challenging. In this paper, we present VersaVIS, an Open Versatile Multi-Camera Visual-Inertial Sensor Suite aimed to be an efficient research platform for easy deployment, integration and extension for many mobile robotic applications. VersaVIS provides a complete, open-source hardware, firmware and software bundle to perform time synchronization of multiple cameras with an IMU featuring exposure compensation, host clock translation and independent and stereo camera triggering. The sensor suite supports a wide range of cameras and IMUs to match the requirements of the application. The synchronization accuracy of the framework is evaluated on multiple experiments achieving timing accuracy of less than 1 ms. Furthermore, the applicability and versatility of the sensor suite is demonstrated in multiple applications including visual-inertial SLAM, multi-camera applications, multimodal mapping, reconstruction and object based mapping."
A Large Vision-Language Model based Environment Perception System for Visually Impaired People,"Chen, Zezhou; Liu, Zhaoxiang; Wang, Kai; wang, kohou; Lian, Shiguo",,
Identification of Flexible Joint Robot Inertia Matrix Using Frequency Response Analysis,"Choi, Kiyoung; Song, JunHo; Yun, WonBum; Lee, Deokjin; Oh, Sehoon",,
Towards Robotised Palpation for Cancer Detection through Online Tissue Viscoelastic Characterisation with a Collaborative Robotic Arm,"Beber, Luca; Lamon, Edoardo; Moretti, Giacomo; Fontanelli, Daniele; Saveriano, Matteo; Palopoli, Luigi",https://arxiv.org/abs/2404.09542,"This paper introduces a new method for estimating the penetration of the end effector and the parameters of a soft body using a collaborative robotic arm. This is possible using the dimensionality reduction method that simplifies the Hunt-Crossley model. The parameters can be found without a force sensor thanks to the information of the robotic arm controller. To achieve an online estimation, an extended Kalman filter is employed, which embeds the contact dynamic model. The algorithm is tested with various types of silicone, including samples with hard intrusions to simulate cancerous cells within a soft tissue. The results indicate that this technique can accurately determine the parameters and estimate the penetration of the end effector into the soft body. These promising preliminary results demonstrate the potential for robots to serve as an effective tool for early-stage cancer diagnostics."
Trajectory Optimization with Global Yaw Parameterization for Field-of-View Constrained Autonomous Flight,"Wu, Yuwei; Tao, Yuezhan; Spasojevic, Igor; Kumar, Vijay",https://arxiv.org/abs/2403.17067,"Trajectory generation for quadrotors with limited field-of-view sensors has numerous applications such as aerial exploration, coverage, inspection, videography, and target tracking. Most previous works simplify the task of optimizing yaw trajectories by either aligning the heading of the robot with its velocity, or potentially restricting the feasible space of candidate trajectories by using a limited yaw domain to circumvent angular singularities. In this paper, we propose a novel \textit{global} yaw parameterization method for trajectory optimization that allows a 360-degree yaw variation as demanded by the underlying algorithm. This approach effectively bypasses inherent singularities by including supplementary quadratic constraints and transforming the final decision variables into the desired state representation. This method significantly reduces the needed control effort, and improves optimization feasibility. Furthermore, we apply the method to several examples of different applications that require jointly optimizing over both the yaw and position trajectories. Ultimately, we present a comprehensive numerical analysis and evaluation of our proposed method in both simulation and real-world experiments."
Fractional Order Modeling and Control of Hydrogel-based Soft Pneumatic Bending Actuators,"de la Morena, Jesús; Redrejo López, David; Ramos, Francisco; Feliu, Vicente; Vazquez, Andres S.",,
Wirelessly Actuated Rotation-free Magnetic Motor,"Harman, Umur Ulas; Hafez, Ahmed; Duffield, Cameron; Zhao, Zihan; Dixon, Luke; Rus, Daniela; Miyashita, Shuhei",,
Active Learning for Forward/Inverse Kinematics of Redundantly-driven Flexible Tensegrity Manipulator,"Yoshimitsu, Yuhei; Osa, Takayuki; Ben Amor, Heni; Ikemoto, Shuhei",,
TDA-Track: Prompt-Driven Temporal Domain Adaptation for Nighttime UAV Tracking,"Fu, Changhong; Wang, Yiheng; Yao, Liangliang; Zheng, Guangze; Zuo, Haobo; Pan, Jia",,
Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for Multi-Robot Cooperation Tasks,"Feng, Pu; Liang, Junkang; Wang, Size; Yu, Xin; Shi, Rongye; Wu, Wenjun",https://arxiv.org/abs/2407.08164,"In multi-agent reinforcement learning (MARL), the Centralized Training with Decentralized Execution (CTDE) framework is pivotal but struggles due to a gap: global state guidance in training versus reliance on local observations in execution, lacking global signals. Inspired by human societal consensus mechanisms, we introduce the Hierarchical Consensus-based Multi-Agent Reinforcement Learning (HC-MARL) framework to address this limitation. HC-MARL employs contrastive learning to foster a global consensus among agents, enabling cooperative behavior without direct communication. This approach enables agents to form a global consensus from local observations, using it as an additional piece of information to guide collaborative actions during execution. To cater to the dynamic requirements of various tasks, consensus is divided into multiple layers, encompassing both short-term and long-term considerations. Short-term observations prompt the creation of an immediate, low-layer consensus, while long-term observations contribute to the formation of a strategic, high-layer consensus. This process is further refined through an adaptive attention mechanism that dynamically adjusts the influence of each consensus layer. This mechanism optimizes the balance between immediate reactions and strategic planning, tailoring it to the specific demands of the task at hand. Extensive experiments and real-world applications in multi-robot systems showcase our framework's superior performance, marking significant advancements over baselines."
Reinforcement Learning Control for Autonomous Hydraulic Material Handling Machines with Underactuated Tools,"Spinelli, Filippo Alberto; Egli, Pascal Arturo; Nubert, Julian; Nan, Fang; Bleumer, Thilo; Goegler, Patrick; Brockes, Stephan; Hofmann, Ferdinand; Hutter, Marco",,
RCAL:A Lightweight Road Cognition and Automated Labeling System for Autonomous Driving Scenarios,"Chen, Jiancheng; Yu, Chao; Wang, Huayou; Xue, Changliang; Zhan, Yifei; Liu, Kun",,
Progressive Representation Learning for Real-Time UAV Tracking,"Fu, Changhong; Lei, Xiang; Zuo, Haobo; Yao, Liangliang; Zheng, Guangze; Pan, Jia",https://arxiv.org/abs/2403.11095,"Motivated by agility, 3D mobility, and low-risk operation compared to human-operated management systems of autonomous unmanned aerial vehicles (UAVs), this work studies UAV-based active wildfire monitoring where a UAV detects fire incidents in remote areas and tracks the fire frontline. A UAV path planning solution is proposed considering realistic wildfire management missions, where a single low-altitude drone with limited power and flight time is available. Noting the limited field of view of commercial low-altitude UAVs, the problem formulates as a partially observable Markov decision process (POMDP), in which wildfire progression outside the field of view causes inaccurate state representation that prevents the UAV from finding the optimal path to track the fire front in limited time. Common deep reinforcement learning (DRL)-based trajectory planning solutions require diverse drone-recorded wildfire data to generalize pre-trained models to real-time systems, which is not currently available at a diverse and standard scale. To narrow down the gap caused by partial observability in the space of possible policies, a belief-based state representation with broad, extensive simulated data is proposed where the beliefs (i.e., ignition probabilities of different grid areas) are updated using a Bayesian framework for the cells within the field of view. The performance of the proposed solution in terms of the ratio of detected fire cells and monitored ignited area (MIA) is evaluated in a complex fire scenario with multiple rapidly growing fire batches, indicating that the belief state representation outperforms the observation state representation both in fire coverage and the distance to fire frontline."
Tube-GAN: A Novel Virtual Tube Generation Method for Unmanned Aerial Swarms Based on Generative Adversarial Network,"Zhai, Shixun; Zhang, Kaige; Nan, Bo; Sun, Yanwen; fu, qianyi",,
Robotic valve turning: misalignment estimation from reaction torques,"Golani, Gautami; Turlapati, Sri Harsha; Yang, Lin; Ariffin, Mohammad; Campolo, Domenico",,
Self-Supervised Depth Estimation Based on Camera Models,"Zhang, Jinchang; Lu, Guoyu",https://arxiv.org/abs/1903.01092,"In this work, we present a novel meta-learning algorithm, i.e. TTNet, that regresses model parameters for novel tasks for which no ground truth is available (zero-shot tasks). In order to adapt to novel zero-shot tasks, our meta-learner learns from the model parameters of known tasks (with ground truth) and the correlation of known tasks to zero-shot tasks. Such intuition finds its foothold in cognitive science, where a subject (human baby) can adapt to a novel-concept (depth understanding) by correlating it with old concepts (hand movement or self-motion), without receiving explicit supervision. We evaluated our model on the Taskonomy dataset, with four tasks as zero-shot: surface-normal, room layout, depth, and camera pose estimation. These tasks were chosen based on the data acquisition complexity and the complexity associated with the learning process using a deep network. Our proposed methodology out-performs state-of-the-art models (which use ground truth)on each of our zero-shot tasks, showing promise on zero-shot task transfer. We also conducted extensive experiments to study the various choices of our methodology, as well as showed how the proposed method can also be used in transfer learning. To the best of our knowledge, this is the firstsuch effort on zero-shot learning in the task space."
Online Planning for Multi Agent Path Finding in Inaccurate Maps,"Malka Nir, Nir; Shani, Guy; Stern, Roni",,
DaDiff: Domain-aware Diffusion Model for Nighttime UAV Tracking,"Zuo, Haobo; Fu, Changhong; Zheng, Guangze; Yao, Liangliang; Lu, Kunhan; Pan, Jia",,
Visual Servo Control of a Conceptual Magnetically Anchored and Guided Flexible Endoscope,"Li, Weibing; Yang, Yang; Pan, Yongping",,
3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards,"Qiu, Tian; Zoubi, Alan; Spine, Nikolai; Cheng, Lailiang; Jiang, Yu",https://arxiv.org/abs/2404.05953,"Robotic branch pruning is a significantly growing research area to cope with the shortage of labor force in the context of agriculture. One fundamental requirement in robotic pruning is the perception of detailed geometry and topology of branches. However, the point clouds obtained in agricultural settings often exhibit incompleteness due to several constraints, thereby restricting the accuracy of downstream robotic pruning. In this work, we addressed the issue of point cloud quality through a simulation-based deep neural network, leveraging a Real-to-Simulation (Real2Sim) data generation pipeline that not only eliminates the need for manual parameterization but also guarantees the realism of simulated data. The simulation-based neural network was applied to jointly perform point cloud completion and skeletonization on real-world partial branches, without additional real-world training. The Sim2Real qualitative completion and skeletonization results showed the model's remarkable capability for geometry reconstruction and topology prediction. Additionally, we quantitatively evaluated the Sim2Real performance by comparing branch-level trait characterization errors using raw incomplete data and complete data. The Mean Absolute Error (MAE) reduced by 75% and 8% for branch diameter and branch angle estimation, respectively, using the best complete data, which indicates the effectiveness of the Real2Sim data in a zero-shot generalization setting. The characterization improvements contributed to the precision and efficacy of robotic branch pruning."
Centroidal State Estimation based on the Koopman Embedding for Dynamic Legged Locomotion,"Khorshidi, Shahram; Elnagdi, Murad; Bennewitz, Maren",https://arxiv.org/abs/2403.13366,"In this paper, we introduce a novel approach to centroidal state estimation, which plays a crucial role in predictive model-based control strategies for dynamic legged locomotion. Our approach uses the Koopman operator theory to transform the robot's complex nonlinear dynamics into a linear system, by employing dynamic mode decomposition and deep learning for model construction. We evaluate both models on their linearization accuracy and capability to capture both fast and slow dynamic system responses. We then select the most suitable model for estimation purposes, and integrate it within a moving horizon estimator. This estimator is formulated as a convex quadratic program, to facilitate robust, real-time centroidal state estimation. Through extensive simulation experiments on a quadruped robot executing various dynamic gaits, our data-driven framework outperforms conventional filtering techniques based on nonlinear dynamics. Our estimator addresses challenges posed by force/torque measurement noise in highly dynamic motions and accurately recovers the centroidal states, demonstrating the adaptability and effectiveness of the Koopman-based linear representation for complex locomotive behaviors. Importantly, our model based on dynamic mode decomposition, trained with two locomotion patterns (trot and jump), successfully estimates the centroidal states for a different motion (bound) without retraining."
Conditional Generative Denoiser for Nighttime UAV Tracking,"Wang, Yucheng; Fu, Changhong; Lu, Kunhan; Yao, Liangliang; Zuo, Haobo",,
HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers,"Lenz, Christian; Menon, Rohit; Schreiber, Michael; Paul, Jacob, Melvin; Behnke, Sven; Bennewitz, Maren",https://arxiv.org/abs/2403.15306,"Horticultural tasks such as pruning and selective harvesting are labor intensive and horticultural staff are hard to find. Automating these tasks is challenging due to the semi-structured greenhouse workspaces, changing environmental conditions such as lighting, dense plant growth with many occlusions, and the need for gentle manipulation of non-rigid plant organs. In this work, we present the three-armed system HortiBot, with two arms for manipulation and a third arm as an articulated head for active perception using stereo cameras. Its perception system detects not only peppers, but also peduncles and stems in real time, and performs online data association to build a world model of pepper plants. Collision-aware online trajectory generation allows all three arms to safely track their respective targets for observation, grasping, and cutting. We integrated perception and manipulation to perform selective harvesting of peppers and evaluated the system in lab experiments. Using active perception coupled with end-effector force torque sensing for compliant manipulation, HortiBot achieves high success rates."
Bridging the Sim-to-Real Gap with Bayesian Inference,"Rothfuss, Jonas; Sukhija, Bhavya; Treven, Lenart; Dorfler, Florian; Coros, Stelian; Krause, Andreas",https://arxiv.org/abs/2403.16644,"We present SIM-FSVGD for learning robot dynamics from data. As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models. While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available. We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification. We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system. Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art."
PINN-Ray: A Physics-Informed Neural Network to Model Soft Robotic Fin-Ray Fingers,"Wang, Xing; Dabrowski, Joel Janek; Pinskier, Joshua; Liow, Lois; Viswanathan, VinothKumar; Scalzo, Richard; Howard, David",https://arxiv.org/abs/2407.08222,"Modelling complex deformation for soft robotics provides a guideline to understand their behaviour, leading to safe interaction with the environment. However, building a surrogate model with high accuracy and fast inference speed can be challenging for soft robotics due to the nonlinearity from complex geometry, large deformation, material nonlinearity etc. The reality gap from surrogate models also prevents their further deployment in the soft robotics domain. In this study, we proposed a physics-informed Neural Networks (PINNs) named PINN-Ray to model complex deformation for a Fin Ray soft robotic gripper, which embeds the minimum potential energy principle from elastic mechanics and additional high-fidelity experimental data into the loss function of neural network for training. This method is significant in terms of its generalisation to complex geometry and robust to data scarcity as compared to other data-driven neural networks. Furthermore, it has been extensively evaluated to model the deformation of the Fin Ray finger under external actuation. PINN-Ray demonstrates improved accuracy as compared with Finite element modelling (FEM) after applying the data assimilation scheme to treat the sim-to-real gap. Additionally, we introduced our automated framework to design, fabricate soft robotic fingers, and characterise their deformation by visual tracking, which provides a guideline for the fast prototype of soft robotics."
Manta Ray-Inspired Soft Robotic Swimmer for High-speed and Multi-modal Swimming,"Xu, Zefeng; Liang, jiaqiao; Zhou, Yitong",,
State Estimation of an Adaptive 3-Finger Gripper using Recurrent Neural Networks,"Jonetzko, Yannick; Naß, Theresa Alexandra Aurelia; Fiedler, Niklas; Zhang, Jianwei",,
DESectBot Design and Validation of a Novel Two-Segment Decoupled Continuum Robotic System for Endoscopic Submucosal Dissection,"liu, wenjie; Shao, Yuancheng; Zhang, Yao; Chen, Zixi; Wu, Di; Chen, Yuqiao; Stefanini, Cesare; Ling, Li; Qi, Peng",,
Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models,"Wang, Tianyu; Lin, Haitao; Yu, Junqiu; Fu, Yanwei",https://arxiv.org/abs/2408.07975,"This paper investigates the task of the open-ended interactive robotic manipulation on table-top scenarios. While recent Large Language Models (LLMs) enhance robots' comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment. This is because the robot needs to locate the target object for manipulation within the physical workspace. To this end, we introduce an interactive robotic manipulation framework called Polaris, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models. For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image. Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks. The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories. Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks. This indicates its potential to generalize to scenarios beyond the tabletop. More information and video results are available here: https://star-uu-wang.github.io/Polaris/"
Depth Completion using Galerkin Attention,"Xu, Yinuo; Zhang, Xuesong",,
DVT: Decoupled Dual-Branch View Transformation for Monocular Bird's Eye View Semantic Segmentation,"Du, Jiayuan; Pan, Xianghui; Shen, Mengjiao; Su, Shuai; Yang, Jingwei; Liu, Chengju; Chen, Qijun",,
Momentum-Aware Trajectory Optimisation using Full-Centroidal Dynamics and Implicit Inverse Kinematics,"PAPATHEODOROU, ARISTOTELIS; Merkt, Wolfgang Xaver; Mitchell, Alexander Luis; Havoutis, Ioannis",https://arxiv.org/abs/2310.06074,"The current state-of-the-art gradient-based optimisation frameworks are able to produce impressive dynamic manoeuvres such as linear and rotational jumps. However, these methods, which optimise over the full rigid-body dynamics of the robot, often require precise foothold locations apriori, while real-time performance is not guaranteed without elaborate regularisation and tuning of the cost function. In contrast, we investigate the advantages of a task-space optimisation framework, with special focus on acrobatic motions. Our proposed formulation exploits the system's high-order nonlinearities, such as the nonholonomy of the angular momentum, in order to produce feasible, high-acceleration manoeuvres. By leveraging the full-centroidal dynamics of the quadruped ANYmal C and directly optimising its footholds and contact forces, the framework is capable of producing efficient motion plans with low computational overhead. Finally, we deploy our proposed framework on the ANYmal C platform, and demonstrate its true capabilities through real-world experiments, with the successful execution of high-acceleration motions, such as linear and rotational jumps. Extensive analysis of these shows that the robot's dynamics can be exploited to surpass its hardware limitations of having a high mass and low-torque limits."
Large Language Model-based Task and Motion Planning with Motion Failure Reasoning,"Wang, Shu; Han, Muzhi; Jiao, Ziyuan; Zhang, Zeyu; Wu, Ying Nian; Zhu, Song-Chun; Liu, Hangxin",https://arxiv.org/abs/2307.14535,"We present a framework for robot skill acquisition, which 1) efficiently scale up data generation of language-labelled robot data and 2) effectively distills this data down into a robust multi-task language-conditioned visuo-motor policy. For (1), we use a large language model (LLM) to guide high-level planning, and sampling-based robot planners (e.g. motion or grasp samplers) for generating diverse and rich manipulation trajectories. To robustify this data-collection process, the LLM also infers a code-snippet for the success condition of each task, simultaneously enabling the data-collection process to detect failure and retry as well as the automatic labeling of trajectories with success/failure. For (2), we extend the diffusion policy single-task behavior-cloning approach to multi-task settings with language conditioning. Finally, we propose a new multi-task benchmark with 18 tasks across five domains to test long-horizon behavior, common-sense reasoning, tool-use, and intuitive physics. We find that our distilled policy successfully learned the robust retrying behavior in its data collection procedure, while improving absolute success rates by 33.2% on average across five domains. Code, data, and additional qualitative results are available on https://www.cs.columbia.edu/~huy/scalingup/."
Enhancing 3D Single Object Tracking with Efficient Point Cloud Segmentation,"Yang, Yu Shi; fan, baojie; Jiang, yuyu; zhou, wuyang; Xu, Hongxin",,
High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization,"Sun, Shuo; Mielle, Malcolm; Lilienthal, Achim J.; Magnusson, Martin",https://arxiv.org/abs/2403.12535,"We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction. To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas. Second, we introduce extra regularization parameters to alleviate the forgetting problem in the continuous mapping problem, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames. Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way. Compared to recent neural and concurrently developed gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM."
Vehicle Trajectory Prediction with Soft Behavior Constraints,"Ye, Ke; Zhou, Sanping; kang, miao; Fu, Jingwen; Zheng, Nanning",https://arxiv.org/abs/2404.10295,"The ability to accurately predict feasible multimodal future trajectories of surrounding traffic participants is crucial for behavior planning in autonomous vehicles. The Motion Transformer (MTR), a state-of-the-art motion prediction method, alleviated mode collapse and instability during training and enhanced overall prediction performance by replacing conventional dense future endpoints with a small set of fixed prior motion intention points. However, the fixed prior intention points make the MTR multi-modal prediction distribution over-scattered and infeasible in many scenarios. In this paper, we propose the ControlMTR framework to tackle the aforementioned issues by generating scene-compliant intention points and additionally predicting driving control commands, which are then converted into trajectories by a simple kinematic model with soft constraints. These control-generated trajectories will guide the directly predicted trajectories by an auxiliary loss function. Together with our proposed scene-compliant intention points, they can effectively restrict the prediction distribution within the road boundaries and suppress infeasible off-road predictions while enhancing prediction performance. Remarkably, without resorting to additional model ensemble techniques, our method surpasses the baseline MTR model across all performance metrics, achieving notable improvements of 5.22% in SoftmAP and a 4.15% reduction in MissRate. Our approach notably results in a 41.85% reduction in the cross-boundary rate of the MTR, effectively ensuring that the prediction distribution is confined within the drivable area."
EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization,"Yuan, Runze; Liu, Tao; Dai, Zijia; Zuo, Yi-Fan; Kneip, Laurent",https://arxiv.org/abs/2408.01370,"Event cameras are an interesting visual exteroceptive sensor that reacts to brightness changes rather than integrating absolute image intensities. Owing to this design, the sensor exhibits strong performance in situations of challenging dynamics and illumination conditions. While event-based simultaneous tracking and mapping remains a challenging problem, a number of recent works have pointed out the sensor's suitability for prior map-based tracking. By making use of cross-modal registration paradigms, the camera's ego-motion can be tracked across a large spectrum of illumination and dynamics conditions on top of accurate maps that have been created a priori by more traditional sensors. The present paper follows up on a recently introduced event-based geometric semi-dense tracking paradigm, and proposes the addition of inertial signals in order to robustify the estimation. More specifically, the added signals provide strong cues for pose initialization as well as regularization during windowed, multi-frame tracking. As a result, the proposed framework achieves increased performance under challenging illumination conditions as well as a reduction of the rate at which intermediate event representations need to be registered in order to maintain stable tracking across highly dynamic sequences. Our evaluation focuses on a diverse set of real world sequences and comprises a comparison of our proposed method against a purely event-based alternative running at different rates."
CTS: Sim-to-Real Unsupervised Domain Adaptation on 3D Detection,"zhang, meiying; Peng, Weiyuan; Ding, Guangyao; Lei, Chenyang; Ji, Chunlin; HAO, QI",https://arxiv.org/abs/2406.18129,"Simulation data can be accurately labeled and have been expected to improve the performance of data-driven algorithms, including object detection. However, due to the various domain inconsistencies from simulation to reality (sim-to-real), cross-domain object detection algorithms usually suffer from dramatic performance drops. While numerous unsupervised domain adaptation (UDA) methods have been developed to address cross-domain tasks between real-world datasets, progress in sim-to-real remains limited. This paper presents a novel Complex-to-Simple (CTS) framework to transfer models from labeled simulation (source) to unlabeled reality (target) domains. Based on a two-stage detector, the novelty of this work is threefold: 1) developing fixed-size anchor heads and RoI augmentation to address size bias and feature diversity between two domains, thereby improving the quality of pseudo-label; 2) developing a novel corner-format representation of aleatoric uncertainty (AU) for the bounding box, to uniformly quantify pseudo-label quality; 3) developing a noise-aware mean teacher domain adaptation method based on AU, as well as object-level and frame-level sampling strategies, to migrate the impact of noisy labels. Experimental results demonstrate that our proposed approach significantly enhances the sim-to-real domain adaptation capability of 3D object detection models, outperforming state-of-the-art cross-domain algorithms, which are usually developed for real-to-real UDA tasks."
DOB-based Wind Estimation of A UAV Using Its Onboard Sensor,"Yu, Haowen; Liang, Xianqi; Lyu, Ximin",https://arxiv.org/abs/2409.01549,"Unmanned Aerial Vehicles (UAVs) play a crucial role in meteorological research, particularly in environmental wind field measurements. However, several challenges exist in current wind measurement methods using UAVs that need to be addressed. Firstly, the accuracy of measurement is low, and the measurement range is limited. Secondly, the algorithms employed lack robustness and adaptability across different UAV platforms. Thirdly, there are limited approaches available for wind estimation during dynamic flight. Finally, while horizontal plane measurements are feasible, vertical direction estimation is often missing. To tackle these challenges, we present and implement a comprehensive wind estimation algorithm. Our algorithm offers several key features, including the capability to estimate the 3-D wind vector, enabling wind estimation even during dynamic flight of the UAV. Furthermore, our algorithm exhibits adaptability across various UAV platforms. Experimental results in the wind tunnel validate the effectiveness of our algorithm, showcasing improvements such as wind speed accuracy of $0.11$ m/s and wind direction errors of less than $2.8^\circ$. Additionally, our approach extends the measurement range to $10$ m/s."
RT-RRT: Reverse Tree Guided Real-Time Path Planning/Replanning in Unpredictable Dynamic Environments,"Cui, Bo; Cui, Rongxin; Yan, Weisheng; Wang, Y.K; zhang, shi",,
IDF-MFL: Infrastructure-free and Drift-free Magnetic Field Localization for Mobile Robot,"Shen, Hongming; Wu, Zhenyu; Wang, Wei; Lyu, Qiyang; Zhou, Huiqin; Wang, Danwei",,
SwarmPRM: Probabilistic Roadmap Motion Planning for Large-Scale Swarm Robotic Systems,"Hu, Yunze; Yang, Xuru; Zhou, Kangjie; Liu, Qinghang; Ding, Kang; Gao, Han; Zhu, Pingping; Liu, Chang",https://arxiv.org/abs/2402.16699,"Large-scale swarm robotic systems consisting of numerous cooperative agents show considerable promise for performing autonomous tasks across various sectors. Nonetheless, traditional motion planning approaches often face a trade-off between scalability and solution quality due to the exponential growth of the joint state space of robots. In response, this work proposes SwarmPRM, a hierarchical, scalable, computationally efficient, and risk-aware sampling-based motion planning approach for large-scale swarm robots. SwarmPRM utilizes a Gaussian Mixture Model (GMM) to represent the swarm's macroscopic state and constructs a Probabilistic Roadmap in Gaussian space, referred to as the Gaussian roadmap, to generate a transport trajectory of GMM. This trajectory is then followed by each robot at the microscopic stage. To enhance trajectory safety, SwarmPRM incorporates the conditional value-at-risk (CVaR) in the collision checking process to impart the property of risk awareness to the constructed Gaussian roadmap. SwarmPRM then crafts a linear programming formulation to compute the optimal GMM transport trajectory within this roadmap. Extensive simulations demonstrate that SwarmPRM outperforms state-of-the-art methods in computational efficiency, scalability, and trajectory quality while offering the capability to adjust the risk tolerance of generated trajectories."
MICP-L: Mesh-based ICP for Robot Localization using Hardware-Accelerated Ray Casting,"Mock, Alexander; Wiemann, Thomas; Pütz, Sebastian; Hertzberg, Joachim",https://arxiv.org/abs/2210.13904,"Triangle mesh maps are a versatile 3D environment representation for robots to navigate in challenging indoor and outdoor environments exhibiting tunnels, hills and varying slopes. To make use of these mesh maps, methods are needed to accurately localize robots in such maps to perform essential tasks like path planning and navigation. We present Mesh ICP Localization (MICP-L), a novel and computationally efficient method for registering one or more range sensors to a triangle mesh map to continuously localize a robot in 6D, even in GPS-denied environments. We accelerate the computation of ray casting correspondences (RCC) between range sensors and mesh maps by supporting different parallel computing devices like multicore CPUs, GPUs and the latest NVIDIA RTX hardware. By additionally transforming the covariance computation into a reduction operation, we can optimize the initial guessed poses in parallel on CPUs or GPUs, making our implementation applicable in real-time on many architectures. We demonstrate the robustness of our localization approach with datasets from agricultural, aerial, and automotive domains."
A Data-Informed Analysis of Scalable Supervision for Safety in Autonomous Vehicle Fleets,"Hickert, Cameron; Yan, Zhongxia; Wu, Cathy",,
Gaining the Sparse Rewards by Exploring Lottery Tickets in Spiking Neural Network,"Cheng, Hao; Cao, Jiahang; Xiao, Erjia; Sun, Mengshu; Xu, Renjing",https://arxiv.org/abs/2309.13302,"Deploying energy-efficient deep learning algorithms on computational-limited devices, such as robots, is still a pressing issue for real-world applications. Spiking Neural Networks (SNNs), a novel brain-inspired algorithm, offer a promising solution due to their low-latency and low-energy properties over traditional Artificial Neural Networks (ANNs). Despite their advantages, the dense structure of deep SNNs can still result in extra energy consumption. The Lottery Ticket Hypothesis (LTH) posits that within dense neural networks, there exist winning Lottery Tickets (LTs), namely sub-networks, that can be obtained without compromising performance. Inspired by this, this paper delves into the spiking-based LTs (SLTs), examining their unique properties and potential for extreme efficiency. Then, two significant sparse \textbf{\textit{Rewards}} are gained through comprehensive explorations and meticulous experiments on SLTs across various dense structures. Moreover, a sparse algorithm tailored for spiking transformer structure, which incorporates convolution operations into the Patch Embedding Projection (ConvPEP) module, has been proposed to achieve Multi-level Sparsity (MultiSp). MultiSp refers to (1) Patch number sparsity; (2) ConvPEP weights sparsity and binarization; and (3) ConvPEP activation layer binarization. Extensive experiments demonstrate that our method achieves extreme sparsity with only a slight performance decrease, paving the way for deploying energy-efficient neural networks in robotics and beyond."
Construction of Musculoskeletal Simulation for Shoulder Complex with Ligaments and Its Validation via Model Predictive Control,"Sahara, Yuta; Miki, Akihiro; Ribayashi, Yoshimoto; Yoshimura, Shunnosuke; Kawaharazuka, Kento; Okada, Kei; Inaba, Masayuki",,
Beyond the Cascade: Juggling Vanilla Siteswap Patterns,"Gomez Andreu, Mario Alejandro; Ploeger, Kai; Peters, Jan",,
Rain-Reaper: Unmasking LiDAR-based Detector Vulnerabilities in Rain,"Capraru, Richard; Lupu, Emil Constantin; Demetriou, Soteris; Wang, Jian-Gang; Soong, Boon Hee",,
Collaborative Graph Exploration with Reduced Pose-SLAM Uncertainty via Submodular Optimization,"Bai, Ruofei; Yuan, Shenghai; Guo, Hongliang; Yin, Pengyu; Yau, Wei-Yun; Xie, Lihua",https://arxiv.org/abs/2407.01013,"This paper considers the collaborative graph exploration problem in GPS-denied environments, where a group of robots are required to cover a graph environment while maintaining reliable pose estimations in collaborative simultaneous localization and mapping (SLAM). Considering both objectives presents challenges for multi-robot pathfinding, as it involves the expensive covariance inference for SLAM uncertainty evaluation, especially considering various combinations of robots' paths. To reduce the computational complexity, we propose an efficient two-stage strategy where exploration paths are first generated for quick coverage, and then enhanced by adding informative and distance-efficient loop-closing actions, called loop edges, along the paths for reliable pose estimation. We formulate the latter problem as a non-monotone submodular maximization problem by relating SLAM uncertainty with pose graph topology, which (1) facilitates more efficient evaluation of SLAM uncertainty than covariance inference, and (2) allows the application of approximation algorithms in submodular optimization to provide optimality guarantees. We further introduce the ordering heuristics to improve objective values while preserving the optimality bound. Simulation experiments over randomly generated graph environments verify the efficiency of our methods in finding paths for quick coverage and enhanced pose graph reliability, and benchmark the performance of the approximation algorithms and the greedy-based algorithm in the loop edge selection problem. Our implementations will be open-source at https://github.com/bairuofei/CGE."
Development of five-finger hand-type robotic forceps for laparoscopic gastrointestinal surgery,"Wakamatsu, Hiroyuki; Kobayashi, Ibuki; Nagase, Yuya; Kato, Ryu; Mukai, Masaya",,
Robot Shape and Location Retention in Video Generation Using Diffusion Models,"Wang, Peng; Guo, Zhihao; Sait, Abdul Latheef; Pham, Minh Huy",https://arxiv.org/abs/2407.02873,"Diffusion models have marked a significant milestone in the enhancement of image and video generation technologies. However, generating videos that precisely retain the shape and location of moving objects such as robots remains a challenge. This paper presents diffusion models specifically tailored to generate videos that accurately maintain the shape and location of mobile robots. This development offers substantial benefits to those working on detecting dangerous interactions between humans and robots by facilitating the creation of training data for collision detection models, circumventing the need for collecting data from the real world, which often involves legal and ethical issues. Our models incorporate techniques such as embedding accessible robot pose information and applying semantic mask regulation within the ConvNext backbone network. These techniques are designed to refine intermediate outputs, therefore improving the retention performance of shape and location. Through extensive experimentation, our models have demonstrated notable improvements in maintaining the shape and location of different robots, as well as enhancing overall video generation quality, compared to the benchmark diffusion model. Codes will be opensourced at \href{https://github.com/PengPaulWang/diffusion-robots}{Github}."
Fine Manipulation Using a Tactile Skin: Learning in Simulation and Sim-to-Real Transfer,"Bäuml, Berthold; Kasolowsky, Ulf",,
"Frozen Assets: Leveraging Ice, Water, and Phase Transitions in Robots","Wilhelm, Aaron; Wilhelm, Andrew; Calderón-Aceituno, Lydia Isabela; Napp, Nils; Petersen, Kirstin Hagelskjaer; Helbling, E. Farrell",,
Active Pose Refinement for Textureless Shiny Objects using the Structured Light Camera,"Yang, Jun; Yao, Jian; Waslander, Steven Lake",https://arxiv.org/abs/2308.14665,"6D pose estimation of textureless shiny objects has become an essential problem in many robotic applications. Many pose estimators require high-quality depth data, often measured by structured light cameras. However, when objects have shiny surfaces (e.g., metal parts), these cameras fail to sense complete depths from a single viewpoint due to the specular reflection, resulting in a significant drop in the final pose accuracy. To mitigate this issue, we present a complete active vision framework for 6D object pose refinement and next-best-view prediction. Specifically, we first develop an optimization-based pose refinement module for the structured light camera. Our system then selects the next best camera viewpoint to collect depth measurements by minimizing the predicted uncertainty of the object pose. Compared to previous approaches, we additionally predict measurement uncertainties of future viewpoints by online rendering, which significantly improves the next-best-view prediction performance. We test our approach on the challenging real-world ROBI dataset. The results demonstrate that our pose refinement method outperforms the traditional ICP-based approach when given the same input depth data, and our next-best-view strategy can achieve high object pose accuracy with significantly fewer viewpoints than the heuristic-based policies."
Intention-Aware Planner for Robust and Safe Aerial Tracking,"Ren, Qiuyu; Yu, Huan; Dai, Jiajun; Zheng, Zhi; Meng, Jun; Xu, Li; Xu, Chao; Gao, Fei; Cao, Yanjun",https://arxiv.org/abs/2309.08854,"Autonomous target tracking with quadrotors has wide applications in many scenarios, such as cinematographic follow-up shooting or suspect chasing. Target motion prediction is necessary when designing the tracking planner. However, the widely used constant velocity or constant rotation assumption can not fully capture the dynamics of the target. The tracker may fail when the target happens to move aggressively, such as sudden turn or deceleration. In this paper, we propose an intention-aware planner by additionally considering the intention of the target to enhance safety and robustness in aerial tracking applications. Firstly, a designated intention prediction method is proposed, which combines a user-defined potential assessment function and a state observation function. A reachable region is generated to specifically evaluate the turning intentions. Then we design an intention-driven hybrid A* method to predict the future possible positions for the target. Finally, an intention-aware optimization approach is designed to generate a spatial-temporal optimal trajectory, allowing the tracker to perceive unexpected situations from the target. Benchmark comparisons and real-world experiments are conducted to validate the performance of our method."
High Rate Mechanical Coupling of Interacting Objects in the Context of Needle Insertion Simulation With Haptic Feedback,"MARTIN, Claire; Duriez, Christian; Courtecuisse, Hadrien",,
Markerless Aerial-Terrestrial Co-Registration of Forest Point Clouds using a Deformable Pose Graph,"Casseau, Benoit; Chebrolu, Nived; Mattamala, Matias; Freißmuth, Leonard; Fallon, Maurice",,
DSVT: Dynamic 3D Surround View for Tractor-Trailer Vehicles Based on Real-Time Pose Estimation with Drop Model,"Dong, Zhipeng; Fu, Mengyin; Liang, Hao; Zhu, Chunhui; Yang, Yi",,
SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal Conditioned Diffusion Policy,"Bartsch, Alison; Car, Arvind; Avra, Charlotte; Barati Farimani, Amir",https://arxiv.org/abs/2403.10401,"Manipulating deformable objects remains a challenge within robotics due to the difficulties of state estimation, long-horizon planning, and predicting how the object will deform given an interaction. These challenges are the most pronounced with 3D deformable objects. We propose SculptDiff, a goal-conditioned diffusion-based imitation learning framework that works with point cloud state observations to directly learn clay sculpting policies for a variety of target shapes. To the best of our knowledge this is the first real-world method that successfully learns manipulation policies for 3D deformable objects. For sculpting videos and access to our dataset and hardware CAD models, see the project website: https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home"
Synthetic Dataset Using Diffusion Model for Pixel-Level Dense Pose Estimation,"Wen, Jiaixiao; Liu, Qiong",,
Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?,"Hallgarten, Marcel; Zapata Manjarres, Julian Jose; Stoll, Martin; Renz, Katrin; Zell, Andreas",https://arxiv.org/abs/2404.07569,"Real-world autonomous driving systems must make safe decisions in the face of rare and diverse traffic scenarios. Current state-of-the-art planners are mostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan (closed-loop). In particular, nuPlan seems to be an expressive evaluation method since it is based on real-world data and closed-loop, yet it mostly covers basic driving scenarios. This makes it difficult to judge a planner's capabilities to generalize to rarely-seen situations. Therefore, we propose a novel closed-loop benchmark interPlan containing several edge cases and challenging driving scenarios. We assess existing state-of-the-art planners on our benchmark and show that neither rule-based nor learning-based planners can safely navigate the interPlan scenarios. A recently evolving direction is the usage of foundation models like large language models (LLM) to handle generalization. We evaluate an LLM-only planner and introduce a novel hybrid planner that combines an LLM-based behavior planner with a rule-based motion planner that achieves state-of-the-art performance on our benchmark."
A Neurosymbolic Approach to Adaptive Feature Extraction in SLAM,"Chandio, Yasra; Khan, Momin Ahmad; Selialia, Khotso; Garcia, Luis Antonio; DeGol, Joseph; Anwar, Fatima M",https://arxiv.org/abs/2407.06889,"Autonomous robots, autonomous vehicles, and humans wearing mixed-reality headsets require accurate and reliable tracking services for safety-critical applications in dynamically changing real-world environments. However, the existing tracking approaches, such as Simultaneous Localization and Mapping (SLAM), do not adapt well to environmental changes and boundary conditions despite extensive manual tuning. On the other hand, while deep learning-based approaches can better adapt to environmental changes, they typically demand substantial data for training and often lack flexibility in adapting to new domains. To solve this problem, we propose leveraging the neurosymbolic program synthesis approach to construct adaptable SLAM pipelines that integrate the domain knowledge from traditional SLAM approaches while leveraging data to learn complex relationships. While the approach can synthesize end-to-end SLAM pipelines, we focus on synthesizing the feature extraction module. We first devise a domain-specific language (DSL) that can encapsulate domain knowledge on the important attributes for feature extraction and the real-world performance of various feature extractors. Our neurosymbolic architecture then undertakes adaptive feature extraction, optimizing parameters via learning while employing symbolic reasoning to select the most suitable feature extractor. Our evaluations demonstrate that our approach, neurosymbolic Feature EXtraction (nFEX), yields higher-quality features. It also reduces the pose error observed for the state-of-the-art baseline feature extractors ORB and SIFT by up to 90% and up to 66%, respectively, thereby enhancing the system's efficiency and adaptability to novel environments."
Imagine2Servo: Intelligent Visual Servoing with Diffusion-Driven Goal Generation for Robotic Tasks,"Pathre, Pranjali; Gupta, Gunjan; Qureshi, Mohammad Nomaan; Mandyam, Brunda; Brahmbhatt, Samarth Manoj; Krishna, Madhava",,
SDTrack:Spatially decoupled tracker for visual tracking,"xia, zihao; fan, baojie; Wang, Zhiquan; Ai, Jiajun",,
Mobility Performance Characterization of Transformable Nano Rover for Lunar Exploration,"Sutoh, Masataku; Hirano, Daichi; Inazawa, Mariko; Kawai, Yuta; SAWADA, HIROTAKA",,
"NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices","Zhang, Zhiyong; Singh, Hanumant; Jiang, Huaizu",https://arxiv.org/abs/2403.10425,"Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision. While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs. In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns. The architecture follows a global-to-local scheme. Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different computing platforms. We achieve a notable 10x-80x speedup compared to several state-of-the-art methods, while maintaining comparable accuracy. Our approach achieves around 30 FPS on edge computing platforms, which represents a significant breakthrough in deploying complex computer vision tasks such as SLAM on small robots like drones. The full training and evaluation code is available at https://github.com/neufieldrobotics/NeuFlow."
A non-invasive device for skin cancer diagnosis: first clinical evidence with spectroscopic data enhanced by Machine Learning algorithms,"Mainardi, Vanessa; Carletti, Laura; Tsiakmakis, Dimitrios; Dal Canto, Marco; Mellilo, Tommaso; Noferi, Stefano; Bagnoni, Giovanni; Rubegni, Pietro; Ciuti, Gastone",,
To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions,"Tanneberg, Daniel; Ocker, Felix; Hasler, Stephan; Deigmoeller, Joerg; Belardinelli, Anna; Wang, Chao; Wersing, Heiko; Sendhoff, Bernhard; Gienger, Michael",https://arxiv.org/abs/2403.12533,"How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed."
An Ultrafast Multi-object Zooming System Based on Low-latency Stereo Correspondence,"Li, Qing; HU, SHAOPENG; Shimasaki, Kohei; Ishii, Idaku",,
Camera-Based Belief Space Planning in Discrete Partially-Observable Domains,"Freund, Janis Eric; Phiquepal, Camille; Orthey, Andreas; Toussaint, Marc",https://arxiv.org/abs/2309.10672,"Robots often have to operate in discrete partially observable worlds, where the states of world are only observable at runtime. To react to different world states, robots need contingencies. However, computing contingencies is costly and often non-optimal. To address this problem, we develop the improved path tree optimization (PTO) method. PTO computes motion contingencies by constructing a tree of motion paths in belief space. This is achieved by constructing a graph of configurations, then adding observation edges to extend the graph to belief space. Afterwards, we use a dynamic programming step to extract the path tree. PTO extends prior work by adding a camera-based state sampler to improve the search for observation points. We also add support to non-euclidean state spaces, provide an implementation in the open motion planning library (OMPL), and evaluate PTO on four realistic scenarios with a virtual camera in up to 10-dimensional state spaces. We compare PTO with a default and with the new camera-based state sampler. The results indicate that the camera-based state sampler improves success rates in 3 out of 4 scenarios while having a significant lower memory footprint. This makes PTO an important contribution to advance the state-of-the-art for discrete belief space planning."
Efficient Motion Prediction: A Lightweight & Accurate Trajectory Prediction Model With Fast Training and Inference Speed,"Prutsch, Alexander; Bischof, Horst; Possegger, Horst",https://arxiv.org/abs/1801.01444,"The rapid growth of IoT era is shaping the future of mobile services. Advanced communication technology enables a heterogeneous connectivity where mobile devices broadcast information to everything. Mobile applications such as robotics and vehicles connecting to cloud and surroundings transfer the short-range on-board sensor perception system to long-range mobile-sensing perception system. However, the mobile sensing perception brings new challenges for how to efficiently analyze and intelligently interpret the deluge of IoT data in mission- critical services. In this article, we model the challenges as latency, packet loss and measurement noise which severely deteriorate the reliability and quality of IoT data. We integrate the artificial intelligence into IoT to tackle these challenges. We propose a novel architecture that leverages recurrent neural networks (RNN) and Kalman filtering to anticipate motions and interac- tions between objects. The basic idea is to learn environment dynamics by recurrent networks. To improve the robustness of IoT communication, we use the idea of Kalman filtering and deploy a prediction and correction step. In this way, the architecture learns to develop a biased belief between prediction and measurement in the different situation. We demonstrate our approach with synthetic and real-world datasets with noise that mimics the challenges of IoT communications. Our method brings a new level of IoT intelligence. It is also lightweight compared to other state-of-the-art convolutional recurrent architecture and is ideally suitable for the resource-limited mobile applications."
A Robust and Efficient Robotic Packing Pipeline with Dissipativity-Based Adaptive Impedance-Force Control,"Zhou, Zhenning; Zhou, Lei; Sun, Shengxin; Ang Jr, Marcelo H",,
MAGYC: A Factor Graph Based Method using Angular Rate Measurements for Full Three-Axis Magnetometer and Gyroscope Bias Estimation,"Rodríguez-Martínez, Sebastián; Troni, Giancarlo",,
Enhancing LiDAR Scene Upsampling with Instance-aware Feature-embedding and Attention Mechanism,"Wang, Wei-Ren; Do, You-Sheng; Lin, Wen-Chieh; Wang, Chieh-Chih",,
Hyperbolic Image-and-Pointcloud Contrastive Learning for 3D Classification,"Hu, Naiwen; Cheng, Haozhe; Xie, Yifan; Shi, Pengcheng; Zhu, Jihua",,
Inverse Kinematics for Neuro-Robotic Grasping with Humanoid Embodied Agents,"Habekost, Jan-Gerrit; Gäde, Connor; Allgeuer, Philipp; Wermter, Stefan",https://arxiv.org/abs/2404.08825,"This paper introduces a novel zero-shot motion planning method that allows users to quickly design smooth robot motions in Cartesian space. A Bézier curve-based Cartesian plan is transformed into a joint space trajectory by our neuro-inspired inverse kinematics (IK) method CycleIK, for which we enable platform independence by scaling it to arbitrary robot designs. The motion planner is evaluated on the physical hardware of the two humanoid robots NICO and NICOL in a human-in-the-loop grasping scenario. Our method is deployed with an embodied agent that is a large language model (LLM) at its core. We generalize the embodied agent, that was introduced for NICOL, to also be embodied by NICO. The agent can execute a discrete set of physical actions and allows the user to verbally instruct various different robots. We contribute a grasping primitive to its action space that allows for precise manipulation of household objects. The new CycleIK method is compared to popular numerical IK solvers and state-of-the-art neural IK methods in simulation and is shown to be competitive with or outperform all evaluated methods when the algorithm runtime is very short. The grasping primitive is evaluated on both NICOL and NICO robots with a reported grasp success of 72% to 82% for each robot, respectively."
Towards Electricity-free Pneumatic Miniature Rotation Actuator for Optical Coherence Tomography Endoscopy,"ZHANG, Tinghua; Yuan, Sishen; Xu, Chao; Liu, Peng; Ren, Hongliang; Yuan, Wu",,
"Indoor Scene Change Understanding (SCU): Segment, Describe, and Revert Any Change","Khan, Mariia; QIU, YUE; Cong, Yuren; Rosenhahn, Bodo; Suter, David; Abu-Khalaf, Jumana",,
Good Things Always Come in Threes: How Robot Responsiveness Affects Workload and Trust in Non-Dyadic Human-Robot Collaboration,"Semeraro, Francesco; Carberry, Jon; Leadbetter, James Hugo; Cangelosi, Angelo",,
Generating Continuous Paths On Learned Constraint Manifolds Using Policy Search,"Canzini, Ethan; Pope, Simon A.; Tiwari, Ashutosh",,
Stick Roller: Precise In-hand Stick Rolling with a Sample-Efficient Tactile Model,"Du, Yipai; Zhou, Pokuang; Wang, Michael Yu; Lian, Wenzhao; She, Yu",,
Strain-based Modeling of Rod-driven Soft Continuum Robots with Co-located Embedded Sensors,"Wang, Peiyi; Feliu, Daniel; Guo, Sheng; Renda, Federico; Laschi, Cecilia",,
Design and Development of a Work Cell with a One-Handed Soldering Tool for Enhanced Human-Robot Collaboration,"Suppaadirek, Natchanon; Sonnic, Maximilien; Duran Jimenez, Raul Ariel; Shibata, Tomohiro",,
TeFF: Tracking-enhanced Forgetting-free Few-shot 3D LiDAR Semantic Segmentation,"Zhou, Junbao; Mei, Jilin; Wu, Pengze; Chen, Liang; Zhao, Fangzhou; Zhao, Xijun; Hu, Yu",https://arxiv.org/abs/2408.15657,"In autonomous driving, 3D LiDAR plays a crucial role in understanding the vehicle's surroundings. However, the newly emerged, unannotated objects presents few-shot learning problem for semantic segmentation. This paper addresses the limitations of current few-shot semantic segmentation by exploiting the temporal continuity of LiDAR data. Employing a tracking model to generate pseudo-ground-truths from a sequence of LiDAR frames, our method significantly augments the dataset, enhancing the model's ability to learn on novel classes. However, this approach introduces a data imbalance biased to novel data that presents a new challenge of catastrophic forgetting. To mitigate this, we incorporate LoRA, a technique that reduces the number of trainable parameters, thereby preserving the model's performance on base classes while improving its adaptability to novel classes. This work represents a significant step forward in few-shot 3D LiDAR semantic segmentation for autonomous driving. Our code is available at https://github.com/junbao-zhou/Track-no-forgetting."
Risk-Averse Planning and Plan Assessment for Marine Robots,"Mohammadi Kashani, Mahya; John, Tobias; Coffelt, Jeremy Paul; Johnsen, Einar Broch; Wasowski, Andrzej",,
Enhancing Robustness in Manipulability Assessment: The Pseudo-Ellipsoid Approach,"Shahriari, Erfan; Peper, Kim Kristin; Hoffmann, Matej; Haddadin, Sami",,
Repairing Neural Networks for Safety in Robotic Systems using Predictive Models,"Majd, Keyvan; Clark, Geoffrey; Fainekos, Georgios; Ben Amor, Heni",,
Effect of Tactile and Deep Sensory Feedback Synchronized with the Manipulation of Myoelectric Hand on Body Recognition,"Hamaoka, Rintaro; Kato, Ryu",,
DSLO: Deep Sequence LiDAR Odometry Based on Inconsistent Spatio-temporal Propagation,"Zhang, Huixin; Wang, Guangming; Wu, Xinrui; Xu, Chenfeng; Ding, Mingyu; Tomizuka, Masayoshi; Zhan, Wei; Wang, Hesheng",https://arxiv.org/abs/2409.00744,"This paper introduces a 3D point cloud sequence learning model based on inconsistent spatio-temporal propagation for LiDAR odometry, termed DSLO. It consists of a pyramid structure with a spatial information reuse strategy, a sequential pose initialization module, a gated hierarchical pose refinement module, and a temporal feature propagation module. First, spatial features are encoded using a point feature pyramid, with features reused in successive pose estimations to reduce computational overhead. Second, a sequential pose initialization method is introduced, leveraging the high-frequency sampling characteristic of LiDAR to initialize the LiDAR pose. Then, a gated hierarchical pose refinement mechanism refines poses from coarse to fine by selectively retaining or discarding motion information from different layers based on gate estimations. Finally, temporal feature propagation is proposed to incorporate the historical motion information from point cloud sequences, and address the spatial inconsistency issue when transmitting motion information embedded in point clouds between frames. Experimental results on the KITTI odometry dataset and Argoverse dataset demonstrate that DSLO outperforms state-of-the-art methods, achieving at least a 15.67\% improvement on RTE and a 12.64\% improvement on RRE, while also achieving a 34.69\% reduction in runtime compared to baseline methods. Our implementation will be available at https://github.com/IRMVLab/DSLO."
All-day Depth Completion,"Ezhov, Vadim; Park, Hyoungseob; Zhang, Zhaoyang; Upadhyay, Rishi; Zhang, Howard; Chandrappa, Chethan Chinder; Kadambi, Achuta; Ba, Yunhao; Dorsey, Julie; Wong, Alex",https://arxiv.org/abs/2405.17315,"We propose a method for depth estimation under different illumination conditions, i.e., day and night time. As photometry is uninformative in regions under low-illumination, we tackle the problem through a multi-sensor fusion approach, where we take as input an additional synchronized sparse point cloud (i.e., from a LiDAR) projected onto the image plane as a sparse depth map, along with a camera image. The crux of our method lies in the use of the abundantly available synthetic data to first approximate the 3D scene structure by learning a mapping from sparse to (coarse) dense depth maps along with their predictive uncertainty - we term this, SpaDe. In poorly illuminated regions where photometric intensities do not afford the inference of local shape, the coarse approximation of scene depth serves as a prior; the uncertainty map is then used with the image to guide refinement through an uncertainty-driven residual learning (URL) scheme. The resulting depth completion network leverages complementary strengths from both modalities - depth is sparse but insensitive to illumination and in metric scale, and image is dense but sensitive with scale ambiguity. SpaDe can be used in a plug-and-play fashion, which allows for 25% improvement when augmented onto existing methods to preprocess sparse depth. We demonstrate URL on the nuScenes dataset where we improve over all baselines by an average 11.65% in all-day scenarios, 11.23% when tested specifically for daytime, and 13.12% for nighttime scenes."
PARE: A Plane-Assisted Autonomous Robot Exploration Framework in Unknown and Uneven Terrain,"Xu, Pu; Bai, Zhaoqiang; Liu, Haoming; Fang, Zheng",,
Improving Out-of-Distribution Generalization of Trajectory Prediction for Autonomous Driving via Polynomial Representations,"Yao, Yue; Yan, Shengchao; Goehring, Daniel; Burgard, Wolfram; Reichardt, Joerg",https://arxiv.org/abs/2407.13431,"Robustness against Out-of-Distribution (OoD) samples is a key performance indicator of a trajectory prediction model. However, the development and ranking of state-of-the-art (SotA) models are driven by their In-Distribution (ID) performance on individual competition datasets. We present an OoD testing protocol that homogenizes datasets and prediction tasks across two large-scale motion datasets. We introduce a novel prediction algorithm based on polynomial representations for agent trajectory and road geometry on both the input and output sides of the model. With a much smaller model size, training effort, and inference time, we reach near SotA performance for ID testing and significantly improve robustness in OoD testing. Within our OoD testing protocol, we further study two augmentation strategies of SotA models and their effects on model generalization. Highlighting the contrast between ID and OoD performance, we suggest adding OoD testing to the evaluation criteria of trajectory prediction models."
Single Protoplasts Pickup System Combining Brightfield and Confocal Images,"Ando, Daito; Turan, Bilal; Amaya, Satoshi; Ukai, Yuko; Sato, Yoshikatsu; Arai, Fumihito",,
Multi-Spectral Visual Servoing,"Fiasche, Enrico; Malis, Ezio; Martinet, Philippe",,
Weakly Scene Segmentation Using Efficient Transformer,"Huang, Hao; Yuan, Shuaihang; Wen, Congcong; Hao, Yu; Fang, Yi",https://arxiv.org/abs/2309.04105,"The annotation of 3D datasets is required for semantic-segmentation and object detection in scene understanding. In this paper we present a framework for the weakly supervision of a point clouds transformer that is used for 3D object detection. The aim is to decrease the required amount of supervision needed for training, as a result of the high cost of annotating a 3D datasets. We propose an Unsupervised Voting Proposal Module, which learns randomly preset anchor points and uses voting network to select prepared anchor points of high quality. Then it distills information into student and teacher network. In terms of student network, we apply ResNet network to efficiently extract local characteristics. However, it also can lose much global information. To provide the input which incorporates the global and local information as the input of student networks, we adopt the self-attention mechanism of transformer to extract global features, and the ResNet layers to extract region proposals. The teacher network supervises the classification and regression of the student network using the pre-trained model on ImageNet. On the challenging KITTI datasets, the experimental results have achieved the highest level of average precision compared with the most recent weakly supervised 3D object detectors."
Learning-based Hierarchical Control: Emulating the Central Nervous System for Bio-Inspired Legged Robot Locomotion,"SUN, GE; Shafiee, Milad; LI, Peizhuo; Bellegarda, Guillaume; Ijspeert, Auke; Sartoretti, Guillaume Adrien",https://arxiv.org/abs/2404.17815,"Animals possess a remarkable ability to navigate challenging terrains, achieved through the interplay of various pathways between the brain, central pattern generators (CPGs) in the spinal cord, and musculoskeletal system. Traditional bioinspired control frameworks often rely on a singular control policy that models both higher (supraspinal) and spinal cord functions. In this work, we build upon our previous research by introducing two distinct neural networks: one tasked with modulating the frequency and amplitude of CPGs to generate the basic locomotor rhythm (referred to as the spinal policy, SCP), and the other responsible for receiving environmental perception data and directly modulating the rhythmic output from the SCP to execute precise movements on challenging terrains (referred to as the descending modulation policy). This division of labor more closely mimics the hierarchical locomotor control systems observed in legged animals, thereby enhancing the robot's ability to navigate various uneven surfaces, including steps, high obstacles, and terrains with gaps. Additionally, we investigate the impact of sensorimotor delays within our framework, validating several biological assumptions about animal locomotion systems. Specifically, we demonstrate that spinal circuits play a crucial role in generating the basic locomotor rhythm, while descending pathways are essential for enabling appropriate gait modifications to accommodate uneven terrain. Notably, our findings also reveal that the multi-layered control inherent in animals exhibits remarkable robustness against time delays. Through these investigations, this paper contributes to a deeper understanding of the fundamental principles of interplay between spinal and supraspinal mechanisms in biological locomotion. It also supports the development of locomotion controllers in parallel to biological structures which are ..."
Tracking Tumors under Deformation from Partial Point Clouds using Occupancy Networks,"Henrich, Pit; Liu, Jiawei; Ge, Jiawei; Schmidgall, Samuel; Shepard, Lauren; Ghazi, Ahmed; Mathis-Ullrich, Franziska; Krieger, Axel",,
BAM: Box Abstraction Monitors for Real-time OoD Detection in Object Detection,"Wu, Changshun; HE, Weicheng; Cheng, Chih-Hong; Huang, Xiaowei; Bensalem, Saddek",https://arxiv.org/abs/2403.18373,"Out-of-distribution (OoD) detection techniques for deep neural networks (DNNs) become crucial thanks to their filtering of abnormal inputs, especially when DNNs are used in safety-critical applications and interact with an open and dynamic environment. Nevertheless, integrating OoD detection into state-of-the-art (SOTA) object detection DNNs poses significant challenges, partly due to the complexity introduced by the SOTA OoD construction methods, which require the modification of DNN architecture and the introduction of complex loss functions. This paper proposes a simple, yet surprisingly effective, method that requires neither retraining nor architectural change in object detection DNN, called Box Abstraction-based Monitors (BAM). The novelty of BAM stems from using a finite union of convex box abstractions to capture the learned features of objects for in-distribution (ID) data, and an important observation that features from OoD data are more likely to fall outside of these boxes. The union of convex regions within the feature space allows the formation of non-convex and interpretable decision boundaries, overcoming the limitations of VOS-like detectors without sacrificing real-time performance. Experiments integrating BAM into Faster R-CNN-based object detection DNNs demonstrate a considerably improved performance against SOTA OoD detection techniques."
Cross-Observability Learning for Vehicle Routing Problems,"Liu, Ruifan; Shin, Hyo-Sang; Tsourdos, Antonios",,
Optimal view point and kinematic control for grape stem detection and cutting with an in-hand camera robot,"Stavridis, Sotiris; Doulgeri, Zoe",,
Self-reconfiguration strategies for space-distributed spacecraft,"Liu, Tianle; Wang, Zhixiang; Zhang, Yongwei; Zhang, Yizhai; Huang, Panfeng",,
Foot Arch Stiffness-Based Dynamic Plantar Support Control of Human Walking Gait with Active Pneumatic Insoles,"Liu, Chenhao; Yi, Jingang; He, Long; Zhang, Yijun; Zhang, Xiufeng; Liu, Tao",,
Preventing Catastrophic Forgetting in Continuous Online Learning for Autonomous Driving,"Yang, Rui; Yan, Zhi; Yang, Tao; Krajník, Tomá; Ruichek, Yassine",,
LDP: A Local Diffusion Planner for Efficient Robot Navigation and Collision Avoidance,"Yu, Wenhao; Peng, Jie; yang, huanyu; Zhang, Junrui; Duan, Yifan; Ji, Jianmin; Zhang, Yanyong",https://arxiv.org/abs/2407.01950,"The conditional diffusion model has been demonstrated as an efficient tool for learning robot policies, owing to its advancement to accurately model the conditional distribution of policies. The intricate nature of real-world scenarios, characterized by dynamic obstacles and maze-like structures, underscores the complexity of robot local navigation decision-making as a conditional distribution problem. Nevertheless, leveraging the diffusion model for robot local navigation is not trivial and encounters several under-explored challenges: (1) Data Urgency. The complex conditional distribution in local navigation needs training data to include diverse policy in diverse real-world scenarios; (2) Myopic Observation. Due to the diversity of the perception scenarios, diffusion decisions based on the local perspective of robots may prove suboptimal for completing the entire task, as they often lack foresight. In certain scenarios requiring detours, the robot may become trapped. To address these issues, our approach begins with an exploration of a diverse data generation mechanism that encompasses multiple agents exhibiting distinct preferences through target selection informed by integrated global-local insights. Then, based on this diverse training data, a diffusion agent is obtained, capable of excellent collision avoidance in diverse scenarios. Subsequently, we augment our Local Diffusion Planner, also known as LDP by incorporating global observations in a lightweight manner. This enhancement broadens the observational scope of LDP, effectively mitigating the risk of becoming ensnared in local optima and promoting more robust navigational decisions."
DEAR: Disentangled Environment and Agent Representations for Reinforcement Learning without Reconstruction,"Pore, Ameya; Muradore, Riccardo; Dall'Alba, Diego",https://arxiv.org/abs/2407.00633,"Reinforcement Learning (RL) algorithms can learn robotic control tasks from visual observations, but they often require a large amount of data, especially when the visual scene is complex and unstructured. In this paper, we explore how the agent's knowledge of its shape can improve the sample efficiency of visual RL methods. We propose a novel method, Disentangled Environment and Agent Representations (DEAR), that uses the segmentation mask of the agent as supervision to learn disentangled representations of the environment and the agent through feature separation constraints. Unlike previous approaches, DEAR does not require reconstruction of visual observations. These representations are then used as an auxiliary loss to the RL objective, encouraging the agent to focus on the relevant features of the environment. We evaluate DEAR on two challenging benchmarks: Distracting DeepMind control suite and Franka Kitchen manipulation tasks. Our findings demonstrate that DEAR surpasses state-of-the-art methods in sample efficiency, achieving comparable or superior performance with reduced parameters. Our results indicate that integrating agent knowledge into visual RL methods has the potential to enhance their learning efficiency and robustness."
Priority-Based Deadlock Recovery for Distributed Swarm Obstacle Avoidance in Cluttered Environments,"He, jiacheng; Zhao, Fangguo; Zhu, Shaohao; Li, Shuo; Xu, Jinming",,
Environment-Adaptive Gait Planning for Obstacle Avoidance in Lower-Limb Robotic Exoskeletons,"Trombin, Edoardo; Tortora, Stefano; Menegatti, Emanuele; Tonin, Luca",,
RATE: Real-time Asynchronous Feature Tracking with Event Cameras,"Ikura, Mikihiro; Le Gentil, Cedric; Müller, Marcus Gerhard; Yamashita, Atsushi; Stuerzl, Wolfgang",https://arxiv.org/abs/2004.07398,"Robotic vision plays a major role in factory automation to service robot applications. However, the traditional use of frame-based camera sets a limitation on continuous visual feedback due to their low sampling rate and redundant data in real-time image processing, especially in the case of high-speed tasks. Event cameras give human-like vision capabilities such as observing the dynamic changes asynchronously at a high temporal resolution ($1μs$) with low latency and wide dynamic range.   In this paper, we present a visual servoing method using an event camera and a switching control strategy to explore, reach and grasp to achieve a manipulation task. We devise three surface layers of active events to directly process stream of events from relative motion. A purely event based approach is adopted to extract corner features, localize them robustly using heat maps and generate virtual features for tracking and alignment. Based on the visual feedback, the motion of the robot is controlled to make the temporal upcoming event features converge to the desired event in spatio-temporal space. The controller switches its strategy based on the sequence of operation to establish a stable grasp. The event based visual servoing (EVBS) method is validated experimentally using a commercial robot manipulator in an eye-in-hand configuration. Experiments prove the effectiveness of the EBVS method to track and grasp objects of different shapes without the need for re-tuning."
Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments,"Tellaroli, Mauro; Luperto, Matteo; Antonazzi, Michele; Basilico, Nicola",https://arxiv.org/abs/2403.11617,"Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems. This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous. Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages. We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities. We validate our approach in 3D realistic simulations using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies."
Haptic Contour Following with the Smart Suction Cup,"Lee, Sebastian; Lee, Jungpyo; Stuart, Hannah",,
An Agile Robotic Penguin Driven by Submersible Geared&#12288;Servomotors: Various Maneuvers by Active Feathering of the Wings,"Shimooka, Taiki; Kakogawa, Atsushi; Tanaka, Hiroto",,
"A Service Robot in the Wild: Analysis of Users Intentions,Robot Behaviors, and Their Mutual Impact","Arreghini, Simone; Abbate, Gabriele; Giusti, Alessandro; Paolillo, Antonio",,
Real-Time Semantic Segmentation in Natural Environments with SAM-assisted Sim-to-Real Domain Transfer,"Wang, Han; Mascaro, Ruben; Chli, Margarita; Teixeira, Lucas",,
OmniNxt: A Fully Open-source and Compact Aerial Robot with Omnidirectional Visual Perception,"LIU, Peize; XU, Yang; NING, Yan; XU, Hao; Feng, Chen; Shen, Shaojie",https://arxiv.org/abs/2403.20085,"Adopting omnidirectional Field of View (FoV) cameras in aerial robots vastly improves perception ability, significantly advancing aerial robotics's capabilities in inspection, reconstruction, and rescue tasks. However, such sensors also elevate system complexity, e.g., hardware design, and corresponding algorithm, which limits researchers from utilizing aerial robots with omnidirectional FoV in their research. To bridge this gap, we propose OmniNxt, a fully open-source aerial robotics platform with omnidirectional perception. We design a high-performance flight controller NxtPX4 and a multi-fisheye camera set for OmniNxt. Meanwhile, the compatible software is carefully devised, which empowers OmniNxt to achieve accurate localization and real-time dense mapping with limited computation resource occupancy. We conducted extensive real-world experiments to validate the superior performance of OmniNxt in practical applications. All the hardware and software are open-access at https://github.com/HKUST-Aerial-Robotics/OmniNxt, and we provide docker images of each crucial module in the proposed system. Project page: https://hkust-aerial-robotics.github.io/OmniNxt."
Fast and Communication-Efficient Multi-UAV Exploration Via Voronoi Partition on Dynamic Topological Graph,"Dong, Qianli; Xi, Haobo; Zhang, Shiyong; Bi, Qingchen; Li, Tianyi; Wang, Ziyu; Zhang, Xuebo",https://arxiv.org/abs/2408.05808,"Efficient data transmission and reasonable task allocation are important to improve multi-robot exploration efficiency. However, most communication data types typically contain redundant information and thus require massive communication volume. Moreover, exploration-oriented task allocation is far from trivial and becomes even more challenging for resource-limited unmanned aerial vehicles (UAVs). In this paper, we propose a fast and communication-efficient multi-UAV exploration method for exploring large environments. We first design a multi-robot dynamic topological graph (MR-DTG) consisting of nodes representing the explored and exploring regions and edges connecting nodes. Supported by MR-DTG, our method achieves efficient communication by only transferring the necessary information required by exploration planning. To further improve the exploration efficiency, a hierarchical multi-UAV exploration method is devised using MR-DTG. Specifically, the \emph{graph Voronoi partition} is used to allocate MR-DTG's nodes to the closest UAVs, considering the actual motion cost, thus achieving reasonable task allocation. To our knowledge, this is the first work to address multi-UAV exploration using \emph{graph Voronoi partition}. The proposed method is compared with a state-of-the-art method in simulations. The results show that the proposed method is able to reduce the exploration time and communication volume by up to 38.3\% and 95.5\%, respectively. Finally, the effectiveness of our method is validated in the real-world experiment with 6 UAVs. We will release the source code to benefit the community."
Multimodal Coherent Explanation Generation of Robot Failures,"Pramanick, Pradip; Rossi, Silvia",,
3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration,"HERAU, Quentin; Bennehar, Moussab; Moreau, Arthur; Piasco, Nathan; Roldao, Luis; tsishkou, dzmitry; Migniot, Cyrille; Vasseur, Pascal; Demonceaux, Cédric",https://arxiv.org/abs/2403.11577,"Reliable multimodal sensor fusion algorithms require accurate spatiotemporal calibration. Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new rendering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations. We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset."
Visual Attention Based Cognitive Human--Robot Collaboration for Pedicle Screw Placement in Robot-Assisted Orthopedic Surgery,"Chen, Chen; Zou, Qikai; Song, Yuhang; Song, Shiji; LI, Xiang",https://arxiv.org/abs/2405.09359,"Current orthopedic robotic systems largely focus on navigation, aiding surgeons in positioning a guiding tube but still requiring manual drilling and screw placement. The automation of this task not only demands high precision and safety due to the intricate physical interactions between the surgical tool and bone but also poses significant risks when executed without adequate human oversight. As it involves continuous physical interaction, the robot should collaborate with the surgeon, understand the human intent, and always include the surgeon in the loop. To achieve this, this paper proposes a new cognitive human-robot collaboration framework, including the intuitive AR-haptic human-robot interface, the visual-attention-based surgeon model, and the shared interaction control scheme for the robot. User studies on a robotic platform for orthopedic surgery are presented to illustrate the performance of the proposed method. The results demonstrate that the proposed human-robot collaboration framework outperforms full robot and full human control in terms of safety and ergonomics."
R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement,"Antonazzi, Michele; Luperto, Matteo; Borghese, N. Alberto; Basilico, Nicola",https://arxiv.org/abs/2403.11567,"We introduce a novel approach for scalable domain adaptation in cloud robotics scenarios where robots rely on third-party AI inference services powered by large pre-trained deep neural networks. Our method is based on a downstream proposal-refinement stage running locally on the robots, exploiting a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate performance degradation from domain shifts by adapting the object detection process to the target environment, focusing on relabeling, rescoring, and suppression of bounding-box proposals. Our method allows for local execution on robots, addressing the scalability challenges of domain adaptation without incurring significant computational costs. Real-world results on mobile service robots performing door detection show the effectiveness of the proposed method in achieving scalable domain adaptation."
ROBOVERINE: A human-inspired neural robotic process model of active visual search and scene grammar in naturalistic environments,"Grieben, Raul; Sehring, Stephan; Tekülve, Jan; Spencer, John P.; Schöner, Gregor",,
A Non-Homogeneity Mapless Navigation Based on Hierarchical Safe Reinforcement Learning in Dynamic Complex Environments,"Qin, Jianmin; Liu, Qingchen; Ma, Qichao; Wu, Zipeng; Qin, Jiahu",,
Potential Field-Based Online Path Planning for Robust Cable Routing,"Monguzzi, Andrea; Mantegna, Niccolò; Zanchettin, Andrea Maria; Rocco, Paolo",,
Position Control of a Low-Energy C-Core Reluctance Actuator in a Motion System,"Al Saaideh, Mohammad; al-rawashdeh, Yazan; Alatawneh, Natheer; Aljanaideh, Khaled; Al Janaideh, Mohammad",,
StratXplore: Strategic Novelty-seeking and Instruction-aligned Exploration for Vision and Language Navigation,"Gopinathan, Muraleekrishna; Abu-Khalaf, Jumana; Suter, David; Masek, Martin",https://arxiv.org/abs/2409.05593,"Embodied navigation requires robots to understand and interact with the environment based on given tasks. Vision-Language Navigation (VLN) is an embodied navigation task, where a robot navigates within a previously seen and unseen environment, based on linguistic instruction and visual inputs. VLN agents need access to both local and global action spaces; former for immediate decision making and the latter for recovering from navigational mistakes. Prior VLN agents rely only on instruction-viewpoint alignment for local and global decision making and back-track to a previously visited viewpoint, if the instruction and its current viewpoint mismatches. These methods are prone to mistakes, due to the complexity of the instruction and partial observability of the environment. We posit that, back-tracking is sub-optimal and agent that is aware of its mistakes can recover efficiently. For optimal recovery, exploration should be extended to unexplored viewpoints (or frontiers). The optimal frontier is a recently observed but unexplored viewpoint that aligns with the instruction and is novel. We introduce a memory-based and mistake-aware path planning strategy for VLN agents, called \textit{StratXplore}, that presents global and local action planning to select the optimal frontier for path correction. The proposed method collects all past actions and viewpoint features during navigation and then selects the optimal frontier suitable for recovery. Experimental results show this simple yet effective strategy improves the success rate on two VLN datasets with different task complexities."
DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions,"Kalwar, Sanket; UNGARALA, SRI MIHIR DEVAPI; Jain, Shruti; Monis, Aaron; konda, Krishna; Garg, Sourav; Krishna, Madhava",https://arxiv.org/abs/2310.04181,"Semantic segmentation in adverse weather scenarios is a critical task for autonomous driving systems. While foundation models have shown promise, the need for specialized adaptors becomes evident for handling more challenging scenarios. We introduce DiffPrompter, a novel differentiable visual and latent prompting mechanism aimed at expanding the learning capabilities of existing adaptors in foundation models. Our proposed $\nabla$HFC image processing block excels particularly in adverse weather conditions, where conventional methods often fall short. Furthermore, we investigate the advantages of jointly training visual and latent prompts, demonstrating that this combined approach significantly enhances performance in out-of-distribution scenarios. Our differentiable visual prompts leverage parallel and series architectures to generate prompts, effectively improving object segmentation tasks in adverse conditions. Through a comprehensive series of experiments and evaluations, we provide empirical evidence to support the efficacy of our approach. Project page at https://diffprompter.github.io."
Redefining Data Pairing for Motion Retargeting Leveraging a Human Body Prior,"FIGUERA MICHAL, XIYANA VEROSKA; Park, Soogeun; Ahn, Hyemin",,
An Adaptive Robotic Exoskeleton for Comprehensive Force-Controlled Hand Rehabilitation,"Wilhelm, Nikolas Jakob; Victor, Schaack; Leisching, Annick; Micheler, Carina M.; Haddadin, Sami; Burgkart, Rainer",,
Multi-Fingered Dragging of Unknown Objects and Orientations Using Distributed Tactile Information Through Vision-Transformer and LSTM,"Ueno, Takahisa; Funabashi, Satoshi; Ito, Hiroshi; Schmitz, Alexander; Kulkarni, Shardul; Ogata, Tetsuya; Sugano, Shigeki",,
A Soft Robotic System Automatically Learns Precise Agile Motions Without Model Information,"Bachhuber, Simon; Pawluchin, Alexander; Pal, Arka; Boblan, Ivo; Seel, Thomas",https://arxiv.org/abs/2408.03754,"Many application domains, e.g., in medicine and manufacturing, can greatly benefit from pneumatic Soft Robots (SRs). However, the accurate control of SRs has remained a significant challenge to date, mainly due to their nonlinear dynamics and viscoelastic material properties. Conventional control design methods often rely on either complex system modeling or time-intensive manual tuning, both of which require significant amounts of human expertise and thus limit their practicality. In recent works, the data-driven method, Automatic Neural ODE Control (ANODEC) has been successfully used to -- fully automatically and utilizing only input-output data -- design controllers for various nonlinear systems in silico, and without requiring prior model knowledge or extensive manual tuning. In this work, we successfully apply ANODEC to automatically learn to perform agile, non-repetitive reference tracking motion tasks in a real-world SR and within a finite time horizon. To the best of the authors' knowledge, ANODEC achieves, for the first time, performant control of a SR with hysteresis effects from only 30 seconds of input-output data and without any prior model knowledge. We show that for multiple, qualitatively different and even out-of-training-distribution reference signals, a single feedback controller designed by ANODEC outperforms a manually tuned PID baseline consistently. Overall, this contribution not only further strengthens the validity of ANODEC, but it marks an important step towards more practical, easy-to-use SRs that can automatically learn to perform agile motions from minimal experimental interaction time."
Object Instance Retrieval in Assistive Robotics: Leveraging Fine-Tuned SimSiam with Multi-View Images Based on 3D Semantic Map,"Sakaguchi, Taichi; Taniguchi, Akira; Hagiwara, Yoshinobu; El Hafi, Lotfi; Hasegawa, Shoichi; Taniguchi, Tadahiro",https://arxiv.org/abs/2404.09647,"Robots that assist humans in their daily lives should be able to locate specific instances of objects in an environment that match a user's desired objects. This task is known as instance-specific image goal navigation (InstanceImageNav), which requires a model that can distinguish different instances of an object within the same class. A significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ significantly, making it difficult to recognize and locate accurately. In this paper, we introduce a method called SimView, which leverages multi-view images based on a 3D semantic map of an environment and self-supervised learning using SimSiam to train an instance-identification model on-site. The effectiveness of our approach was validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning actual home environments. Our results demonstrate a 1.7-fold improvement in task accuracy compared with contrastive language-image pre-training (CLIP), a pre-trained multimodal contrastive learning method for object searching. This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks. The project website is https://emergentsystemlabstudent.github.io/MultiViewRetrieve/."
Optimizing Crowd-Aware Multi-Agent Path Finding through Local Broadcasting with Graph Neural Networks,"Pham, Phu; Bera, Aniket",,
An Origami-Inspired Pneumatic Continuum Module with Active Variable Stiffness,"Li, Zhuowen; Chen, Huaiyuan; Xu, Fan; Wang, Hesheng",,
Autonomous Guidewire Navigation in Dynamic Environments,"Scarponi, Valentina; Lecomte, François; Duprez, Michel; Nageotte, Florent; Cotin, Stephane",,
Exploring Latent Pathways: Enhancing the Interpretability of Autonomous Driving with a Variational Autoencoder,"BAIROUK, Anass; Maras, Mirjana; Herlin, Simon; Amini, Alexander; Blanchon, Marc; Hasani, Ramin; Chareyre, Patrick; Rus, Daniela",https://arxiv.org/abs/2404.01750,"Autonomous driving presents a complex challenge, which is usually addressed with artificial intelligence models that are end-to-end or modular in nature. Within the landscape of modular approaches, a bio-inspired neural circuit policy model has emerged as an innovative control module, offering a compact and inherently interpretable system to infer a steering wheel command from abstract visual features. Here, we take a leap forward by integrating a variational autoencoder with the neural circuit policy controller, forming a solution that directly generates steering commands from input camera images. By substituting the traditional convolutional neural network approach to feature extraction with a variational autoencoder, we enhance the system's interpretability, enabling a more transparent and understandable decision-making process.   In addition to the architectural shift toward a variational autoencoder, this study introduces the automatic latent perturbation tool, a novel contribution designed to probe and elucidate the latent features within the variational autoencoder. The automatic latent perturbation tool automates the interpretability process, offering granular insights into how specific latent variables influence the overall model's behavior. Through a series of numerical experiments, we demonstrate the interpretative power of the variational autoencoder-neural circuit policy model and the utility of the automatic latent perturbation tool in making the inner workings of autonomous driving systems more transparent."
"Interactive-FAR: Interactive, Fast and Adaptable Routing for Navigation Among Movable Obstacles in Complex Unknown Environments","He, Botao; Chen, Luke; Wang, Wenshan; Zhang, Ji; Fermuller, Cornelia; Aloimonos, Yiannis",,
Clutter-Aware Spill-Free Liquid Transport via Learned Dynamics,"Abderezaei, Ava; Pasricha, Anuj; Klausenstock, Alex; Roncone, Alessandro",https://arxiv.org/abs/2408.00215,"In this work, we present a novel algorithm to perform spill-free handling of open-top liquid-filled containers that operates in cluttered environments. By allowing liquid-filled containers to be tilted at higher angles and enabling motion along all axes of end-effector orientation, our work extends the reachable space and enhances maneuverability around obstacles, broadening the range of feasible scenarios. Our key contributions include: i) generating spill-free paths through the use of RRT* with an informed sampler that leverages container properties to avoid spill-inducing states (such as an upside-down container), ii) parameterizing the resulting path to generate spill-free trajectories through the implementation of a time parameterization algorithm, coupled with a transformer-based machine-learning model capable of classifying trajectories as spill-free or not. We validate our approach in real-world, obstacle-rich task settings using containers of various shapes and fill levels and demonstrate an extended solution space that is at least 3x larger than an existing approach."
Cooperative Modular Manipulation with Numerous Cable-Driven Robots for Assistive Construction and Gap Crossing,"Murphy, Kevin; Soares, João Carlos Virgolino; Yim, Justin K.; Nottage, Dustin; Soylemezoglu, Ahmet; Ramos, Joao",https://arxiv.org/abs/2403.13124,"Soldiers in the field often need to cross negative obstacles, such as rivers or canyons, to reach goals or safety. Military gap crossing involves on-site temporary bridges construction. However, this procedure is conducted with dangerous, time and labor intensive operations, and specialized machinery. We envision a scalable robotic solution inspired by advancements in force-controlled and Cable Driven Parallel Robots (CDPRs); this solution can address the challenges inherent in this transportation problem, achieving fast, efficient, and safe deployment and field operations. We introduce the embodied vision in Co3MaNDR, a solution to the military gap crossing problem, a distributed robot consisting of several modules simultaneously pulling on a central payload, controlling the cables' tensions to achieve complex objectives, such as precise trajectory tracking or force amplification. Hardware experiments demonstrate teleoperation of a payload, trajectory following, and the sensing and amplification of operators' applied physical forces during slow operations. An operator was shown to manipulate a 27.2 kg (60 lb) payload with an average force utilization of 14.5\% of its weight. Results indicate that the system can be scaled up to heavier payloads without compromising performance or introducing superfluous complexity. This research lays a foundation to expand CDPR technology to uncoordinated and unstable mobile platforms in unknown environments."
Driving Style Alignment for LLM-powered Driver Agent,"Yang, Ruoxuan; Zhang, Xinyue; Fernandez-Laaksonen, Anais; Ding, Xin; Gong, Jiangtao",https://arxiv.org/abs/2403.11368,"Recently, LLM-powered driver agents have demonstrated considerable potential in the field of autonomous driving, showcasing human-like reasoning and decision-making abilities.However, current research on aligning driver agent behaviors with human driving styles remains limited, partly due to the scarcity of high-quality natural language data from human driving behaviors.To address this research gap, we propose a multi-alignment framework designed to align driver agents with human driving styles through demonstrations and feedback. Notably, we construct a natural language dataset of human driver behaviors through naturalistic driving experiments and post-driving interviews, offering high-quality human demonstrations for LLM alignment. The framework's effectiveness is validated through simulation experiments in the CARLA urban traffic simulator and further corroborated by human evaluations. Our research offers valuable insights into designing driving agents with diverse driving styles.The implementation of the framework and details of the dataset can be found at the link."
Efficient Dynamic SLAM for Mobile Robots with Structured Point Clouds,"Lichtenfeld, Jonathan; Daun, Kevin; von Stryk, Oskar",,
ReLoc-Aligner : Orientation-aware Scene Descriptor for Re-Localization within a 3D Point Cloud Map,"Cho, SungJoon; Kim, Jun-Sik",,
Reconfigurable Soft Gripper Based on Eversion and Electroadhesion for Cluttered Environments,"Ragab, Dana; Rendon-Morales, Elizabeth; Althoefer, Kaspar; Godaba, Hareesh",,
Harnessing with Twisting: Single-Arm Deformable Linear Object Manipulation for Industrial Harnessing Task,"Zhang, Xiang; Lin, Hsien-Chung; Zhao, Yu; Tomizuka, Masayoshi",,
Robot Active Vision-Based Path Planning for Localization Improvement in Indoor Environments,"Barlakas, Sotirios; Alexiou, Dimitrios; Tsiakas, Kosmas; Katsatos, Dimitrios; Kostavelis, Ioannis; Giakoumis, Dimitris; Gasteratos, Antonios; Tzovaras, Dimitrios",,
Adaptive Reinforcement Learning for Robot Control,"Liu, Yu Tang; Nilaksh, Nilaksh; Ahmad, Aamir",https://arxiv.org/abs/2404.18713,"Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes. To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions. The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential. The agent is trained using a custom, highly parallelized simulator built on IsaacGym. We perform zero-shot transfer to fly the blimp in the real world to solve various tasks. We share our code at \url{https://github.com/robot-perception-group/adaptive\_agent/}."
MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for Multi-View 3D Object Detection,"Adeline, Michelle; Loo, Junn Yong; Baskaran, Vishnu Monn",https://arxiv.org/abs/2406.17654,"Multi-view 3D object detection is a crucial component of autonomous driving systems. Contemporary query-based methods primarily depend either on dataset-specific initialization of 3D anchors, introducing bias, or utilize dense attention mechanisms, which are computationally inefficient and unscalable. To overcome these issues, we present MDHA, a novel sparse query-based framework, which constructs adaptive 3D output proposals using hybrid anchors from multi-view, multi-scale input. Fixed 2D anchors are combined with depth predictions to form 2.5D anchors, which are projected to obtain 3D proposals. To ensure high efficiency, our proposed Anchor Encoder performs sparse refinement and selects the top-k anchors and features. Moreover, while existing multi-view attention mechanisms rely on projecting reference points to multiple images, our novel Circular Deformable Attention mechanism only projects to a single image but allows reference points to seamlessly attend to adjacent images, improving efficiency without compromising on performance. On the nuScenes val set, it achieves 46.4% mAP and 55.0% NDS with a ResNet101 backbone. MDHA significantly outperforms the baseline, where anchor proposals are modelled as learnable embeddings."
GSRM: Building Roadmaps for Query-Efficient and Near-Optimal Path Planning Using a Reaction Diffusion System,"Henkel, Christian; Toussaint, Marc; Hoenig, Wolfgang",,
Synergizing Morphological Computation and Generative Design: Automatic Synthesis of Tendon-Driven Grippers,"Zharkov, Kirill; Chaikovskii, Mikhail; Osipov, Yefim; Alshaowa, Rahaf; Borisov, Ivan; Kolyubin, Sergey",,
Evaluation and Design Recommendations for a Folding Morphing-wheg Robot for Nuclear Characterisation,"Murphy, Dominic; Giuliani, Manuel; Bremner, Paul",,
CSR: A Lightweight Crowdsourced Road Structure Reconstruction System for Autonomous Driving,"Wang, Huayou; Liu, Qingyao; Wu, Jiazheng; Liu, Kun; Ding, Chao; Xue, Changliang",,
The control strategy for vehicle transfer robots in RO/RO terminal environments,"Liu, Zhi; Xu, Yongkang; Zhang, Lin; Wang, Shoukun; Wang, Junzheng",,
SFTrack: A Robust Scale and Motion Adaptive Algorithm for Tracking Small and Fast Moving Objects,"Song, Inpyo; Lee, Jangwon",,
Asynchronous Microphone Array Calibration using Hybrid TDOA Information,"Zhang, Chengjie; Wang, Jiang; Kong, He",https://arxiv.org/abs/2403.05791,"Asynchronous microphone array calibration is a prerequisite for most audition robot applications. A popular solution to the above calibration problem is the batch form of Simultaneous Localisation and Mapping (SLAM), using the time difference of arrival measurements between two microphones (TDOA-M), and the robot (which serves as a moving sound source during calibration) odometry information. In this paper, we introduce a new form of measurement for microphone array calibration, i.e. the time difference of arrival between adjacent sound events (TDOA-S) with respect to the microphone channels. We propose to combine TDOA-S and TDOA-M, called hybrid TDOA, together with odometry measurements for bath SLAM-based calibration of asynchronous microphone arrays. Simulation and real-world experiment results consistently show that our method is more independent of microphone number, less sensitive to initial values (when using off-the-shelf algorithms such as Gauss-Newton iterations), and has better calibration accuracy and robustness under various TDOA noises. In addition, the simulation result demonstrates that our method has a lower Cramér-Rao lower bound (CRLB) for microphone parameters. To benefit the community, we open-source our code and data at https://github.com/zcj808/Hybrid-TDOA-Calib."
Inverse Submodular Maximization with Application to Human-in-the-Loop Multi-Robot Multi-Objective Coverage Control,"Shi, Guangyao; Sukhatme, Gaurav",https://arxiv.org/abs/2403.10991,"We consider a new type of inverse combinatorial optimization, Inverse Submodular Maximization (ISM), for human-in-the-loop multi-robot coordination.   Forward combinatorial optimization, defined as the process of solving a combinatorial problem given the reward (cost)-related parameters, is widely used in multi-robot coordination. In the standard pipeline, the reward (cost)-related parameters are designed offline by domain experts first and then these parameters are utilized for coordinating robots online. What if we need to change these parameters by non-expert human supervisors who watch over the robots during tasks to adapt to some new requirements? We are interested in the case where human supervisors can suggest what actions to take, and the robots need to change the internal parameters based on such suggestions. We study such problems from the perspective of inverse combinatorial optimization, i.e., the process of finding parameters given solutions to the problem. Specifically, we propose a new formulation for ISM, in which we aim to find a new set of parameters that minimally deviate from the current parameters and can make the greedy algorithm output actions the same as those suggested by humans. We show that such problems can be formulated as a Mixed Integer Quadratic Program (MIQP). However, MIQP involves exponentially many binary variables, making it intractable for the existing solver when the problem size is large. We propose a new algorithm under the Branch $\&$ Bound paradigm to solve such problems. In numerical simulations, we demonstrate how to use ISM in multi-robot multi-objective coverage control, and we show that the proposed algorithm achieves significant advantages in running time and peak memory usage compared to directly using an existing solver."
Insert-One: One-Shot Robust Visual-Force Servoing for Novel Object Insertion with 6-DoF Tracking,"Chang, Haonan; Boularias, Abdeslam; Jain, Siddarth",,
SOS-Match: Segmentation for Open-Set Robust Correspondence Search and Robot Localization in Unstructured Environments,"Thomas, Annika; Kinnari, Jouko; Lusk, Parker C.; Kondo, Kota; How, Jonathan",https://arxiv.org/abs/2401.04791,"We present SOS-Match, a novel framework for detecting and matching objects in unstructured environments. Our system consists of 1) a front-end mapping pipeline using a zero-shot segmentation model to extract object masks from images and track them across frames and 2) a frame alignment pipeline that uses the geometric consistency of object relationships to efficiently localize across a variety of conditions. We evaluate SOS-Match on the Batvik seasonal dataset which includes drone flights collected over a coastal plot of southern Finland during different seasons and lighting conditions. Results show that our approach is more robust to changes in lighting and appearance than classical image feature-based approaches or global descriptor methods, and it provides more viewpoint invariance than learning-based feature detection and description approaches. SOS-Match localizes within a reference map up to 46x faster than other feature-based approaches and has a map size less than 0.5% the size of the most compact other maps. SOS-Match is a promising new approach for landmark detection and correspondence search in unstructured environments that is robust to changes in lighting and appearance and is more computationally efficient than other approaches, suggesting that the geometric arrangement of segments is a valuable localization cue in unstructured environments. We release our datasets at https://acl.mit.edu/SOS-Match/."
Drones Guiding Drones: Cooperative Navigation of a Less-Equipped Micro Aerial Vehicle in Cluttered Environments,"Pritzl, Vaclav; Vrba, Matous; Stasinchuk, Yurii; Kratky, Vit; Horyna, Jiri; Stepan, Petr; Saska, Martin",https://arxiv.org/abs/2312.09786,Reliable deployment of Unmanned Aerial Vehicles (UAVs) in cluttered unknown environments requires accurate sensors for Global Navigation Satellite System (GNSS)-denied localization and obstacle avoidance. Such a requirement limits the usage of cheap and micro-scale vehicles with constrained payload capacity if industrial-grade reliability and precision are required. This paper investigates the possibility of offloading the necessity to carry heavy sensors to another member of the UAV team while preserving the desired capability of the smaller robot intended for exploring narrow passages. A novel cooperative guidance framework offloading the sensing requirements from a minimalistic secondary UAV to a superior primary UAV is proposed. The primary UAV constructs a dense occupancy map of the environment and plans collision-free paths for both UAVs to ensure reaching the desired secondary UAV's goals even in areas not accessible by the bigger robot. The primary UAV guides the secondary UAV to follow the planned path while tracking the UAV using Light Detection and Ranging (LiDAR)-based relative localization. The proposed approach was verified in real-world experiments with a heterogeneous team of a 3D LiDAR-equipped primary UAV and a micro-scale camera-equipped secondary UAV moving autonomously through unknown cluttered GNSS-denied environments with the proposed framework running fully on board the UAVs.
PACC: A Passive-Arm Approach for High-Payload Collaborative Carrying with Quadruped Robots Using Model Predictive Control,"Barasuol, Victor; Turrisi, Giulio; Schulze, Lucas; Suzano Medeiros, Vivian; Semini, Claudio",https://arxiv.org/abs/2403.19862,"In this paper, we introduce the concept of using passive arm structures with intrinsic impedance for robot-robot and human-robot collaborative carrying with quadruped robots. The concept is meant for a leader-follower task and takes a minimalist approach that focuses on exploiting the robots' payload capabilities and reducing energy consumption, without compromising the robot locomotion capabilities. We introduce a preliminary arm mechanical design and describe how to use its joint displacements to guide the robot's motion. To control the robot's locomotion, we propose a decentralized Model Predictive Controller that incorporates an approximation of the arm dynamics and the estimation of the external forces from the collaborative carrying. We validate the overall system experimentally by performing both robot-robot and human-robot collaborative carrying on a stair-like obstacle and on rough terrain."
Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression,"Acero, Fernando; Li, Zhibin (Alex)",https://arxiv.org/abs/2403.14328,"Recent advancements in reinforcement learning (RL) have led to remarkable achievements in robot locomotion capabilities. However, the complexity and ``black-box'' nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability. This paper introduces a novel approach to distill neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression. By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent ``glass-box'' models. We train expert neural network policies using RL and subsequently distill them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies. To address the inherent distribution shift challenge of behavioral cloning, we propose to use the Dataset Aggregation (DAgger) algorithm with a curriculum of episode-dependent alternation of actions between expert and distilled policies, to enable efficient distillation of feedback control policies. We evaluate our approach on various robot locomotion gaits -- walking, trotting, bounding, and pacing -- and study the importance of different observations in joint actions for distilled policies using various methods. We train neural expert policies for 205 hours of simulated experience and distill interpretable policies with only 10 minutes of simulated interaction for each gait using the proposed method."
Temporal- and Viewpoint-Invariant Registration for Under-Canopy Footage using Deep-Learning-based Birds Eye View Prediction,"Zhou, Jiawei; Mascaro, Ruben; Cadena Lerma, Cesar; Chli, Margarita; Teixeira, Lucas",,
Embodied Uncertainty-Aware Object Segmentation,"Fang, Xiaolin; Kaelbling, Leslie; Lozano-Perez, Tomas",https://arxiv.org/abs/2408.04760,"We introduce uncertainty-aware object instance segmentation (UncOS) and demonstrate its usefulness for embodied interactive segmentation. To deal with uncertainty in robot perception, we propose a method for generating a hypothesis distribution of object segmentation. We obtain a set of region-factored segmentation hypotheses together with confidence estimates by making multiple queries of large pre-trained models. This process can produce segmentation results that achieve state-of-the-art performance on unseen object segmentation problems. The output can also serve as input to a belief-driven process for selecting robot actions to perturb the scene to reduce ambiguity. We demonstrate the effectiveness of this method in real-robot experiments. Website: https://sites.google.com/view/embodied-uncertain-seg"
Structure-Invariant Range-Visual-Inertial Odometry,"Alberico, Ivan; Delaune, Jeff; Cioffi, Giovanni; Scaramuzza, Davide",https://arxiv.org/abs/2409.04633,"The Mars Science Helicopter (MSH) mission aims to deploy the next generation of unmanned helicopters on Mars, targeting landing sites in highly irregular terrain such as Valles Marineris, the largest canyons in the Solar system with elevation variances of up to 8000 meters. Unlike its predecessor, the Mars 2020 mission, which relied on a state estimation system assuming planar terrain, MSH requires a novel approach due to the complex topography of the landing site. This work introduces a novel range-visual-inertial odometry system tailored for the unique challenges of the MSH mission. Our system extends the state-of-the-art xVIO framework by fusing consistent range information with visual and inertial measurements, preventing metric scale drift in the absence of visual-inertial excitation (mono camera and constant velocity descent), and enabling landing on any terrain structure, without requiring any planar terrain assumption. Through extensive testing in image-based simulations using actual terrain structure and textures collected in Mars orbit, we demonstrate that our range-VIO approach estimates terrain-relative velocity meeting the stringent mission requirements, and outperforming existing methods."
Leveraging Computation of Expectation Models for Commonsense Affordance Estimation on 3D Scene Graphs,"Valdes Saucedo, Mario Alberto; Stathoulopoulos, Nikolaos; Patel, Akash; Kanellakis, Christoforos; Nikolakopoulos, George",https://arxiv.org/abs/2409.05392,"This article studies the commonsense object affordance concept for enabling close-to-human task planning and task optimization of embodied robotic agents in urban environments. The focus of the object affordance is on reasoning how to effectively identify object's inherent utility during the task execution, which in this work is enabled through the analysis of contextual relations of sparse information of 3D scene graphs. The proposed framework develops a Correlation Information (CECI) model to learn probability distributions using a Graph Convolutional Network, allowing to extract the commonsense affordance for individual members of a semantic class. The overall framework was experimentally validated in a real-world indoor environment, showcasing the ability of the method to level with human commonsense. For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/BDCMVx2GiQE"
DECAF: a Discrete-Event based Collaborative Human-Robot Framework for Furniture Assembly,"Giacomuzzo, Giulio; Terreran, Matteo; Jain, Siddarth; Romeres, Diego",https://arxiv.org/abs/2408.16125,"This paper proposes a task planning framework for collaborative Human-Robot scenarios, specifically focused on assembling complex systems such as furniture. The human is characterized as an uncontrollable agent, implying for example that the agent is not bound by a pre-established sequence of actions and instead acts according to its own preferences. Meanwhile, the task planner computes reactively the optimal actions for the collaborative robot to efficiently complete the entire assembly task in the least time possible. We formalize the problem as a Discrete Event Markov Decision Problem (DE-MDP), a comprehensive framework that incorporates a variety of asynchronous behaviors, human change of mind and failure recovery as stochastic events. Although the problem could theoretically be addressed by constructing a graph of all possible actions, such an approach would be constrained by computational limitations. The proposed formulation offers an alternative solution utilizing Reinforcement Learning to derive an optimal policy for the robot. Experiments where conducted both in simulation and on a real system with human subjects assembling a chair in collaboration with a 7-DoF manipulator."
Energy-Optimal Planning of Waypoint-Based UAV Missions - Does Minimum Distance Mean Minimum Energy?,"Michel, Nicolas; Patnaik, Ayush; Kong, Zhaodan; Lin, Xinfan",,
Learning High-level Semantic-Relational Concepts for SLAM,"Millan Romera, Jose Andres; Bavle, Hriday; Shaheer, Muhammad; Oswald, Martin R.; Voos, Holger; Sanchez-Lopez, Jose Luis",https://arxiv.org/abs/2310.00401,"Recent works on SLAM extend their pose graphs with higher-level semantic concepts like Rooms exploiting relationships between them, to provide, not only a richer representation of the situation/environment but also to improve the accuracy of its estimation. Concretely, our previous work, Situational Graphs (S-Graphs+), a pioneer in jointly leveraging semantic relationships in the factor optimization process, relies on semantic entities such as Planes and Rooms, whose relationship is mathematically defined. Nevertheless, there is no unique approach to finding all the hidden patterns in lower-level factor-graphs that correspond to high-level concepts of different natures. It is currently tackled with ad-hoc algorithms, which limits its graph expressiveness.   To overcome this limitation, in this work, we propose an algorithm based on Graph Neural Networks for learning high-level semantic-relational concepts that can be inferred from the low-level factor graph. Given a set of mapped Planes our algorithm is capable of inferring Room entities relating to the Planes. Additionally, to demonstrate the versatility of our method, our algorithm can infer an additional semantic-relational concept, i.e. Wall, and its relationship with its Planes. We validate our method in both simulated and real datasets demonstrating improved performance over two baseline approaches. Furthermore, we integrate our method into the S-Graphs+ algorithm providing improved pose and map accuracy compared to the baseline while further enhancing the scene representation."
Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking,"Zheng, Xinlong; Zhang, Xiaozhou; Xu, Donghao",https://arxiv.org/abs/2403.17234,"In this paper, we address a method that integrates reinforcement learning into the Monte Carlo tree search to boost online path planning under fully observable environments for automated parking tasks. Sampling-based planning methods under high-dimensional space can be computationally expensive and time-consuming. State evaluation methods are useful by leveraging the prior knowledge into the search steps, making the process faster in a real-time system. Given the fact that automated parking tasks are often executed under complex environments, a solid but lightweight heuristic guidance is challenging to compose in a traditional analytical way. To overcome this limitation, we propose a reinforcement learning pipeline with a Monte Carlo tree search under the path planning framework. By iteratively learning the value of a state and the best action among samples from its previous cycle's outcomes, we are able to model a value estimator and a policy generator for given states. By doing that, we build up a balancing mechanism between exploration and exploitation, speeding up the path planning process while maintaining its quality without using human expert driver data."
Risk-Aware Non-Myopic Motion Planner for Large-Scale Robotic Swarm Using CVaR Constraints,"Yang, Xuru; Hu, Yunze; Gao, Han; Ding, Kang; Li, Zhaoyang; Zhu, Pingping; Sun, Ying; Liu, Chang",https://arxiv.org/abs/2402.16690,"Swarm robotics has garnered significant attention due to its ability to accomplish elaborate and synchronized tasks. Existing methodologies for motion planning of swarm robotic systems mainly encounter difficulties in scalability and safety guarantee. To address these limitations, we propose a Risk-aware swarm mOtion planner using conditional ValuE at Risk (ROVER) that systematically navigates large-scale swarms through cluttered environments while ensuring safety. ROVER formulates a finite-time model predictive control (FTMPC) problem predicated upon the macroscopic state of the robot swarm represented by a Gaussian Mixture Model (GMM) and integrates conditional value-at-risk (CVaR) to ensure collision avoidance. The key component of ROVER is imposing a CVaR constraint on the distribution of the Signed Distance Function between the swarm GMM and obstacles in the FTMPC to enforce collision avoidance. Utilizing the analytical expression of CVaR of a GMM derived in this work, we develop a computationally efficient solution to solve the non-linear constrained FTMPC through sequential linear programming. Simulations and comparisons with representative benchmark approaches demonstrate the effectiveness of ROVER in flexibility, scalability, and risk mitigation."
Dynamics-Based Trajectory Planning for Vibration Suppression of a Flexible Long-Reach Robotic Manipulator System,"Chen, Anthony Siming; Lopez Pulgarin, Erwin Jose; Herrmann, Guido; Lanzon, Alexander; Carrasco, Joaquin; Lennox, Barry; Carrera-Knowles, Benji; Brotherhood, John; Sakaue, Tomoki; Kaiqiang, Zhang",,
Evaluation of Predictive Display for Teleoperated Driving using CARLA Simulator,"Kashwani, Fatima; Hassan, Bilal; Kong, Peng-Yong; Khonji, Majid; Dias, Jorge",,
Efficient Feature Mapping Using a Collaborative Team of AUVs,"Biggs, Benjamin; Stilwell, Daniel; Yetkin, Harun; McMahon, James",,
REPeat: A Real2Sim2Real Approach for Pre-acquisition of Soft Food Items in Robot-assisted Feeding,"Ha, Nayoung; Ye, Ruolin; Liu, Ziang; Sinha, Shubhangi; Madan, Rishabh; Bhattacharjee, Tapomayukh",,
MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment,"Xiong, Ziyan; chen, bo; Huang, Shiyu; Tu, Wei-Wei; He, Zhaofeng; Gao, Yang",https://arxiv.org/abs/2403.16015,"The advent of deep reinforcement learning (DRL) has significantly advanced the field of robotics, particularly in the control and coordination of quadruped robots. However, the complexity of real-world tasks often necessitates the deployment of multi-robot systems capable of sophisticated interaction and collaboration. To address this need, we introduce the Multi-agent Quadruped Environment (MQE), a novel platform designed to facilitate the development and evaluation of multi-agent reinforcement learning (MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex interactions between robots and objects, hierarchical policy structures, and challenging evaluation scenarios that reflect real-world applications. We present a series of collaborative and competitive tasks within MQE, ranging from simple coordination to complex adversarial interactions, and benchmark state-of-the-art MARL algorithms. Our findings indicate that hierarchical reinforcement learning can simplify task learning, but also highlight the need for advanced algorithms capable of handling the intricate dynamics of multi-agent interactions. MQE serves as a stepping stone towards bridging the gap between simulation and practical deployment, offering a rich environment for future research in multi-agent systems and robot learning. For open-sourced code and more details of MQE, please refer to https://ziyanx02.github.io/multiagent-quadruped-environment/ ."
SocialNav-FTI: Field-Theory-Inspired Social-aware Navigation Framework based on Human Behavior and Social Norms,"Lu, Siyi; Zhong, Ping; Ye, Shuqi; Chen, Bolei; Yu, Sheng; Liu, Run",,
MAkEable: Memory-centered and Affordance-based Task Execution Framework for Transferable Mobile Manipulation Skills,"Pohl, Christoph; Reister, Fabian; Peller-Konrad, Fabian; Asfour, Tamim",https://arxiv.org/abs/2401.16899,"To perform versatile mobile manipulation tasks in human-centered environments, the ability to efficiently transfer learned tasks and experiences from one robot to another or across different environments is key. In this paper, we present MAkEable, a versatile uni- and multi-manual mobile manipulation framework that facilitates the transfer of capabilities and knowledge across different tasks, environments, and robots. Our framework integrates an affordance-based task description into the memory-centric cognitive architecture of the ARMAR humanoid robot family, which supports the sharing of experiences and demonstrations for transfer learning. By representing mobile manipulation actions through affordances, i.e., interaction possibilities of the robot with its environment, we provide a unifying framework for the autonomous uni- and multi-manual manipulation of known and unknown objects in various environments. We demonstrate the applicability of the framework in real-world experiments for multiple robots, tasks, and environments. This includes grasping known and unknown objects, object placing, bimanual object grasping, memory-enabled skill transfer in a drawer opening scenario across two different humanoid robots, and a pouring task learned from human demonstration."
A Graph-Based Self-Calibration Technique for Cable-Driven Robots with Sagging Cable,"Dindarloo, Mohammadreza; Mirjalili, Amir Saman; Khalilpour, S. Ahmad; Khorrambakht, Rooholla; Weiss, Stephan; Taghirad, Hamid D.",,
AS-LIO: Spatial Overlap Guided Adaptive Sliding Window LiDAR-Inertial Odometry for Aggressive FOV Variation,"Zhang, Tianxiang; zhang, xuanxuan; Liao, Zongbo; Xia, Xin; Li, You",https://arxiv.org/abs/2408.11426,"LiDAR-Inertial Odometry (LIO) demonstrates outstanding accuracy and stability in general low-speed and smooth motion scenarios. However, in high-speed and intense motion scenarios, such as sharp turns, two primary challenges arise: firstly, due to the limitations of IMU frequency, the error in estimating significantly non-linear motion states escalates; secondly, drastic changes in the Field of View (FOV) may diminish the spatial overlap between LiDAR frame and pointcloud map (or between frames), leading to insufficient data association and constraint degradation.   To address these issues, we propose a novel Adaptive Sliding window LIO framework (AS-LIO) guided by the Spatial Overlap Degree (SOD). Initially, we assess the SOD between the LiDAR frames and the registered map, directly evaluating the adverse impact of current FOV variation on pointcloud alignment. Subsequently, we design an adaptive sliding window to manage the continuous LiDAR stream and control state updates, dynamically adjusting the update step according to the SOD. This strategy enables our odometry to adaptively adopt higher update frequency to precisely characterize trajectory during aggressive FOV variation, thus effectively reducing the non-linear error in positioning. Meanwhile, the historical constraints within the sliding window reinforce the frame-to-map data association, ensuring the robustness of state estimation. Experiments show that our AS-LIO framework can quickly perceive and respond to challenging FOV change, outperforming other state-of-the-art LIO frameworks in terms of accuracy and robustness."
3D Localization of Objects Buried within Granular Materials Using a Distributed 3-Axis Tactile Sensor,"Chen, Zhengqi; Versace, Elisabetta; Jamone, Lorenzo",,
Predicting Long-Term Human Behaviors in Discrete Representations via Physics-Guided Diffusion,"Zhang, Zhitian; Li, Anjian; Lim, Angelica; Chen, Mo",https://arxiv.org/abs/2405.19528,"Long-term human trajectory prediction is a challenging yet critical task in robotics and autonomous systems. Prior work that studied how to predict accurate short-term human trajectories with only unimodal features often failed in long-term prediction. Reinforcement learning provides a good solution for learning human long-term behaviors but can suffer from challenges in data efficiency and optimization. In this work, we propose a long-term human trajectory forecasting framework that leverages a guided diffusion model to generate diverse long-term human behaviors in a high-level latent action space, obtained via a hierarchical action quantization scheme using a VQ-VAE to discretize continuous trajectories and the available context. The latent actions are predicted by our guided diffusion model, which uses physics-inspired guidance at test time to constrain generated multimodal action distributions. Specifically, we use reachability analysis during the reverse denoising process to guide the diffusion steps toward physically feasible latent actions. We evaluate our framework on two publicly available human trajectory forecasting datasets: SFU-Store-Nav and JRDB, and extensive experimental results show that our framework achieves superior performance in long-term human trajectory forecasting."
From CAD to URDF: Co-Design of a Jet-Powered Humanoid Robot Including CAD Geometry,"Vanteddu, Punith Reddy; Nava, Gabriele; Bergonti, Fabio; L'Erario, Giuseppe; Paolino, Antonello; Pucci, Daniele",,
Perfecting Periodic Trajectory Tracking: Model Predictive Control with a Periodic Observer,"Pabon, Luis; Köhler, Johannes; Alora, John Irvin; Eberhard, Patrick Benito; Carron, Andrea; Zeilinger, Melanie N.; Pavone, Marco",https://arxiv.org/abs/2404.01550,"In Model Predictive Control (MPC), discrepancies between the actual system and the predictive model can lead to substantial tracking errors and significantly degrade performance and reliability. While such discrepancies can be alleviated with more complex models, this often complicates controller design and implementation. By leveraging the fact that many trajectories of interest are periodic, we show that perfect tracking is possible when incorporating a simple observer that estimates and compensates for periodic disturbances. We present the design of the observer and the accompanying tracking MPC scheme, proving that their combination achieves zero tracking error asymptotically, regardless of the complexity of the unmodelled dynamics. We validate the effectiveness of our method, demonstrating asymptotically perfect tracking on a high-dimensional soft robot with nearly 10,000 states and a fivefold reduction in tracking errors compared to a baseline MPC on small-scale autonomous race car experiments."
SoftNeRF: A self-modeling soft robot plugin for various tasks,"Shan, Jiwei; Li, Yirui; Feng, Qiyu; Wang, Hesheng",,
CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language Models,"Pueyo, Pablo; Montijano, Eduardo; Murillo, Ana Cristina; Schwager, Mac",https://arxiv.org/abs/2403.13467,"This paper introduces CLIPSwarm, a new algorithm designed to automate the modeling of swarm drone formations based on natural language. The algorithm begins by enriching a provided word, to compose a text prompt that serves as input to an iterative approach to find the formation that best matches the provided word. The algorithm iteratively refines formations of robots to align with the textual description, employing different steps for ""exploration"" and ""exploitation"". Our framework is currently evaluated on simple formation targets, limited to contour shapes. A formation is visually represented through alpha-shape contours and the most representative color is automatically found for the input word. To measure the similarity between the description and the visual representation of the formation, we use CLIP [1], encoding text and images into vectors and assessing their similarity. Subsequently, the algorithm rearranges the formation to visually represent the word more effectively, within the given constraints of available drones. Control actions are then assigned to the drones, ensuring robotic behavior and collision-free movement. Experimental results demonstrate the system's efficacy in accurately modeling robot formations from natural language descriptions. The algorithm's versatility is showcased through the execution of drone shows in photorealistic simulation with varying shapes. We refer the reader to the supplementary video for a visual reference of the results."
Synergistic Reinforcement and Imitation Learning for Vision-driven Autonomous Flight of UAV Along River,"Wang, Zihan; Li, Jianwen; Mahmoudian, Nina",https://arxiv.org/abs/2401.09332,"Vision-driven autonomous flight and obstacle avoidance of Unmanned Aerial Vehicles (UAVs) along complex riverine environments for tasks like rescue and surveillance requires a robust control policy, which is yet difficult to obtain due to the shortage of trainable riverine environment simulators. To easily verify the vision-based navigation controller performance for the river following task before real-world deployment, we developed a trainable photo-realistic dynamics-free riverine simulation environment using Unity. In this paper, we address the shortcomings that vanilla Reinforcement Learning (RL) algorithm encounters in learning a navigation policy within this partially observable, non-Markovian environment. We propose a synergistic approach that integrates RL and Imitation Learning (IL). Initially, an IL expert is trained on manually collected demonstrations, which then guides the RL policy training process. Concurrently, experiences generated by the RL agent are utilized to re-train the IL expert, enhancing its ability to generalize to unseen data. By leveraging the strengths of both RL and IL, this framework achieves a faster convergence rate and higher performance compared to pure RL, pure IL, and RL combined with static IL algorithms. The results validate the efficacy of the proposed method in terms of both task completion and efficiency. The code and trainable environments are available."
Model-based Policy Optimization using Symbolic World Model,"Gorodetsky, Andrey; Mironov, Konstantin; Panov, Aleksandr",https://arxiv.org/abs/2407.13518,"The application of learning-based control methods in robotics presents significant challenges. One is that model-free reinforcement learning algorithms use observation data with low sample efficiency. To address this challenge, a prevalent approach is model-based reinforcement learning, which involves employing an environment dynamics model. We suggest approximating transition dynamics with symbolic expressions, which are generated via symbolic regression. Approximation of a mechanical system with a symbolic model has fewer parameters than approximation with neural networks, which can potentially lead to higher accuracy and quality of extrapolation. We use a symbolic dynamics model to generate trajectories in model-based policy optimization to improve the sample efficiency of the learning algorithm. We evaluate our approach across various tasks within simulated environments. Our method demonstrates superior sample efficiency in these tasks compared to model-free and model-based baseline methods."
iMTSP: Solving Min-Max Multiple Traveling Salesman Problem with Imperative Learning,"Guo, Yifan; Ren, Zhongqiang; Wang, Chen",https://arxiv.org/abs/2405.00285,"This paper considers a Min-Max Multiple Traveling Salesman Problem (MTSP), where the goal is to find a set of tours, one for each agent, to collectively visit all the cities while minimizing the length of the longest tour. Though MTSP has been widely studied, obtaining near-optimal solutions for large-scale problems is still challenging due to its NP-hardness. Recent efforts in data-driven methods face challenges of the need for hard-to-obtain supervision and issues with high variance in gradient estimations, leading to slow convergence and highly suboptimal solutions. We address these issues by reformulating MTSP as a bilevel optimization problem, using the concept of imperative learning (IL). This involves introducing an allocation network that decomposes the MTSP into multiple single-agent traveling salesman problems (TSPs). The longest tour from these TSP solutions is then used to self-supervise the allocation network, resulting in a new self-supervised, bilevel, end-to-end learning framework, which we refer to as imperative MTSP (iMTSP). Additionally, to tackle the high-variance gradient issues during the optimization, we introduce a control variate-based gradient estimation algorithm. Our experiments showed that these innovative designs enable our gradient estimator to converge 20% faster than the advanced reinforcement learning baseline and find up to 80% shorter tour length compared with Google OR-Tools MTSP solver, especially in large-scale problems (e.g. 1000 cities and 15 agents)."
A visually-guided tactile exploration policy for 3D reconstruction and localization of sub-dermal tumors with surgical robotic palpation,"Bhattacharjee, Abhinaba; She, Yu; Anwar, Sohel; Uppuluri, Raghava",https://arxiv.org/abs/2408.13699,"Surgical scene understanding in Robot-assisted Minimally Invasive Surgery (RMIS) is highly reliant on visual cues and lacks tactile perception. Force-modulated surgical palpation with tactile feedback is necessary for localization, geometry/depth estimation, and dexterous exploration of abnormal stiff inclusions in subsurface tissue layers. Prior works explored surface-level tissue abnormalities or single layered tissue-tumor embeddings with more than 300 palpations for dense 2D stiffness mapping. Our approach focuses on 3D reconstructions of sub-dermal tumor surface profiles in multi-layered tissue (skin-fat-muscle) using a visually-guided novel tactile navigation policy. A robotic palpation probe with tri-axial force sensing was leveraged for tactile exploration of the phantom. From a surface mesh of the surgical region initialized from a depth camera, the policy explores a surgeon's region of interest through palpation, sampled from bayesian optimization. Each palpation includes contour following using a contact-safe impedance controller to trace the sub-dermal tumor geometry, until the underlying tumor-tissue boundary is reached. Projections of these contour following palpation trajectories allows 3D reconstruction of the subdermal tumor surface profile in less than 100 palpations. Our approach generates high-fidelity 3D surface reconstructions of rigid tumor embeddings in tissue layers with isotropic elasticities, although soft tumor geometries are yet to be explored."
High-Frequency Capacitive Sensing for Electrohydraulic Soft Actuators,"Vogt, Michel Ryan; Eberlein, Maximilian; Christoph, Clemens Claudio; Baumann, Felix; Bourquin, Fabrice; Wende, Wim; Schaub, Fabio; Kazemipour, Amirhossein; Katzschmann, Robert Kevin",https://arxiv.org/abs/2404.04071,"The need for compliant and proprioceptive actuators has grown more evident in pursuing more adaptable and versatile robotic systems. Hydraulically Amplified Self-Healing Electrostatic (HASEL) actuators offer distinctive advantages with their inherent softness and flexibility, making them promising candidates for various robotic tasks, including delicate interactions with humans and animals, biomimetic locomotion, prosthetics, and exoskeletons. This has resulted in a growing interest in the capacitive self-sensing capabilities of HASEL actuators to create miniature displacement estimation circuitry that does not require external sensors. However, achieving HASEL self-sensing for actuation frequencies above 1 Hz and with miniature high-voltage power supplies has remained limited. In this paper, we introduce the F-HASEL actuator, which adds an additional electrode pair used exclusively for capacitive sensing to a Peano-HASEL actuator. We demonstrate displacement estimation of the F-HASEL during high-frequency actuation up to 20 Hz and during external loading using miniaturized circuitry comprised of low-cost off-the-shelf components and a miniature high-voltage power supply. Finally, we propose a circuitry to estimate the displacement of multiple F-HASELs and demonstrate it in a wearable application to track joint rotations of a virtual reality user in real-time."
RECOVER: A Neuro-Symbolic Framework for Failure Detection and Recovery,"Cornelio, Cristina; Diab, Mohammed",https://arxiv.org/abs/2404.00756,"Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs."
PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning,"Lin, Fangze; He, Ying; Yu, Fei",https://arxiv.org/abs/2407.18569,"Personalized motion planning holds significant importance within urban automated driving, catering to the unique requirements of individual users. Nevertheless, prior endeavors have frequently encountered difficulties in simultaneously addressing two crucial aspects: personalized planning within intricate urban settings and enhancing planning performance through data utilization. The challenge arises from the expensive and limited nature of user data, coupled with the scene state space tending towards infinity. These factors contribute to overfitting and poor generalization problems during model training. Henceforth, we propose an instance-based transfer imitation learning approach. This method facilitates knowledge transfer from extensive expert domain data to the user domain, presenting a fundamental resolution to these issues. We initially train a pre-trained model using large-scale expert data. Subsequently, during the fine-tuning phase, we feed the batch data, which comprises expert and user data. Employing the inverse reinforcement learning technique, we extract the style feature distribution from user demonstrations, constructing the regularization term for the approximation of user style. In our experiments, we conducted extensive evaluations of the proposed method. Compared to the baseline methods, our approach mitigates the overfitting issue caused by sparse user data. Furthermore, we discovered that integrating the driving model with a differentiable nonlinear optimizer as a safety protection layer for end-to-end personalized fine-tuning results in superior planning performance."
A Tactile Lightweight Exoskeleton for Teleoperation; Design and Control Performance,"Forouhar, Moein; Sadeghian, Hamid; Pérez-Suay, Daniel; Naceri, Abdeldjallil; Haddadin, Sami",,
Neural Semantic Map-Learning for Autonomous Vehicles,"Herb, Markus; Navab, Nassir; Tombari, Federico",https://arxiv.org/abs/1801.04340,"In the event of sensor failure, autonomous vehicles need to safely execute emergency maneuvers while avoiding other vehicles on the road. To accomplish this, the sensor-failed vehicle must predict the future semantic behaviors of other drivers, such as lane changes, as well as their future trajectories given a recent window of past sensor observations. We address the first issue of semantic behavior prediction in this paper, which is a precursor to trajectory prediction, by introducing a framework that leverages the power of recurrent neural networks (RNNs) and graphical models. Our goal is to predict the future categorical driving intent, for lane changes, of neighboring vehicles up to three seconds into the future given as little as a one-second window of past LIDAR, GPS, inertial, and map data.   We collect real-world data containing over 20 hours of highway driving using an autonomous Toyota vehicle. We propose a composite RNN model by adopting the methodology of Structural Recurrent Neural Networks (RNNs) to learn factor functions and take advantage of both the high-level structure of graphical models and the sequence modeling power of RNNs, which we expect to afford more transparent modeling and activity than opaque, single RNN models. To demonstrate our approach, we validate our model using authentic interstate highway driving to predict the future lane change maneuvers of other vehicles neighboring our autonomous vehicle. We find that our composite Structural RNN outperforms baselines by as much as 12% in balanced accuracy metrics."
Learning Multi-Reference Frame Skills from Demonstration with Task-Parameterized Gaussian Processes,"Ramirez Montero, Mariano; Franzese, Giovanni; Kober, Jens; Della Santina, Cosimo",,
PEERNet: An End-to-End Profiling Tool for Real-Time Networked Robotic Systems,"Narayanan, Aditya; Kasibhatla, Pranav; Choi, Minkyu; Li, Po-han; Zhao, Ruihan; Chinchali, Sandeep",https://arxiv.org/abs/2409.06078,"Networked robotic systems balance compute, power, and latency constraints in applications such as self-driving vehicles, drone swarms, and teleoperated surgery. A core problem in this domain is deciding when to offload a computationally expensive task to the cloud, a remote server, at the cost of communication latency. Task offloading algorithms often rely on precise knowledge of system-specific performance metrics, such as sensor data rates, network bandwidth, and machine learning model latency. While these metrics can be modeled during system design, uncertainties in connection quality, server load, and hardware conditions introduce real-time performance variations, hindering overall performance. We introduce PEERNet, an end-to-end and real-time profiling tool for cloud robotics. PEERNet enables performance monitoring on heterogeneous hardware through targeted yet adaptive profiling of system components such as sensors, networks, deep-learning pipelines, and devices. We showcase PEERNet's capabilities through networked robotics tasks, such as image-based teleoperation of a Franka Emika Panda arm and querying vision language models using an Nvidia Jetson Orin. PEERNet reveals non-intuitive behavior in robotic systems, such as asymmetric network transmission and bimodal language model output. Our evaluation underscores the effectiveness and importance of benchmarking in networked robotics, demonstrating PEERNet's adaptability. Our code is open-source and available at github.com/UTAustin-SwarmLab/PEERNet."
SwinMTL: A Shared Architecture for Simultaneous Depth Estimation and Semantic Segmentation from Monocular Camera Images,"taghavi, pardis; Pandey, Gaurav; Langari, Reza",https://arxiv.org/abs/2403.10662,"This research paper presents an innovative multi-task learning framework that allows concurrent depth estimation and semantic segmentation using a single camera. The proposed approach is based on a shared encoder-decoder architecture, which integrates various techniques to improve the accuracy of the depth estimation and semantic segmentation task without compromising computational efficiency. Additionally, the paper incorporates an adversarial training component, employing a Wasserstein GAN framework with a critic network, to refine model's predictions. The framework is thoroughly evaluated on two datasets - the outdoor Cityscapes dataset and the indoor NYU Depth V2 dataset - and it outperforms existing state-of-the-art methods in both segmentation and depth estimation tasks. We also conducted ablation studies to analyze the contributions of different components, including pre-training strategies, the inclusion of critics, the use of logarithmic depth scaling, and advanced image augmentations, to provide a better understanding of the proposed framework. The accompanying source code is accessible at \url{https://github.com/PardisTaghavi/SwinMTL}."
Robust Backstepping Controller with Adaptive Sliding Mode Observer for a Tilt-Augmented Quadrotor With Uncertainty Using SO(3),"Seshasayanan, Sathyanarayanan; Sahoo, Soumya Ranjan",,
Arm-Constrained Curriculum Learning for Loco-Manipulation of the Wheel-legged Robot,"Jia, Yufei; Wang, Zifan; Wang, Haoyu; Li, Xueyang; Zhao, Haizhou; Zhou, Guyue; Shi, Lu; Ma, Jun; ZHOU, Jinni",https://arxiv.org/abs/2403.16535,"Incorporating a robotic manipulator into a wheel-legged robot enhances its agility and expands its potential for practical applications. However, the presence of potential instability and uncertainties presents additional challenges for control objectives. In this paper, we introduce an arm-constrained curriculum learning architecture to tackle the issues introduced by adding the manipulator. Firstly, we develop an arm-constrained reinforcement learning algorithm to ensure safety and stability in control performance. Additionally, to address discrepancies in reward settings between the arm and the base, we propose a reward-aware curriculum learning method. The policy is first trained in Isaac gym and transferred to the physical robot to do dynamic grasping tasks, including the door-opening task, fan-twitching task and the relay-baton-picking and following task. The results demonstrate that our proposed approach effectively controls the arm-equipped wheel-legged robot to master dynamic grasping skills, allowing it to chase and catch a moving object while in motion. Please refer to our website (https://acodedog.github.io/wheel-legged-loco-manipulation) for the code and supplemental videos."
Adapting Skills to Different Grasps: A Self-Supervised Approach,"Papagiannis, Georgios; Dreczkowski, Kamil; Vosylius, Vitalis; Johns, Edward",,
MonoForce: Self-supervised Learning of Physics-aware Model for Predicting Robot-terrain Interaction,"Agishev, Ruslan; Zimmermann, Karel; Kubelka, Vladimir; Pecka, Martin; Svoboda, Tomas",https://arxiv.org/abs/2309.09007,"While autonomous navigation of mobile robots on rigid terrain is a well-explored problem, navigating on deformable terrain such as tall grass or bushes remains a challenge. To address it, we introduce an explainable, physics-aware and end-to-end differentiable model which predicts the outcome of robot-terrain interaction from camera images, both on rigid and non-rigid terrain. The proposed MonoForce model consists of a black-box module which predicts robot-terrain interaction forces from onboard cameras, followed by a white-box module, which transforms these forces and a control signals into predicted trajectories, using only the laws of classical mechanics. The differentiable white-box module allows backpropagating the predicted trajectory errors into the black-box module, serving as a self-supervised loss that measures consistency between the predicted forces and ground-truth trajectories of the robot. Experimental evaluation on a public dataset and our data has shown that while the prediction capabilities are comparable to state-of-the-art algorithms on rigid terrain, MonoForce shows superior accuracy on non-rigid terrain such as tall grass or bushes. To facilitate the reproducibility of our results, we release both the code and datasets."
A 'MAP' to find high-performing soft robot designs: Traversing complex design spaces using MAP-elites and Topology Optimization,"Xie, Yue; Pinskier, Joshua; Liow, Lois; Howard, David; Iida, Fumiya",https://arxiv.org/abs/2407.07591,"Soft robotics has emerged as the standard solution for grasping deformable objects, and has proven invaluable for mobile robotic exploration in extreme environments. However, despite this growth, there are no widely adopted computational design tools that produce quality, manufacturable designs. To advance beyond the diminishing returns of heuristic bio-inspiration, the field needs efficient tools to explore the complex, non-linear design spaces present in soft robotics, and find novel high-performing designs. In this work, we investigate a hierarchical design optimization methodology which combines the strengths of topology optimization and quality diversity optimization to generate diverse and high-performance soft robots by evolving the design domain. The method embeds variably sized void regions within the design domain and evolves their size and position, to facilitating a richer exploration of the design space and find a diverse set of high-performing soft robots. We demonstrate its efficacy on both benchmark topology optimization problems and soft robotic design problems, and show the method enhances grasp performance when applied to soft grippers. Our method provides a new framework to design parts in complex design domains, both soft and rigid."
Strong Compliant Grasps Using a Cable-Driven Soft Gripper,"Xie, Gregory; Chin, Lillian; Kim, Byungchul; Holladay, Rachel; Rus, Daniela",,
BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning,"Huang, Tianle; Sontakke, Nitish Rajnish; Kannabiran, Niranjan Kumar; Essa, Irfan; Nikolaidis, Stefanos; Hong, Dennis; Ha, Sehoon",https://arxiv.org/abs/2310.10606,"Domain randomization (DR), which entails training a policy with randomized dynamics, has proven to be a simple yet effective algorithm for reducing the gap between simulation and the real world. However, DR often requires careful tuning of randomization parameters. Methods like Bayesian Domain Randomization (Bayesian DR) and Active Domain Randomization (Adaptive DR) address this issue by automating parameter range selection using real-world experience. While effective, these algorithms often require long computation time, as a new policy is trained from scratch every iteration. In this work, we propose Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune), which inherits the spirit of BayRn but aims to significantly accelerate the learning processes by fine-tuning from previously learned policy. This idea leads to a critical question: which previous policy should we use as a prior during fine-tuning? We investigated four different fine-tuning strategies and compared them against baseline algorithms in five simulated environments, ranging from simple benchmark tasks to more complex legged robot environments. Our analysis demonstrates that our method yields better rewards in the same amount of timesteps compared to vanilla domain randomization or Bayesian DR."
Towards Unconstrained Collision Injury Protection Data Sets: Initial Surrogate Experiments for the Human Hand,"Kirschner, Robin Jeanne; Yang, Jinyu; Elshani, Edonis; Micheler, Carina M.; Leibbrand, Tobias; Müller, Dirk; Glowalla, Claudio; Rajaei, Nader; Burgkart, Rainer; Haddadin, Sami",https://arxiv.org/abs/2408.06175,"Safety for physical human-robot interaction (pHRI) is a major concern for all application domains. While current standardization for industrial robot applications provide safety constraints that address the onset of pain in blunt impacts, these impact thresholds are difficult to use on edged or pointed impactors. The most severe injuries occur in constrained contact scenarios, where crushing is possible. Nevertheless, situations potentially resulting in constrained contact only occur in certain areas of a workspace and design or organisational approaches can be used to avoid them. What remains are risks to the human physical integrity caused by unconstrained accidental contacts, which are difficult to avoid while maintaining robot motion efficiency. Nevertheless, the probability and severity of injuries occurring with edged or pointed impacting objects in unconstrained collisions is hardly researched. In this paper, we propose an experimental setup and procedure using two pendulums modeling human hands and arms and robots to understand the injury potential of unconstrained collisions of human hands with edged objects. Pig feet are used as ex vivo surrogate samples - as these closely resemble the physiological characteristics of human hands - to create an initial injury database on the severity of injuries caused by unconstrained edged or pointed impacts. For the effective mass range of typical lightweight robots, the data obtained show low probabilities of injuries such as skin cuts or bone/tendon injuries in unconstrained collisions when the velocity is reduced to < 0.5 m/s. The proposed experimental setups and procedures should be complemented by sufficient human modeling and will eventually lead to a complete understanding of the biomechanical injury potential in pHRI."
User-customizable Shared Control for Fine Teleoperation via Virtual Reality,"Luo, Rui; Zolotas, Mark; Moore, Drake; Padir, Taskin",https://arxiv.org/abs/2403.13177,"Shared control can ease and enhance a human operator's ability to teleoperate robots, particularly for intricate tasks demanding fine control over multiple degrees of freedom. However, the arbitration process dictating how much autonomous assistance to administer in shared control can confuse novice operators and impede their understanding of the robot's behavior. To overcome these adverse side-effects, we propose a novel formulation of shared control that enables operators to tailor the arbitration to their unique capabilities and preferences. Unlike prior approaches to customizable shared control where users could indirectly modify the latent parameters of the arbitration function by issuing a feedback command, we instead make these parameters observable and directly editable via a virtual reality (VR) interface. We present our user-customizable shared control method for a teleoperation task in SE(3), known as the buzz wire game. A user study is conducted with participants teleoperating a robotic arm in VR to complete the game. The experiment spanned two weeks per subject to investigate longitudinal trends. Our findings reveal that users allowed to interactively tune the arbitration parameters across trials generalize well to adaptations in the task, exhibiting improvements in precision and fluency over direct teleoperation and conventional shared control."
Leader-Follower Cooperative Manipulation Under Spatio-Temporal Constraints,"Sewlia, Mayank; Verginis, Christos; Dimarogonas, Dimos V.",,
Exploiting Local Features and Range Images for Small Data Real-Time Point Cloud Semantic Segmentation,"Fusaro, Daniel; Mosco, Simone; Menegatti, Emanuele; Pretto, Alberto",,
Extending Task and Motion Planning with Feasibility Prediction: Towards Multi-Robot Manipulation Planning of Realistic Objects,"Ait Bouhsain, Smail; Alami, Rachid; Simeon, Thierry",,
Virtual model control for compliant reaching under uncertainties,"Zhang, Yi; Larby, Daniel; Iida, Fumiya; Forni, Fulvio",,
Safety-First Tracker: A Trajectory Planning Framework for Omnidirectional Robot Tracking,"Lin, Yue; Liu, Yang; Zhang, Pingping; CHEN, Xin; Wang, Dong; Lu, Huchuan",,
Data-Driven System Identification of Quadrotors Subject to Motor Delays,"Eschmann, Jonas; Albani, Dario; Loianno, Giuseppe",https://arxiv.org/abs/2404.07837,"Recently non-linear control methods like Model Predictive Control (MPC) and Reinforcement Learning (RL) have attracted increased interest in the quadrotor control community. In contrast to classic control methods like cascaded PID controllers, MPC and RL heavily rely on an accurate model of the system dynamics. The process of quadrotor system identification is notoriously tedious and is often pursued with additional equipment like a thrust stand. Furthermore, low-level details like motor delays which are crucial for accurate end-to-end control are often neglected. In this work, we introduce a data-driven method to identify a quadrotor's inertia parameters, thrust curves, torque coefficients, and first-order motor delay purely based on proprioceptive data. The estimation of the motor delay is particularly challenging as usually, the RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based method to estimate the latent time constant. Our approach only requires about a minute of flying data that can be collected without any additional equipment and usually consists of three simple maneuvers. Experimental results demonstrate the ability of our method to accurately recover the parameters of multiple quadrotors. It also facilitates the deployment of RL-based, end-to-end quadrotor control of a large quadrotor under harsh, outdoor conditions."
Formal and Efficient Guarantees for Robotic Contact Tasks using Reachset Conformance,"Tang, Chencheng; Althoff, Matthias",,
Co-RaL: Complementary Radar-Leg Odometry with 4-DoF Optimization and Rolling Contact,"Jung, Sangwoo; Yang, Wooseong; Kim, Ayoung",https://arxiv.org/abs/2407.05820,"Robust and accurate localization in challenging environments is becoming crucial for SLAM. In this paper, we propose a unique sensor configuration for precise and robust odometry by integrating chip radar and a legged robot. Specifically, we introduce a tightly coupled radar-leg odometry algorithm for complementary drift correction. Adopting the 4-DoF optimization and decoupled RANSAC to mmWave chip radar significantly enhances radar odometry beyond the existing method, especially z-directional even when using a single radar. For the leg odometry, we employ rolling contact modeling-aided forward kinematics, accommodating scenarios with the potential possibility of contact drift and radar failure. We evaluate our method by comparing it with other chip radar odometry algorithms using real-world datasets with diverse environments while the datasets will be released for the robotics community. https://github.com/SangwooJung98/Co-RaL-Dataset"
Map-Aware Human Pose Prediction for Robot Follow-Ahead,"Jiang, Qingyuan; Susam, Burak; Chao, Jun-Jee; Isler, Volkan",https://arxiv.org/abs/2403.13294,"In the robot follow-ahead task, a mobile robot is tasked to maintain its relative position in front of a moving human actor while keeping the actor in sight. To accomplish this task, it is important that the robot understand the full 3D pose of the human (since the head orientation can be different than the torso) and predict future human poses so as to plan accordingly. This prediction task is especially tricky in a complex environment with junctions and multiple corridors. In this work, we address the problem of forecasting the full 3D trajectory of a human in such environments. Our main insight is to show that one can first predict the 2D trajectory and then estimate the full 3D trajectory by conditioning the estimator on the predicted 2D trajectory. With this approach, we achieve results comparable or better than the state-of-the-art methods three times faster. As part of our contribution, we present a new dataset where, in contrast to existing datasets, the human motion is in a much larger area than a single room. We also present a complete robot system that integrates our human pose forecasting network on the mobile robot to enable real-time robot follow-ahead and present results from real-world experiments in multiple buildings on campus. Our project page, including supplementary material and videos, can be found at: https://qingyuan-jiang.github.io/iros2024_poseForecasting/"
DITTO: Demonstration Imitation by Trajectory Transformation,"Heppert, Nick; Argus, Maximilian; Welschehold, Tim; Brox, Thomas; Valada, Abhinav",https://arxiv.org/abs/2403.15203,"Teaching robots new skills quickly and conveniently is crucial for the broader adoption of robotic systems. In this work, we address the problem of one-shot imitation from a single human demonstration, given by an RGB-D video recording through a two-stage process. In the first stage which is offline, we extract the trajectory of the demonstration. This entails segmenting manipulated objects and determining their relative motion in relation to secondary objects such as containers. Subsequently, in the live online trajectory generation stage, we first \mbox{re-detect} all objects, then we warp the demonstration trajectory to the current scene, and finally, we trace the trajectory with the robot. To complete these steps, our method makes leverages several ancillary models, including those for segmentation, relative object pose estimation, and grasp prediction. We systematically evaluate different combinations of correspondence and re-detection methods to validate our design decision across a diverse range of tasks. Specifically, we collect demonstrations of ten different tasks including pick-and-place tasks as well as articulated object manipulation. Finally, we perform extensive evaluations on a real robot system to demonstrate the effectiveness and utility of our approach in real-world scenarios. We make the code publicly available at http://ditto.cs.uni-freiburg.de."
On the 3D trochoidal motion model of LiDAR sensors placed off-centered inside spherical mobile mapping systems,"Arzberger, Fabian; Nuechter, Andreas",,
DexSkills: Skill Segmentation Using Haptic Data for Learning Autonomous Long-Horizon Robotic Manipulation Tasks,"Mao, Xiaofeng; Giudici, Gabriele; Coppola, Claudio; Althoefer, Kaspar; Farkhatdinov, Ildar; Li, Zhibin (Alex); Jamone, Lorenzo",https://arxiv.org/abs/2405.03476,"Effective execution of long-horizon tasks with dexterous robotic hands remains a significant challenge in real-world problems. While learning from human demonstrations have shown encouraging results, they require extensive data collection for training. Hence, decomposing long-horizon tasks into reusable primitive skills is a more efficient approach. To achieve so, we developed DexSkills, a novel supervised learning framework that addresses long-horizon dexterous manipulation tasks using primitive skills. DexSkills is trained to recognize and replicate a select set of skills using human demonstration data, which can then segment a demonstrated long-horizon dexterous manipulation task into a sequence of primitive skills to achieve one-shot execution by the robot directly. Significantly, DexSkills operates solely on proprioceptive and tactile data, i.e., haptic data. Our real-world robotic experiments show that DexSkills can accurately segment skills, thereby enabling autonomous robot execution of a diverse range of tasks."
DECADE: Towards Designing Efficient-yet-Accurate Distance Estimation Modules for Collision Avoidance in Mobile Advanced Driver Assistance Systems,"Shahzad, Muhammad Zaeem; Hanif, Muhammad Abdullah; Shafique, Muhammad",,
Advancements in Radar Odometry,"Frosi, Matteo; Usuelli, Mirko; Matteucci, Matteo",https://arxiv.org/abs/2310.12729,"Radar odometry estimation has emerged as a critical technique in the field of autonomous navigation, providing robust and reliable motion estimation under various environmental conditions. Despite its potential, the complex nature of radar signals and the inherent challenges associated with processing these signals have limited the widespread adoption of this technology. This paper aims to address these challenges by proposing novel improvements to an existing method for radar odometry estimation, designed to enhance accuracy and reliability in diverse scenarios. Our pipeline consists of filtering, motion compensation, oriented surface points computation, smoothing, one-to-many radar scan registration, and pose refinement. The developed method enforces local understanding of the scene, by adding additional information through smoothing techniques, and alignment of consecutive scans, as a refinement posterior to the one-to-many registration. We present an in-depth investigation of the contribution of each improvement to the localization accuracy, and we benchmark our system on the sequences of the main datasets for radar understanding, i.e., the Oxford Radar RobotCar, MulRan, and Boreas datasets. The proposed pipeline is able to achieve superior results, on all scenarios considered and under harsh environmental constraints."
Tactile Odometry in Aerial Physical Interaction,"Schuster, Micha; Bredenbeck, Anton; Beitelschmidt, Michael; Hamaza, Salua",,
SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation,"Casao, Sara; Peña, Fernando; Sabater, Alberto; Castillón, Rosa; Suárez, Darío; Montijano, Eduardo; Murillo, Ana Cristina",https://arxiv.org/abs/2403.18033,"The increase in non-biodegradable waste is a worldwide concern. Recycling facilities play a crucial role, but their automation is hindered by the complex characteristics of waste recycling lines like clutter or object deformation. In addition, the lack of publicly available labeled data for these environments makes developing robust perception systems challenging. Our work explores the benefits of multimodal perception for object segmentation in real waste management scenarios. First, we present SpectralWaste, the first dataset collected from an operational plastic waste sorting facility that provides synchronized hyperspectral and conventional RGB images. This dataset contains labels for several categories of objects that commonly appear in sorting plants and need to be detected and separated from the main trash flow for several reasons, such as security in the management line or reuse. Additionally, we propose a pipeline employing different object segmentation architectures and evaluate the alternatives on our dataset, conducting an extensive analysis for both multimodal and unimodal alternatives. Our evaluation pays special attention to efficiency and suitability for real-time processing and demonstrates how HSI can bring a boost to RGB-only perception in these realistic industrial settings without much computational overhead."
Exploring Cognitive Load Dynamics in Human-Machine Interaction for Teleoperation: A User-Centric Perspective on Remote Operation System Design,"García Cárdenas, Juan José; Hei, Xiaoxuan; Tapus, Adriana",,
SmartPathfinder: Pushing the Limits of Heuristic Solutions for Vehicle Routing Problem with Drones Using Reinforcement Learning,"Imran, Navid Mohammad; Won, Myounggyu",https://arxiv.org/abs/2404.13068,"The Vehicle Routing Problem with Drones (VRPD) seeks to optimize the routing paths for both trucks and drones, where the trucks are responsible for delivering parcels to customer locations, and the drones are dispatched from these trucks for parcel delivery, subsequently being retrieved by the trucks. Given the NP-Hard complexity of VRPD, numerous heuristic approaches have been introduced. However, improving solution quality and reducing computation time remain significant challenges. In this paper, we conduct a comprehensive examination of heuristic methods designed for solving VRPD, distilling and standardizing them into core elements. We then develop a novel reinforcement learning (RL) framework that is seamlessly integrated with the heuristic solution components, establishing a set of universal principles for incorporating the RL framework with heuristic strategies in an aim to improve both the solution quality and computation speed. This integration has been applied to a state-of-the-art heuristic solution for VRPD, showcasing the substantial benefits of incorporating the RL framework. Our evaluation results demonstrated that the heuristic solution incorporated with our RL framework not only elevated the quality of solutions but also achieved rapid computation speeds, especially when dealing with extensive customer locations."
Real-Time Horizon Locking on Unmanned Surface Vehicles,"Kiefer, Benjamin; Zell, Andreas",,
Context-Aware GAN-based Image Retrieval for Coarse Localization of Autonomous Robots,"Swaminathan, Ruphan; Korupolu, Pradyot",,
Dynamic Object Catching with Quadruped Robot Front Legs,"Schakkal, André; Bellegarda, Guillaume; Ijspeert, Auke",,
Energy-efficient Trajectory Planning with Media Transition for a Hybrid Unmanned Aerial-Underwater Vehicle,"Miranda Pinheiro, Pedro; Alves Neto, Armando; G. Macharet, Douglas; Drews-Jr, Paulo",,
3D Global Path Planning for Walking Robots on Sparse Volumetric Maps,"Grosse Besselmann, Marvin; Häuselmann, Ramona; Mauch, Samuel; Puck, Lennart; Schnell, Tristan; Roennau, Arne; Dillmann, Rüdiger",,
End-to-end Learned Visual Odometry with Events and Frames,"Pellerito, Roberto; Cannici, Marco; Gehrig, Daniel; Belhadj, Joris; Dubois-Matra, Olivier; Casasco, Massimo; Scaramuzza, Davide",https://arxiv.org/abs/2309.09947,"Visual Odometry (VO) is crucial for autonomous robotic navigation, especially in GPS-denied environments like planetary terrains. To improve robustness, recent model-based VO systems have begun combining standard and event-based cameras. While event cameras excel in low-light and high-speed motion, standard cameras provide dense and easier-to-track features. However, the field of image- and event-based VO still predominantly relies on model-based methods and is yet to fully integrate recent image-only advancements leveraging end-to-end learning-based architectures. Seamlessly integrating the two modalities remains challenging due to their different nature, one asynchronous, the other not, limiting the potential for a more effective image- and event-based VO. We introduce RAMP-VO, the first end-to-end learned image- and event-based VO system. It leverages novel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders capable of fusing asynchronous events with image data, providing 8x faster inference and 33% more accurate predictions than existing solutions. Despite being trained only in simulation, RAMP-VO outperforms previous methods on the newly introduced Apollo and Malapert datasets, and on existing benchmarks, where it improves image- and event-based methods by 58.8% and 30.6%, paving the way for robust and asynchronous VO in space."
Opinion-based Strategy for Distributed Multi-Robot Task Allocation in Swarms of Robots,"Zhang, Ziqiao; Chen, Shengkang; Mayberry, Scott; Zhang, Fumin",,
Vinymap: a Vineyard Inspection and 3D Reconstruction Framework for Agricultural Robots,"Zarras, Ioannis; Mastrogeorgiou, Athanasios; Machairas, Konstantinos; Koutsoukis, Konstantinos; Papadopoulos, Evangelos",,
Robust and Safe Task-Driven Planning and Navigation for Heterogeneous Multi-Robot Teams with Uncertain Dynamics,"Pan, Tianyang; Verginis, Christos; Kavraki, Lydia",,
Learning Long-Horizon Predictions for Quadrotor Dynamics,"Rao, Pratyaksh; Saviolo, Alessandro; Castiglione Ferrari, Tommaso; Loianno, Giuseppe",https://arxiv.org/abs/2407.12964,"Accurate modeling of system dynamics is crucial for achieving high-performance planning and control of robotic systems. Although existing data-driven approaches represent a promising approach for modeling dynamics, their accuracy is limited to a short prediction horizon, overlooking the impact of compounding prediction errors over longer prediction horizons. Strategies to mitigate these cumulative errors remain underexplored. To bridge this gap, in this paper, we study the key design choices for efficiently learning long-horizon prediction dynamics for quadrotors. Specifically, we analyze the impact of multiple architectures, historical data, and multi-step loss formulation. We show that sequential modeling techniques showcase their advantage in minimizing compounding errors compared to other types of solutions. Furthermore, we propose a novel decoupled dynamics learning approach, which further simplifies the learning process while also enhancing the approach modularity. Extensive experiments and ablation studies on real-world quadrotor data demonstrate the versatility and precision of the proposed approach. Our outcomes offer several insights and methodologies for enhancing long-term predictive accuracy of learned quadrotor dynamics for planning and control."
Learning to Recover from Plan Execution Errors during Robot Manipulation: A Neuro-symbolic Approach,"Kalithasan, Namasivayam; Tuli, Arnav; Bindal, Vishal; Singh, Himanshu Gaurav; Singla, Parag; Paul, Rohan",https://arxiv.org/abs/2405.18948,"Automatically detecting and recovering from failures is an important but challenging problem for autonomous robots. Most of the recent work on learning to plan from demonstrations lacks the ability to detect and recover from errors in the absence of an explicit state representation and/or a (sub-) goal check function. We propose an approach (blending learning with symbolic search) for automated error discovery and recovery, without needing annotated data of failures. Central to our approach is a neuro-symbolic state representation, in the form of dense scene graph, structured based on the objects present within the environment. This enables efficient learning of the transition function and a discriminator that not only identifies failures but also localizes them facilitating fast re-planning via computation of heuristic distance function. We also present an anytime version of our algorithm, where instead of recovering to the last correct state, we search for a sub-goal in the original plan minimizing the total distance to the goal given a re-planning budget. Experiments on a physics simulator with a variety of simulated failures show the effectiveness of our approach compared to existing baselines, both in terms of efficiency as well as accuracy of our recovery mechanism."
Riemannian Flow Matching Policy for Robot Motion Learning,"Braun, Max; Jaquier, Noémie; Rozo, Leonel; Asfour, Tamim",https://arxiv.org/abs/2403.10672,"We introduce Riemannian Flow Matching Policies (RFMP), a novel model for learning and synthesizing robot visuomotor policies. RFMP leverages the efficient training and inference capabilities of flow matching methods. By design, RFMP inherits the strengths of flow matching: the ability to encode high-dimensional multimodal distributions, commonly encountered in robotic tasks, and a very simple and fast inference process. We demonstrate the applicability of RFMP to both state-based and vision-conditioned robot motion policies. Notably, as the robot state resides on a Riemannian manifold, RFMP inherently incorporates geometric awareness, which is crucial for realistic robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments, comparing its performance against Diffusion Policies. Although both approaches successfully learn the considered tasks, our results show that RFMP provides smoother action trajectories with significantly lower inference times."
Neural ODE-based Imitation Learning (NODE-IL): Data-Efficient Imitation Learning for Long-Horizon Multi-Skill Robot Manipulation,"Zhao, Shiyao; Xu, Yucheng; Kasaei, Mohammadreza; Li, Zhibin (Alex)",,
SE(3) Linear Parameter Varying Dynamical Systems for Globally Asymptotically Stable End-Effector Control,"Sun, Sunan; Figueroa, Nadia",https://arxiv.org/abs/2403.16366,"Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into an autonomous first-order DS that enables reactive responses to perturbations, while ensuring globally asymptotic stability at the target. However, the current LPV-DS framework is established on Euclidean data only and has not been applicable to broader robotic applications requiring pose control. In this paper we present an extension to the current LPV-DS framework, named Quaternion-DS, which efficiently learns a DS-based motion policy for orientation. Leveraging techniques from differential geometry and Riemannian statistics, our approach properly handles the non-Euclidean orientation data in quaternion space, enabling the integration with positional control, namely SE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is preserved. Through simulation and real robot experiments, we validate our method, demonstrating its ability to efficiently and accurately reproduce the original SE(3) trajectory while exhibiting strong robustness to perturbations in task space."
Automatic design of robot swarms that perform composite missions: an approach based on inverse reinforcement learning,"Szpirer, Jeanne; Garzón Ramos, David; Birattari, Mauro",,
"SwiftEagle: An Advanced Open-Source, Miniaturized FPGA UAS Platform with Dual DVS/Frame Camera for Cutting-Edge Low-Latency Autonomous Algorithms","Vogt, Christian; Jost, Michael; Magno, Michele",,
Bi-CL: A Reinforcement Learning Framework for Robots Coordination Through Bi-level Optimization,"Hu, Zechen; Shishika, Daigo; Xiao, Xuesu; Wang, Xuan",https://arxiv.org/abs/2404.14649,"In multi-robot systems, achieving coordinated missions remains a significant challenge due to the coupled nature of coordination behaviors and the lack of global information for individual robots. To mitigate these challenges, this paper introduces a novel approach, Bi-level Coordination Learning (Bi-CL), that leverages a bi-level optimization structure within a centralized training and decentralized execution paradigm. Our bi-level reformulation decomposes the original problem into a reinforcement learning level with reduced action space, and an imitation learning level that gains demonstrations from a global optimizer. Both levels contribute to improved learning efficiency and scalability. We note that robots' incomplete information leads to mismatches between the two levels of learning models. To address this, Bi-CL further integrates an alignment penalty mechanism, aiming to minimize the discrepancy between the two levels without degrading their training efficiency. We introduce a running example to conceptualize the problem formulation and apply Bi-CL to two variations of this example: route-based and graph-based scenarios. Simulation results demonstrate that Bi-CL can learn more efficiently and achieve comparable performance with traditional multi-agent reinforcement learning baselines for multi-robot coordination."
Pose Graph Optimization over Planar Unit Dual Quaternions: Improved Accuracy with Provably Convergent Riemannian Optimization,"Warke, William; Ramos, J Humberto; Ganesh, Prashant; Brink, Kevin; Hale, Matthew",https://arxiv.org/abs/2404.00010,"It is common in pose graph optimization (PGO) algorithms to assume that noise in the translations and rotations of relative pose measurements is uncorrelated. However, existing work shows that in practice these measurements can be highly correlated, which leads to degradation in the accuracy of PGO solutions that rely on this assumption. Therefore, in this paper we develop a novel algorithm derived from a realistic, correlated model of relative pose uncertainty, and we quantify the resulting improvement in the accuracy of the solutions we obtain relative to state-of-the-art PGO algorithms. Our approach utilizes Riemannian optimization on the planar unit dual quaternion (PUDQ) manifold, and we prove that it converges to first-order stationary points of a Lie-theoretic maximum likelihood objective. Then we show experimentally that, compared to state-of-the-art PGO algorithms, this algorithm produces estimation errors that are lower by 10% to 25% across several orders of magnitude of noise levels and graph sizes."
Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning,"Theile, Mirco; Cao, Hongpeng; Caccamo, Marco; Sangiovanni Vincentelli, Alberto",https://arxiv.org/abs/2403.12856,"In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance."
FEDORA: A Flying Event Dataset fOr Reactive behAvior,"Joshi, Amogh; Ponghiran, Wachirawit; Kosta, Adarsh Kumar; Nagaraj, Manish; Roy, Kaushik",https://arxiv.org/abs/2305.14392,"The ability of resource-constrained biological systems such as fruitflies to perform complex and high-speed maneuvers in cluttered environments has been one of the prime sources of inspiration for developing vision-based autonomous systems. To emulate this capability, the perception pipeline of such systems must integrate information cues from tasks including optical flow and depth estimation, object detection and tracking, and segmentation, among others. However, the conventional approach of employing slow, synchronous inputs from standard frame-based cameras constrains these perception capabilities, particularly during high-speed maneuvers. Recently, event-based sensors have emerged as low latency and low energy alternatives to standard frame-based cameras for capturing high-speed motion, effectively speeding up perception and hence navigation. For coherence, all the perception tasks must be trained on the same input data. However, present-day datasets are curated mainly for a single or a handful of tasks and are limited in the rate of the provided ground truths. To address these limitations, we present Flying Event Dataset fOr Reactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks, with raw data from frame-based cameras, event-based cameras, and Inertial Measurement Units (IMU), along with ground truths for depth, pose, and optical flow at a rate much higher than existing datasets."
Learning Deep Dynamical Systems using Stable Neural ODEs,"Sochopoulos, Andreas; Gienger, Michael; Vijayakumar, Sethu",https://arxiv.org/abs/2404.10622,"Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS). State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time. We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information. A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed. We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task."
Adaptive Control Barrier Functions for Near-Structure ROV Operations,"von Benzon, Malte; Marley, Mathias; Sørensen, Fredrik Fogh; Liniger, Jesper; Pedersen, Simon",,
V-PRISM: Probabilistic Mapping of Unknown Tabletop Scenes,"Wright, Herbert; Zhi, Weiming; Johnson-Roberson, Matthew; Hermans, Tucker",https://arxiv.org/abs/2403.08106,"The ability to construct concise scene representations from sensor input is central to the field of robotics. This paper addresses the problem of robustly creating a 3D representation of a tabletop scene from a segmented RGB-D image. These representations are then critical for a range of downstream manipulation tasks. Many previous attempts to tackle this problem do not capture accurate uncertainty, which is required to subsequently produce safe motion plans. In this paper, we cast the representation of 3D tabletop scenes as a multi-class classification problem. To tackle this, we introduce V-PRISM, a framework and method for robustly creating probabilistic 3D segmentation maps of tabletop scenes. Our maps contain both occupancy estimates, segmentation information, and principled uncertainty measures. We evaluate the robustness of our method in (1) procedurally generated scenes using open-source object datasets, and (2) real-world tabletop data collected from a depth camera. Our experiments show that our approach outperforms alternative continuous reconstruction approaches that do not explicitly reason about objects in a multi-class formulation."
Vision-Language Model-based Physical Reasoning for Robot Liquid Perception,"Lai, Wenqiang; Gao, Yuan; Lam, Tin Lun",https://arxiv.org/abs/2404.06904,"There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora. Grounding LLMs in the physical world remains an open challenge as they can only process textual input. Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various geometry and material. Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0% -- achieved by the best-performing vision-only variant -- to 86.0%."
Competitive Multi-Team Behavior in Dynamic Flight Scenarios,"Seyde, Tim Niklas; Lechner, Mathias; Rountree, Joshua; Rus, Daniela",,
LA-LIO: Robust Localizability-Aware LiDAR-Inertial Odometry for Challenging Scenes,"Huang, Junjie; Zhang, Yunzhou; Xu, Qingdong; Wu, Song; Liu, Jun; Wang, Guiyuan; Liu, Wei",,
Deep Learning-based Delay Compensation Framework For Teleoperated Wheeled Rovers on Soft Terrains,"Abubakar, Ahmad; Zweiri, Yahya; Yakubu, Mubarak; Alhammadi, Ruqqayya; Mohiuddin, Mohammed; Haddad, Abdel Gafoor; Dias, Jorge; Seneviratne, Lakmal",,
Avoiding Object Damage in Robotic Manipulation,"Aduh, Erica; Wang, Fan; Randle, Dylan Labatt; Wang, Kaiwen; Shah, Priyesh; Mitash, Chaitanya; Nambi, Manikantan",https://arxiv.org/abs/2203.02389,"Pushing objects through cluttered scenes is a challenging task, especially when the objects to be pushed have initially unknown dynamics and touching other entities has to be avoided to reduce the risk of damage. In this paper, we approach this problem by applying deep reinforcement learning to generate pushing actions for a robotic manipulator acting on a planar surface where objects have to be pushed to goal locations while avoiding other items in the same workspace. With the latent space learned from a depth image of the scene and other observations of the environment, such as contact information between the end effector and the object as well as distance to the goal, our framework is able to learn contact-rich pushing actions that avoid collisions with other objects. As the experimental results with a six degrees of freedom robotic arm show, our system is able to successfully push objects from start to end positions while avoiding nearby objects. Furthermore, we evaluate our learned policy in comparison to a state-of-the-art pushing controller for mobile robots and show that our agent performs better in terms of success rate, collisions with other objects, and continuous object contact in various scenarios."
Monocular 3D Reconstruction of Cheetahs in the Wild,"da Silva, Zico; Muramatsu, Naoya; Parkar, Zuhayr; Nicolls, Fred; Patel, Amir",,
Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning,"Hoque, Ryan; Mandlekar, Ajay Uday; Garrett, Caelan; Goldberg, Ken; Fox, Dieter",https://arxiv.org/abs/2204.07776,"In the peg insertion task, human pays attention to the seam between the peg and the hole and tries to fill it continuously with visual feedback. By imitating the human behavior, we design architectures with position and orientation estimators based on the seam representation for pose alignment, which proves to be general to the unseen peg geometries. By putting the estimators into the closed-loop control with reinforcement learning, we further achieve a higher or comparable success rate, efficiency, and robustness compared with the baseline methods. The policy is trained totally in simulation without any manual intervention. To achieve sim-to-real, a learnable segmentation module with automatic data collecting and labeling can be easily trained to decouple the perception and the policy, which helps the model trained in simulation quickly adapt to the real world with negligible effort. Results are presented in simulation and on a physical robot. Code, videos, and supplemental material are available at https://github.com/xieliang555/SFN.git"
SiSCo: Signal Synthesis for Effective Human-Robot Communication via Large Language Models,"Sonawani, Shubham; Weigend, Fabian Clemens; Ben Amor, Heni",,
Estimating Perceptual Uncertainty to Predict Robust Motion Plans,"Gupta, Arjun; Zhang, Michelle; Gupta, Saurabh",,
A Hybrid Vision/Force Control Strategy for Handheld Robotic Devices Enhancing Probe-Based Confocal Laser Endomicroscopy,"Choi, Ingu; Kim, Eunchan; Yang, Sungwook",,
SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation,"Reed, Alec; Crowe, Brendan; Albin, Doncey; Achey, Lorin; Hayes, Bradley; Heckman, Christoffer",https://arxiv.org/abs/2403.11985,"When exploring new areas, robotic systems generally exclusively plan and execute controls over geometry that has been directly measured. When entering space that was previously obstructed from view such as turning corners in hallways or entering new rooms, robots often pause to plan over the newly observed space. To address this we present SceneScene, a real-time 3D diffusion model for synthesizing 3D occupancy information from partial observations that effectively predicts these occluded or out of view geometries for use in future planning and control frameworks. SceneSense uses a running occupancy map and a single RGB-D camera to generate predicted geometry around the platform at runtime, even when the geometry is occluded or out of view. Our architecture ensures that SceneSense never overwrites observed free or occupied space. By preserving the integrity of the observed map, SceneSense mitigates the risk of corrupting the observed space with generative predictions. While SceneSense is shown to operate well using a single RGB-D camera, the framework is flexible enough to extend to additional modalities. SceneSense operates as part of any system that generates a running occupancy map `out of the box', removing conditioning from the framework. Alternatively, for maximum performance in new modalities, the perception backbone can be replaced and the model retrained for inference in new applications. Unlike existing models that necessitate multiple views and offline scene synthesis, or are focused on filling gaps in observed data, our findings demonstrate that SceneSense is an effective approach to estimating unobserved local occupancy information at runtime. Local occupancy predictions from SceneSense are shown to better represent the ground truth occupancy distribution during the test exploration trajectories than the running occupancy map."
DDS-SLAM: Dense Semantic Neural SLAM for Deforming Endoscopic Scenes,"Shan, Jiwei; Li, Yirui; Yang, Lujia; Feng, Qiyu; Wang, Hesheng",,
Nerve Block Target Localization and Needle Guidance for Autonomous Robotic Ultrasound Guided Regional Anesthesia,"Tyagi, Abhishek; Tyagi, Abhay; Kaur, Manpreet; Aggarwal, Richa; Soni, Kapil Dev; Sivaswamy, Jayanthi; Trikha, Anjan",https://arxiv.org/abs/2308.03717,"Visual servoing for the development of autonomous robotic systems capable of administering UltraSound (US) guided regional anesthesia requires real-time segmentation of nerves, needle tip localization and needle trajectory extrapolation. First, we recruited 227 patients to build a large dataset of 41,000 anesthesiologist annotated images from US videos of brachial plexus nerves and developed models to localize nerves in the US images. Generalizability of the best suited model was tested on the datasets constructed from separate US scanners. Using these nerve segmentation predictions, we define automated anesthesia needle targets by fitting an ellipse to the nerve contours. Next, we developed an image analysis tool to guide the needle toward their targets. For the segmentation of the needle, a natural RGB pre-trained neural network was first fine-tuned on a large US dataset for domain transfer and then adapted for the needle using a small dataset. The segmented needle trajectory angle is calculated using Radon transformation and the trajectory is extrapolated from the needle tip. The intersection of the extrapolated trajectory with the needle target guides the needle navigation for drug delivery. The needle trajectory average error was within acceptable range of 5 mm as per experienced anesthesiologists. The entire dataset has been released publicly for further study by the research community at https://github.com/Regional-US/"
A Novel Approach for Precise Tissue Tracking in Breast Lumpectomy,"Aliyari, Yeganeh; Afshar, Mehrnoosh; Wiebe, Ericka; Peiris, Lashan; Tavakoli, Mahdi",,
Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation,"Shamsah, Abdulaziz; Agarwal, Krishanu; Kousik, Shreyas; Zhao, Ye",https://arxiv.org/abs/2403.16485,"This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation. We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning. Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking. The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN. ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit. The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities."
Integrated 3DOF Trajectory Tracking Control for Underactuated Marine Surface Vehicles By Trajectory Linearization,"Sempertegui, Miguel; Zhu, J. Jim",,
Embodiment Randomization for Cross Embodiment Navigation,"Putta, Pranav; Aggarwal, Gunjan; Mottaghi, Roozbeh; Batra, Dhruv; Yokoyama, Naoki; Truong, Joanne; Majumdar, Arjun",,
An Optimization based Scheme for Real-time Transfer of Human Arm Motion to Robot Arm,"Yang, Zhelin; Bien, Seongjin; Nertinger, Simone; Naceri, Abdeldjallil; Haddadin, Sami",,
Autonomous Robotic Assembly: From Part Singulation to Precise Assembly,"Ota, Kei; Jha, Devesh; Jain, Siddarth; Yerazunis, William; Corcodel, Radu; Shukla, Yash; Bronars, Antonia; Romeres, Diego",https://arxiv.org/abs/2406.05331,"Imagine a robot that can assemble a functional product from the individual parts presented in any configuration to the robot. Designing such a robotic system is a complex problem which presents several open challenges. To bypass these challenges, the current generation of assembly systems is built with a lot of system integration effort to provide the structure and precision necessary for assembly. These systems are mostly responsible for part singulation, part kitting, and part detection, which is accomplished by intelligent system design. In this paper, we present autonomous assembly of a gear box with minimum requirements on structure. The assembly parts are randomly placed in a two-dimensional work environment for the robot. The proposed system makes use of several different manipulation skills such as sliding for grasping, in-hand manipulation, and insertion to assemble the gear box. All these tasks are run in a closed-loop fashion using vision, tactile, and Force-Torque (F/T) sensors. We perform extensive hardware experiments to show the robustness of the proposed methods as well as the overall system. See supplementary video at https://www.youtube.com/watch?v=cZ9M1DQ23OI."
ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition,"Li, Samuel; Bhagat, Sarthak; Campbell, Joseph; Xie, Yaqi; Kim, Woojun; Sycara, Katia; Stepputtis, Simon",https://arxiv.org/abs/2403.18062,"Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information - the object's name and the intended task - to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach's decomposition and reasoning pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate. Additional videos, experiments, code, and data are available on our project website: https://shapegrasp.github.io/."
RT-Grasp: Reasoning Tuning Robotic Grasping via Multi-modal Large Language Model,"Xu, Jinxuan; Jin, Shiyu; Lei, Yutian; Zhang, Yuqian; Zhang, Liangjun",,
Towards intelligent robotic sole deburring: from burrs identification to path planning,"Tafuro, Alessandra; Cacciani, Luigi; Zanchettin, Andrea Maria; Rocco, Paolo",,
Modular Meshed Ultra-Wideband Aided Inertial Navigation with Robust Anchor Calibration,"Jung, Roland; Santoro, Luca; Brunelli, Davide; Fontanelli, Daniele; Weiss, Stephan",https://arxiv.org/abs/2408.14081,"This paper introduces a generic filter-based state estimation framework that supports two state-decoupling strategies based on cross-covariance factorization. These strategies reduce the computational complexity and inherently support true modularity -- a perquisite for handling and processing meshed range measurements among a time-varying set of devices. In order to utilize these measurements in the estimation framework, positions of newly detected stationary devices (anchors) and the pairwise biases between the ranging devices are required. In this work an autonomous calibration procedure for new anchors is presented, that utilizes range measurements from multiple tags as well as already known anchors. To improve the robustness, an outlier rejection method is introduced. After the calibration is performed, the sensor fusion framework obtains initial beliefs of the anchor positions and dictionaries of pairwise biases, in order to fuse range measurements obtained from new anchors tightly-coupled. The effectiveness of the filter and calibration framework has been validated through evaluations on a recorded dataset and real-world experiments."
Context-Aware Conversation Adaptation for Human-Robot Interaction,"Su, Zhidong; Sheng, Weihua",,
2.23mm Diameter Continuum Tools for Suturing in Open Spina Bifida Repair,"Law, Arion; Nimal, Nillan; Kang, Paul Hoseok; Gondokaryono, Radian; Drake, James; Van Mieghem, Tim; Looi, Thomas",,
SPDAGG-TransNet: Integrating Symmetric Positive Definite Networks with Transformers for UAV-Human Action Recognition,"Akremi, Mohamed Sanim; neji, najett; Tabia, Hedi",,
A Tetherless Soft Robotic Wearable Haptic Human Machine Interface for Robot Teleoperation,"Thakur, Shilpa; Diaz Armas, Nathalia; Adegite, Joseph; Pandey, Ritwik; Mead, Joey; Rao, Pratap; Onal, Cagdas",,
FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting,"Gaspard, Clément; Passault, Grégoire; DANIEL, Mélodie; Ly, Olivier",https://arxiv.org/abs/2403.12589,"Designing a humanoid locomotion controller is challenging and classically split up in sub-problems. Footstep planning is one of those, where the sequence of footsteps is defined. Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem. In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*). However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters. In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep Reinforcement Learning (DRL) techniques, with very low computational requirements for on-line inference. Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps. In contrast, other methods necessitate the selection of a relevant discrete set of actions. Second, we propose a forecasting method, allowing to quickly estimate the number of footsteps required to reach different candidates of local targets. This approach relies on inherent computations made by the actor-critic DRL architecture. We demonstrate the validity of our approach with simulation results, and by a deployment on a kid-size humanoid robot during the RoboCup 2023 competition."
Real-time Robotic Flexible Needle Insertion In Deformable Living Organs Using Isolated Objective Constraint,"Ha, Thuc Long; Bert, Julien; Courtecuisse, Hadrien",,
Human-Robot Interaction Control for Multi-Mode Exosuit with Reinforcement Learning,"Huang, Kaizhen; XU, Jiajun; Zhang, Tianyi; Zhao, Mengcheng; Ji, Aihong; Song, Guoli; Li, Y.F.",,
Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies,"Wu, Bo; Lee, Bruce; Daniilidis, Kostas; Bucher, Bernadette; Matni, Nikolai",https://arxiv.org/abs/2403.18222,"Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git"
iDb-RRT: Sampling-based Kinodynamic Motion Planning with Motion Primitives and Trajectory Optimization,"Ortiz-Haro, Joaquim; Hoenig, Wolfgang; Hartmann, Valentin; Toussaint, Marc; Righetti, Ludovic",https://arxiv.org/abs/2403.10745,"Rapidly-exploring Random Trees (RRT) and its variations have emerged as a robust and efficient tool for finding collision-free paths in robotic systems. However, adding dynamic constraints makes the motion planning problem significantly harder, as it requires solving two-value boundary problems (computationally expensive) or propagating random control inputs (uninformative). Alternatively, Iterative Discontinuity Bounded A* (iDb-A*), introduced in our previous study, combines search and optimization iteratively. The search step connects short trajectories (motion primitives) while allowing a bounded discontinuity between the motion primitives, which is later repaired in the trajectory optimization step.   Building upon these foundations, in this paper, we present iDb-RRT, a sampling-based kinodynamic motion planning algorithm that combines motion primitives and trajectory optimization within the RRT framework. iDb-RRT is probabilistically complete and can be implemented in forward or bidirectional mode. We have tested our algorithm across a benchmark suite comprising 30 problems, spanning 8 different systems, and shown that iDb-RRT can find solutions up to 10x faster than previous methods, especially in complex scenarios that require long trajectories or involve navigating through narrow passages."
Multi-Agent Vulcan: An Information-Driven Multi-Agent Path Finding Approach,"Olkin, Jake; Parimi, Viraj; Williams, Brian",,
Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps,"Zhang, Hengyuan; Paz, David; Guo, Yuliang; Das, Arun; Huang, Xinyu; Karsten, Haug; Christensen, Henrik Iskov; Ren, Liu",https://arxiv.org/abs/2408.01471,"Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors-Standard Definition (SD) maps-in the development of online vectorized HD map representations. We first examine the integration of prototypical rasterized SD map representations into various online mapping architectures. Furthermore, to identify lightweight strategies, we extend the OpenLane-V2 dataset with OpenStreetMaps and evaluate the benefits of graphical SD map representations. A key finding from designing SD map integration components is that SD map encoders are model agnostic and can be quickly adapted to new architectures that utilize bird's eye view (BEV) encoders. Our results show that making use of SD maps as priors for the online mapping task can significantly speed up convergence and boost the performance of the online centerline perception task by 30% (mAP). Furthermore, we show that the introduction of the SD maps leads to a reduction of the number of parameters in the perception and reasoning task by leveraging SD map graphs while improving the overall performance. Project Page: https://henryzhangzhy.github.io/sdhdmap/."
The subtle line between personalization and user manipulation in a European regulatory perspective. A proposal for a technology-assessment methodology for Artificial Intelligence Systems,"Bertolini, Andrea",,
Dynamic Throwing with Robotic Material Handling Machines,"Werner, Lennart; Nan, Fang; Eyschen, Pol; Spinelli, Filippo Alberto; Yang, Hongyi; Hutter, Marco",https://arxiv.org/abs/2405.19001,"Automation of hydraulic material handling machinery is currently limited to semi-static pick-and-place cycles. Dynamic throwing motions which utilize the passive joints, can greatly improve time efficiency as well as increase the dumping workspace. In this work, we use Reinforcement Learning (RL) to design dynamic controllers for material handlers with underactuated arms as commonly used in logistics. The controllers are tested both in simulation and in real-world experiments on a 12-ton test platform. The method is able to exploit the passive joints of the gripper to perform dynamic throwing motions. With the proposed controllers, the machine is able to throw individual objects to targets outside the static reachability zone with good accuracy for its practical applications. The work demonstrates the possibility of using RL to perform highly dynamic tasks with heavy machinery, suggesting a potential for improving the efficiency and precision of autonomous material handling tasks."
Revisiting Reward Design and Evaluation for Robust Humanoid Standing and Walking,"van Marum, Bart; shrestha, aayam; Duan, Helei; Dugar, Pranay; Dao, Jeremy; Fern, Alan",https://arxiv.org/abs/2404.19173,"A necessary capability for humanoid robots is the ability to stand and walk while rejecting natural disturbances. Recent progress has been made using sim-to-real reinforcement learning (RL) to train such locomotion controllers, with approaches differing mainly in their reward functions. However, prior works lack a clear method to systematically test new reward functions and compare controller performance through repeatable experiments. This limits our understanding of the trade-offs between approaches and hinders progress. To address this, we propose a low-cost, quantitative benchmarking method to evaluate and compare the real-world performance of standing and walking (SaW) controllers on metrics like command following, disturbance recovery, and energy efficiency. We also revisit reward function design and construct a minimally constraining reward function to train SaW controllers. We experimentally verify that our benchmarking framework can identify areas for improvement, which can be systematically addressed to enhance the policies. We also compare our new controller to state-of-the-art controllers on the Digit humanoid robot. The results provide clear quantitative trade-offs among the controllers and suggest directions for future improvements to the reward functions and expansion of the benchmarks."
Pilot Study for a Robot-Assisted Timed Up and Go Assessment,"Story, Matthew; AIT BELAID, Khaoula; Di Nuovo, Alessandro; Magistro, Daniele; Vagnetti, Roberto; Zecca, Massimiliano",,
Automatic Spatial Calibration of Near-Field MIMO Radar With Respect to Optical Sensors,"Wirth, Vanessa; Bräunig, Johanna; Khouri, Danti; Gutsche, Florian; Vossiek, Martin; Weyrich, Tim; Stamminger, Marc",https://arxiv.org/abs/2403.10981,"Despite an emerging interest in MIMO radar, the utilization of its complementary strengths in combination with optical depth sensors has so far been limited to far-field applications, due to the challenges that arise from mutual sensor calibration in the near field. In fact, most related approaches in the autonomous industry propose target-based calibration methods using corner reflectors that have proven to be unsuitable for the near field. In contrast, we propose a novel, joint calibration approach for optical RGB-D sensors and MIMO radars that is designed to operate in the radar's near-field range, within decimeters from the sensors. Our pipeline consists of a bespoke calibration target, allowing for automatic target detection and localization, followed by the spatial calibration of the two sensor coordinate systems through target registration. We validate our approach using two different depth sensing technologies from the optical domain. The experiments show the efficiency and accuracy of our calibration for various target displacements, as well as its robustness of our localization in terms of signal ambiguities."
Reducing Cognitive Load in Teleoperating Swarms of Robots through a Data-Driven Shared Control Approach,"Turco, Enrico; Castellani, Chiara; Bo, Valerio; Pacchierotti, Claudio; Prattichizzo, Domenico; Lisini Baldi, Tommaso",,
Visual Loop Closure Detection with Thorough Temporal and Spatial Context Exploitation,"Li, Jiaxin; Wang, Zan; Di, Huijun; Li, Jian; Liang, Wei",,
Performing Efficient and Safe Deformable Package Transport Operations Using Suction Cups,"Shukla, Rishabh; Yu, Zeren; Moode, Samrudh; Manyar, Omey Mohan; Wang, Fan; Mayya, Siddharth; Gupta, Satyandra K.",,
Learning Generalizable Tool-use Skills through Trajectory Generation,"Qi, Carl; Wu, Yilin; Yu, Lifan; Liu, Haoyue; Jiang, Bowen; Lin, Xingyu; Held, David",https://arxiv.org/abs/2310.00156,"Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of tool point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model on four different challenging deformable object manipulation tasks, using demonstration data from only one tool per task. The model generalizes to various novel tools, significantly outperforming baselines. We further test our trained policy in the real world with unseen tools, where it achieves the performance comparable to human. Additional materials can be found on our project website: https://sites.google.com/view/toolgen."
Sim2real Cattle Joint Estimation in 3D pointclouds,"Okour, Mohammad; Alempijevic, Alen; Falque, Raphael",,
Learning to Walk and Fly with Adversarial Motion Priors,"L'Erario, Giuseppe; Hanover, Drew; Romero, Angel; Song, Yunlong; Nava, Gabriele; Viceconte, Paolo Maria; Pucci, Daniele; Scaramuzza, Davide",https://arxiv.org/abs/2309.12784,"Robot multimodal locomotion encompasses the ability to transition between walking and flying, representing a significant challenge in robotics. This work presents an approach that enables automatic smooth transitions between legged and aerial locomotion. Leveraging the concept of Adversarial Motion Priors, our method allows the robot to imitate motion datasets and accomplish the desired task without the need for complex reward functions. The robot learns walking patterns from human-like gaits and aerial locomotion patterns from motions obtained using trajectory optimization. Through this process, the robot adapts the locomotion scheme based on environmental feedback using reinforcement learning, with the spontaneous emergence of mode-switching behavior. The results highlight the potential for achieving multimodal locomotion in aerial humanoid robotics through automatic control of walking and flying modes, paving the way for applications in diverse domains such as search and rescue, surveillance, and exploration missions. This research contributes to advancing the capabilities of aerial humanoid robots in terms of versatile locomotion in various environments."
Model Predictive Control for Frenet-Cartesian Trajectory Tracking of a Tricycle Kinematic Automated Guided Vehicle,"Subash, Akash John; Kloeser, Daniel; Frey, Jonathan; Reiter, Rudolf; Diehl, Moritz; Bohlmann, Karsten",,
EMBOSR: Embodied Spatial Reasoning for Enhanced Situated Question Answering in 3D Scenes,"Hao, Yu; Yang, Fan; Fang, Nicholas; Liu, Yu-Shen",,
BEVRender:Vision-based Cross-view Vehicle Registration in Off-road GNSS-denied Environment,"Lihong, Jin; Dong, Wei; Kaess, Michael",https://arxiv.org/abs/2405.09001,"We introduce BEVRender, a novel learning-based approach for the localization of ground vehicles in Global Navigation Satellite System (GNSS)-denied off-road scenarios. These environments are typically challenging for conventional vision-based state estimation due to the lack of distinct visual landmarks and the instability of vehicle poses. To address this, BEVRender generates high-quality local bird's eye view (BEV) images of the local terrain. Subsequently, these images are aligned with a geo-referenced aerial map via template-matching to achieve accurate cross-view registration. Our approach overcomes the inherent limitations of visual inertial odometry systems and the substantial storage requirements of image-retrieval localization strategies, which are susceptible to drift and scalability issues, respectively. Extensive experimentation validates BEVRender's advancement over existing GNSS-denied visual localization methods, demonstrating notable enhancements in both localization accuracy and update frequency. The code for BEVRender will be made available soon."
Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model,"Wang, Jin; Laurenzi, Arturo; Tsagarakis, Nikos",https://arxiv.org/abs/2408.08282,"Enabling humanoid robots to perform autonomously loco-manipulation in unstructured environments is crucial and highly challenging for achieving embodied intelligence. This involves robots being able to plan their actions and behaviors in long-horizon tasks while using multi-modality to perceive deviations between task execution and high-level planning. Recently, large language models (LLMs) have demonstrated powerful planning and reasoning capabilities for comprehension and processing of semantic information through robot control tasks, as well as the usability of analytical judgment and decision-making for multi-modal inputs. To leverage the power of LLMs towards humanoid loco-manipulation, we propose a novel language-model based framework that enables robots to autonomously plan behaviors and low-level execution under given textual instructions, while observing and correcting failures that may occur during task execution. To systematically evaluate this framework in grounding LLMs, we created the robot 'action' and 'sensing' behavior library for task planning, and conducted mobile manipulation tasks and experiments in both simulated and real environments using the CENTAURO robot, and verified the effectiveness and application of this approach in robotic tasks with autonomous behavioral planning."
SAVOR: Sonar-Aided Visual Odometry and Reconstruction for Autonomous Underwater Vehicles,"Coffelt, Jeremy Paul; Kampmann, Peter; Wehbe, Bilal",,
Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments,"Hsu, Christopher D.; Chaudhari, Pratik",https://arxiv.org/abs/2406.07431,"We study pursuit-evasion games in highly occluded urban environments, e.g. tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic targets on the ground. We show that we can build a neural radiance field (NeRF) representation of the city -- online -- using RGB and depth images from different vantage points. This representation is used to calculate the information gain to both explore unknown parts of the city and track the targets -- thereby giving a completely first-principles approach to actively tracking dynamic targets. We demonstrate, using a custom-built simulator using Open Street Maps data of Philadelphia and New York City, that we can explore and locate 20 stationary targets within 300 steps. This is slower than a greedy baseline which which does not use active perception. But for dynamic targets that actively hide behind occlusions, we show that our approach maintains, at worst, a tracking error of 200m; the greedy baseline can have a tracking error as large as 600m. We observe a number of interesting properties in the scout's policies, e.g., it switches its attention to track a different target periodically, as the quality of the NeRF representation improves over time, the scout also becomes better in terms of target tracking."
Portable robot for needle insertion assistance to femoral artery,"Cheng, Zhuoqi; Mány, Bence; Jørgensen, Kasper Balsby; An, Siheon; Jensen, Marcus Leander; Thulstrup, Richard; Frost, Habib; Savarimuthu, Thiusius Rajeeth; Huldt, Olof",,
Autonomous localization of multiple ionizing radiation sources using miniature single-layer Compton cameras onboard a group of micro aerial vehicles,"Werner, Michal; Baca, Tomas; Stibinger, Petr; Doubravova, Daniela; Solc, Jaroslav; Rusnak, Jan; Saska, Martin",,
NeuralFloors++: Consistent Street-Level Scene Generation From BEV Semantic Maps,"Musat, Valentina; De Martini, Daniele; Gadd, Matthew; Newman, Paul",,
Parametric Synthesis of Compliant Joints for Impact Robust Shaftless Leg Mechanisms,"Rakshin, Egor; Ogureckiy, Dmitriy; Borisov, Ivan; Kolyubin, Sergey",,
MEMROC: Multi-Eye to Mobile RObot Calibration,"Allegro, Davide; Terreran, Matteo; Ghidoni, Stefano",,
ContactHandover: Contact-Guided Robot-to-Human Object Handover,"Wang, Zixi; Liu, Zeyi; Ouporov, Nicolas; Song, Shuran",https://arxiv.org/abs/2404.01402,"Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that our system achieves better visibility and reachability of human contacts to the receiver compared to several baselines. More results can be found on https://clairezixiwang.github.io/ContactHandover.github.io"
Long-horizon Visual Action based Food Acquisition,"Bhaskar, Amisha; Liu, Rui; Sharma, Vishnu D.; Shi, Guangyao; Tokekar, Pratap",https://arxiv.org/abs/2403.12876,"Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves. The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table. Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semi-solid and deformable foods. This study introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid, semisolid, and deformable foods. Long-horizon refers to the goal of ""clearing the bowl"" by sequentially acquiring the food from the bowl. LAVA employs a hierarchical policy for long-horizon food acquisition tasks. The framework uses high-level policy to determine primitives by leveraging ScoopNet. At the mid-level, LAVA finds parameters for primitives using vision. To carry out sequential plans in the real world, LAVA delegates action execution which is driven by Low-level policy that uses parameters received from mid-level policy and behavior cloning ensuring precise trajectory execution. We validate our approach on complex real-world acquisition trials involving granular, liquid, semisolid, and deformable food types along with fruit chunks and soup acquisition. Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89 +/- 4% and generalizes across realistic plate variations such as different positions, varieties, and amount of food in the bowl. Code, datasets, videos, and supplementary materials can be found on our website."
Semantic Belief Behavior Graph: Enabling Autonomous Robot Inspection in Unknown Environments,"Ginting, Muhammad Fadhil; Fan, David D; Kim, Sung-Kyun; Kochenderfer, Mykel; Agha-mohammadi, Ali-akbar",https://arxiv.org/abs/2401.17191,"This paper addresses the problem of autonomous robotic inspection in complex and unknown environments. This capability is crucial for efficient and precise inspections in various real-world scenarios, even when faced with perceptual uncertainty and lack of prior knowledge of the environment. Existing methods for real-world autonomous inspections typically rely on predefined targets and waypoints and often fail to adapt to dynamic or unknown settings. In this work, we introduce the Semantic Belief Behavior Graph (SB2G) framework as a novel approach to semantic-aware autonomous robot inspection. SB2G generates a control policy for the robot, featuring behavior nodes that encapsulate various semantic-based policies designed for inspecting different classes of objects. We design an active semantic search behavior to guide the robot in locating objects for inspection while reducing semantic information uncertainty. The edges in the SB2G encode transitions between these behaviors. We validate our approach through simulation and real-world urban inspections using a legged robotic platform. Our results show that SB2G enables a more efficient inspection policy, exhibiting performance comparable to human-operated inspections."
QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding,"Mehan, Yash; Gupta, Kumaraditya; Jayanti, Rohit; Govil, Anirudh; Garg, Sourav; Krishna, Madhava",https://arxiv.org/abs/2404.06442,"Understanding the structural organisation of 3D indoor scenes in terms of rooms is often accomplished via floorplan extraction. Robotic tasks such as planning and navigation require a semantic understanding of the scene as well. This is typically achieved via object-level semantic segmentation. However, such methods struggle to segment out topological regions like ""kitchen"" in the scene. In this work, we introduce a two-step pipeline. First, we extract a topological map, i.e., floorplan of the indoor scene using a novel multi-channel occupancy representation. Then, we generate CLIP-aligned features and semantic labels for every room instance based on the objects it contains using a self-attention transformer. Our language-topology alignment supports natural language querying, e.g., a ""place to cook"" locates the ""kitchen"". We outperform the current state-of-the-art on room segmentation by ~20% and room classification by ~12%. Our detailed qualitative analysis and ablation studies provide insights into the problem of joint structural and semantic 3D scene understanding."
Formalization of Temporal and Spatial Constraints of Bimanual Manipulation Categories,"Krebs, Franziska; Asfour, Tamim",,
Single-Agent Actor Critic for Decentralized Cooperative Driving,"Yan, Shengchao; König, Lukas Maximilian; Burgard, Wolfram",https://arxiv.org/abs/2403.11914,"Active traffic management with autonomous vehicles offers the potential for reduced congestion and improved traffic flow. However, developing effective algorithms for real-world scenarios requires overcoming challenges related to infinite-horizon traffic flow and partial observability. To address these issues and further decentralize traffic management, we propose an asymmetric actor-critic model that learns decentralized cooperative driving policies for autonomous vehicles using single-agent reinforcement learning. By employing attention neural networks with masking, our approach efficiently manages real-world traffic dynamics and partial observability, eliminating the need for predefined agents or agent-specific experience buffers in multi-agent reinforcement learning. Extensive evaluations across various traffic scenarios demonstrate our method's significant potential in improving traffic flow at critical bottleneck points. Moreover, we address the challenges posed by conservative autonomous vehicle driving behaviors that adhere strictly to traffic rules, showing that our cooperative policy effectively alleviates potential slowdowns without compromising safety."
The design of a sensorized laryngoscope training system for pediatric intubation,"Hou, Ningzhe; He, Liang; Albini, Alessandro; Halamek, Louis; Maiolino, Perla",,
The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning,"Sferrazza, Carmelo; Seo, Younggyo; Liu, Hao; Lee, Youngwoon; Abbeel, Pieter",https://arxiv.org/abs/2311.00924,"Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learning a multimodal policy. Code and videos of the experiments are available at https://sferrazza.cc/m3l_site."
Adaptive Social Force Window Planner with Reinforcement Learning,"Martini, Mauro; Perez-Higueras, Noe; Ostuni, Andrea; Chiaberge, Marcello; Caballero, Fernando; Merino, Luis",https://arxiv.org/abs/2404.13678,"Human-aware navigation is a complex task for mobile robots, requiring an autonomous navigation system capable of achieving efficient path planning together with socially compliant behaviors. Social planners usually add costs or constraints to the objective function, leading to intricate tuning processes or tailoring the solution to the specific social scenario. Machine Learning can enhance planners' versatility and help them learn complex social behaviors from data. This work proposes an adaptive social planner, using a Deep Reinforcement Learning agent to dynamically adjust the weighting parameters of the cost function used to evaluate trajectories. The resulting planner combines the robustness of the classic Dynamic Window Approach, integrated with a social cost based on the Social Force Model, and the flexibility of learning methods to boost the overall performance on social navigation tasks. Our extensive experimentation on different environments demonstrates the general advantage of the proposed method over static cost planners."
CoBL-Diffusion: Diffusion-Based Conditional Robot Planning in Dynamic Environments Using Control Barrier and Lyapunov Functions,"Mizuta, Kazuki; Leung, Karen",https://arxiv.org/abs/2406.05309,"Equipping autonomous robots with the ability to navigate safely and efficiently around humans is a crucial step toward achieving trusted robot autonomy. However, generating robot plans while ensuring safety in dynamic multi-agent environments remains a key challenge. Building upon recent work on leveraging deep generative models for robot planning in static environments, this paper proposes CoBL-Diffusion, a novel diffusion-based safe robot planner for dynamic environments. CoBL-Diffusion uses Control Barrier and Lyapunov functions to guide the denoising process of a diffusion model, iteratively refining the robot control sequence to satisfy the safety and stability constraints. We demonstrate the effectiveness of the proposed model using two settings: a synthetic single-agent environment and a real-world pedestrian dataset. Our results show that CoBL-Diffusion generates smooth trajectories that enable the robot to reach goal locations while maintaining a low collision rate with dynamic obstacles."
Diff-Control: A Stateful Diffusion-based Policy for Imitation Learning,"Liu, Xiao; Zhou, Yifan; Weigend, Fabian Clemens; Sonawani, Shubham; Ikemoto, Shuhei; Ben Amor, Heni",https://arxiv.org/abs/2404.12539,"While imitation learning provides a simple and effective framework for policy learning, acquiring consistent actions during robot execution remains a challenging task. Existing approaches primarily focus on either modifying the action representation at data curation stage or altering the model itself, both of which do not fully address the scalability of consistent action generation. To overcome this limitation, we introduce the Diff-Control policy, which utilizes a diffusion-based model to learn the action representation from a state-space modeling viewpoint. We demonstrate that we can reduce diffusion-based policies' uncertainty by making it stateful through a Bayesian formulation facilitated by ControlNet, leading to improved robustness and success rates. Our experimental results demonstrate the significance of incorporating action statefulness in policy learning, where Diff-Control shows improved performance across various tasks. Specifically, Diff-Control achieves an average success rate of 72% and 84% on stateful and dynamic tasks, respectively. Project page: https://github.com/ir-lab/Diff-Control"
Predicting Interaction Shape of Soft Continuum Robots using Deep Visual Models,"Huang, Yunqi; Alkayas, Abdulaziz Y.; Shi, Jialei; Renda, Federico; Wurdemann, Helge Arne; George Thuruthel, Thomas",,
TRAVERSE: Traffic-Responsive Autonomous Vehicle Experience & Rare-event Simulation for Enhanced safety,"Thalapanane, Sandeep; Senthil Kumar, Sandip Sharan; Appiya Dilipkumar Peethambari, Guru Nandhan; Sri hari, Sourang; Zheng, Laura; Poveda, Julio; Lin, Ming C.",https://arxiv.org/abs/2407.09466,"Data for training learning-enabled self-driving cars in the physical world are typically collected in a safe, normal environment. Such data distribution often engenders a strong bias towards safe driving, making self-driving cars unprepared when encountering adversarial scenarios like unexpected accidents. Due to a dearth of such adverse data that is unrealistic for drivers to collect, autonomous vehicles can perform poorly when experiencing such rare events. This work addresses much-needed research by having participants drive a VR vehicle simulator going through simulated traffic with various types of accidental scenarios. It aims to understand human responses and behaviors in simulated accidents, contributing to our understanding of driving dynamics and safety. The simulation framework adopts a robust traffic simulation and is rendered using the Unity Game Engine. Furthermore, the simulation framework is built with portable, light-weight immersive driving simulator hardware, lowering the resource barrier for studies in autonomous driving research.   Keywords: Rare Events, Traffic Simulation, Autonomous Driving, Virtual Reality, User Studies"
Multimodal Failure Prediction for Vision-based Manipulation Tasks with Camera Faults,"Ma, Yuliang; Liu, Jingyi; Mamaev, Ilshat; Morozov, Andrey",,
Camera Pose Estimation from Bounding Boxes,"Vavra, Vaclav; Sattler, Torsten; Kukelova, Zuzana",https://arxiv.org/abs/1708.00965,"Research in Simultaneous Localization And Mapping (SLAM) is increasingly moving towards richer world representations involving objects and high level features that enable a semantic model of the world for robots, potentially leading to a more meaningful set of robot-world interactions. Many of these advances are grounded in state-of-the-art computer vision techniques primarily developed in the context of image-based benchmark datasets, leaving several challenges to be addressed in adapting them for use in robotics. In this paper, we derive a formulation for Simultaneous Localization And Mapping (SLAM) that uses dual quadrics as 3D landmark representations, and show how 2D bounding boxes (such as those typically obtained from visual object detection systems) can directly constrain the quadric parameters. Our paper demonstrates how to jointly estimate the robot pose and dual quadric parameters in factor graph based SLAM with a general perspective camera, and covers the use-cases of a robot moving with a monocular camera with and without the availability of additional depth information."
PINSAT: Parallelized Interleaving of Graph Search and Trajectory Optimization for Kinodynamic Motion Planning,"Natarajan, Ramkumar; Mukherjee, Shohin; Choset, Howie; Likhachev, Maxim",https://arxiv.org/abs/2401.08948,"Trajectory optimization is a widely used technique in robot motion planning for letting the dynamics and constraints on the system shape and synthesize complex behaviors. Several previous works have shown its benefits in high-dimensional continuous state spaces and under differential constraints. However, long time horizons and planning around obstacles in non-convex spaces pose challenges in guaranteeing convergence or finding optimal solutions. As a result, discrete graph search planners and sampling-based planers are preferred when facing obstacle-cluttered environments. A recently developed algorithm called INSAT effectively combines graph search in the low-dimensional subspace and trajectory optimization in the full-dimensional space for global kinodynamic planning over long horizons. Although INSAT successfully reasoned about and solved complex planning problems, the numerous expensive calls to an optimizer resulted in large planning times, thereby limiting its practical use. Inspired by the recent work on edge-based parallel graph search, we present PINSAT, which introduces systematic parallelization in INSAT to achieve lower planning times and higher success rates, while maintaining significantly lower costs over relevant baselines. We demonstrate PINSAT by evaluating it on 6 DoF kinodynamic manipulation planning with obstacles."
Test-Time Certifiable Self-Supervision to Bridge the Sim2Real Gap in Event-Based Satellite Pose Estimation,"Jawaid, Mohsi; Talak, Rajat; Latif, Yasir; Carlone, Luca; Chin, Tat-Jun",https://arxiv.org/abs/2409.06240,"Deep learning plays a critical role in vision-based satellite pose estimation. However, the scarcity of real data from the space environment means that deep models need to be trained using synthetic data, which raises the Sim2Real domain gap problem. A major cause of the Sim2Real gap are novel lighting conditions encountered during test time. Event sensors have been shown to provide some robustness against lighting variations in vision-based pose estimation. However, challenging lighting conditions due to strong directional light can still cause undesirable effects in the output of commercial off-the-shelf event sensors, such as noisy/spurious events and inhomogeneous event densities on the object. Such effects are non-trivial to simulate in software, thus leading to Sim2Real gap in the event domain. To close the Sim2Real gap in event-based satellite pose estimation, the paper proposes a test-time self-supervision scheme with a certifier module. Self-supervision is enabled by an optimisation routine that aligns a dense point cloud of the predicted satellite pose with the event data to attempt to rectify the inaccurately estimated pose. The certifier attempts to verify the corrected pose, and only certified test-time inputs are backpropagated via implicit differentiation to refine the predicted landmarks, thus improving the pose estimates and closing the Sim2Real gap. Results show that the our method outperforms established test-time adaptation schemes."
Rotograb: Combining Biomimetic Hands with Industrial Grippers using a Rotating Thumb,"Bersier, Arnaud; Leonforte, Matteo; Vanetta, Alessio; Wotke, Sarah Lia Andrea; Nappi, Andrea; Zhou, Yifan; Oliani, Sebastiano; Kübler, Alexander M.; Katzschmann, Robert Kevin",,
Simulation-Assisted Learning for Efficient Bin-Packing of Deformable Packages in a Bimanual Robotic Cell,"Manyar, Omey Mohan; Ye, Hantao; Sagare, Meghana; Mayya, Siddharth; Wang, Fan; Gupta, Satyandra K.",,
TrustNavGPT: Trust-Driven Audio-Guided Robot Navigation under Uncertainty with Large Language Models,"Sun, Xingpeng; Zhang, Yiran; Tang, Xindi; Bedi, Amrit Singh; Bera, Aniket",,
A Learning-based Controller for Multi-Contact Grasps on Unknown Objects with a Dexterous Hand,"Winkelbauer, Dominik; Triebel, Rudolph; Bäuml, Berthold",,
Learning Symbolic and Subsymbolic Temporal Task Constraints from Bimanual Human Demonstrations,"Dreher, Christian R. G.; Asfour, Tamim",https://arxiv.org/abs/2403.16953,"Learning task models of bimanual manipulation from human demonstration and their execution on a robot should take temporal constraints between actions into account. This includes constraints on (i) the symbolic level such as precedence relations or temporal overlap in the execution, and (ii) the subsymbolic level such as the duration of different actions, or their starting and end points in time. Such temporal constraints are crucial for temporal planning, reasoning, and the exact timing for the execution of bimanual actions on a bimanual robot. In our previous work, we addressed the learning of temporal task constraints on the symbolic level and demonstrated how a robot can leverage this knowledge to respond to failures during execution. In this work, we propose a novel model-driven approach for the combined learning of symbolic and subsymbolic temporal task constraints from multiple bimanual human demonstrations. Our main contributions are a subsymbolic foundation of a temporal task model that describes temporal nexuses of actions in the task based on distributions of temporal differences between semantic action keypoints, as well as a method based on fuzzy logic to derive symbolic temporal task constraints from this representation. This complements our previous work on learning comprehensive temporal task models by integrating symbolic and subsymbolic information based on a subsymbolic foundation, while still maintaining the symbolic expressiveness of our previous approach. We compare our proposed approach with our previous pure-symbolic approach and show that we can reproduce and even outperform it. Additionally, we show how the subsymbolic temporal task constraints can synchronize otherwise unimanual movement primitives for bimanual behavior on a humanoid robot."
A Digital Twin-Driven Immersive Teleoperation Framework for Robot-Assisted Microsurgery,"Jiang, Peiyang; Zhang, Dandan",,
Scalable Network and Adaptive Refinement Module for 6D Pose Estimation of Diverse Industrial Components,"Qian, Kun; Erden, Mustafa Suphi; Kong, Xianwen",,
Explainable Artificial intelligence for Autonomous UAV Navigation,"Dissanayaka, Didula; Wanasinghe, Thumeera Ruwansiri; Gosine, Raymond G.",,
Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight Control of Quadrotors,"Yazdanshenas, Amin; Faieghi, Reza",https://arxiv.org/abs/2403.10934,"Despite extensive research on sliding mode control (SMC) design for quadrotors, the existing approaches suffer from certain limitations. Euler angle-based SMC formulations suffer from poor performance in high-pitch or -roll maneuvers. Quaternion-based SMC approaches have unwinding issues and complex architecture. Coordinate-free methods are slow and only almost globally stable. This paper presents a new six degrees of freedom SMC flight controller to address the above limitations. We use a cascaded architecture with a position controller in the outer loop and a quaternion-based attitude controller in the inner loop. The position controller generates the desired trajectory for the attitude controller using a coordinate-free approach. The quaternion-based attitude controller uses the natural characteristics of the quaternion hypersphere, featuring a simple structure while providing global stability and avoiding unwinding issues. We compare our controller with three other common control methods conducting challenging maneuvers like flip-over and high-speed trajectory tracking in the presence of model uncertainties and disturbances. Our controller consistently outperforms the benchmark approaches with less control effort and actuator saturation, offering highly effective and efficient flight control."
"A comparison of audible, visual, and multi-modal communication for multi-robot supervision and situational awareness","Attfield, Richard John; Croft, Elizabeth; Kulic, Dana",,
Inverse Kinematics of Robotic Manipulators Using a New Learning-by-Example Method,"Demby's, Jacket; Farag, Ramy; DeSouza, Guilherme",,
3D-BLUE: Backscatter Localization for Underwater Robotics,"Afzal, Sayed Saad; Chen, Wei-Tung; Adib, Fadel",,
A Wearable Platform Based on the Multi-modal Foundation Model to Augment Spatial Cognition for People with Blindness and Low Vision,"Hao, Yu; Magay, Alexey; Huang, Hao; Yuan, Shuaihang; Wen, Congcong; Fang, Yi",,
k-Robust Conflict-based Search with Continuous time for Multi-robot Coordination,"Daudt, Guilherme; Deus, Alleff Dymytry; Kolberg, Mariana; Maffei, Renan",,
A Proxy-Tactile Reactive Control for Robots Moving in Clutter,"Caroleo, Giammarco; Giovinazzo, Francesco; Albini, Alessandro; Grella, Francesco; Cannata, Giorgio; Maiolino, Perla",,
Feelit: Combining Compliant Shape Displays with Vision-Based Tactile Sensors for Real-Time Teletaction,"Yu, Oscar; She, Yu",https://arxiv.org/abs/2408.15480,"Teletaction, the transmission of tactile feedback or touch, is a crucial aspect in the field of teleoperation. High-quality teletaction feedback allows users to remotely manipulate objects and increase the quality of the human-machine interface between the operator and the robot, making complex manipulation tasks possible. Advances in the field of teletaction for teleoperation however, have yet to make full use of the high-resolution 3D data provided by modern vision-based tactile sensors. Existing solutions for teletaction lack in one or more areas of form or function, such as fidelity or hardware footprint. In this paper, we showcase our design for a low-cost teletaction device that can utilize real-time high-resolution tactile information from vision-based tactile sensors, through both physical 3D surface reconstruction and shear displacement. We present our device, the Feelit, which uses a combination of a pin-based shape display and compliant mechanisms to accomplish this task. The pin-based shape display utilizes an array of 24 servomotors with miniature Bowden cables, giving the device a resolution of 6x4 pins in a 15x10 mm display footprint. Each pin can actuate up to 3 mm in 200 ms, while providing 80 N of force and 1.5 um of depth resolution. Shear displacement and rotation is achieved using a compliant mechanism design, allowing a minimum of 1 mm displacement laterally and 10 degrees of rotation. This real-time 3D tactile reconstruction is achieved with the use of a vision-based tactile sensor, the GelSight [1], along with an algorithm that samples the depth data and marker tracking to generate actuator commands. Through a series of experiments including shape recognition and relative weight identification, we show that our device has the potential to expand teletaction capabilities in the teleoperation space."
Learned Slip-Detection-Severity Framework using Tactile Deformation Field Feedback for Robotic Manipulation,"Jawale, Neel Anand; Kaur, Navneet; Santoso, Elizabeth Amy; Chen, Xu",,
Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning,"Pan, Sicong; Jin, Liren; Huang, Xuying; Stachniss, Cyrill; Popovic, Marija; Bennewitz, Maren",https://arxiv.org/abs/2403.16803,"Object reconstruction is relevant for many autonomous robotic tasks that require interaction with the environment. A key challenge in such scenarios is planning view configurations to collect informative measurements for reconstructing an initially unknown object. One-shot view planning enables efficient data collection by predicting view configurations and planning the globally shortest path connecting all views at once. However, geometric priors about the object are required to conduct one-shot view planning. In this work, we propose a novel one-shot view planning approach that utilizes the powerful 3D generation capabilities of diffusion models as priors. By incorporating such geometric priors into our pipeline, we achieve effective one-shot view planning starting with only a single RGB image of the object to be reconstructed. Our planning experiments in simulation and real-world setups indicate that our approach balances well between object reconstruction quality and movement cost."
Radiance Fields for Robotic Teleoperation,"Wilder-Smith, Maximum; Patil, Vaishakh; Hutter, Marco",https://arxiv.org/abs/2407.20194,"Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an ideal visualization for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity operation at the cost of maneuverability, while reconstruction-based approaches offer controllable scenes with lower fidelity. With this in mind, we propose replacing the traditional reconstruction-visualization components of the robotic teleoperation pipeline with online Radiance Fields, offering highly maneuverable scenes with photorealistic quality. As such, there are three main contributions to state of the art: (1) online training of Radiance Fields using live data from multiple cameras, (2) support for a variety of radiance methods including NeRF and 3DGS, (3) visualization suite for these methods including a virtual reality scene. To enable seamless integration with existing setups, these components were tested with multiple robots in multiple configurations and were displayed using traditional tools as well as the VR headset. The results across methods and robots were compared quantitatively to a baseline of mesh reconstruction, and a user study was conducted to compare the different visualization methods. For videos and code, check out https://leggedrobotics.github.io/rffr.github.io/."
Demonstrating Trustworthiness in Model Mediated Teleoperation for Collecting Lunar Regolith Simulant,"Louca, Joe; Zemeny, Aliz; Tzemanaki, Antonia; Charles, Romain",,
A Case Study on Visual-Audio-Tactile Cross-Modal Retrieval,"Wojcik, Jagoda; Jiang, Jiaqi; Wu, Jiacheng; LUO, SHAN",https://arxiv.org/abs/2407.20709,"Cross-Modal Retrieval (CMR), which retrieves relevant items from one modality (e.g., audio) given a query in another modality (e.g., visual), has undergone significant advancements in recent years. This capability is crucial for robots to integrate and interpret information across diverse sensory inputs. However, the retrieval space in existing robotic CMR approaches often consists of only one modality, which limits the robot's performance. In this paper, we propose a novel CMR model that incorporates three different modalities, i.e., visual, audio and tactile, for enhanced multi-modal object retrieval, named as VAT-CMR. In this model, multi-modal representations are first fused to provide a holistic view of object features. To mitigate the semantic gaps between representations of different modalities, a dominant modality is then selected during the classification training phase to improve the distinctiveness of the representations, so as to improve the retrieval performance. To evaluate our proposed approach, we conducted a case study and the results demonstrate that our VAT-CMR model surpasses competing approaches. Further, our proposed dominant modality selection significantly enhances cross-retrieval accuracy."
One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting,"Wu, Albert; Wang, Ruocheng; Chen, Sirui; Eppner, Clemens; Liu, Karen",https://arxiv.org/abs/2404.07468,"Extrinsic manipulation, the use of environment contacts to achieve manipulation objectives, enables strategies that are otherwise impossible with a parallel jaw gripper. However, orchestrating a long-horizon sequence of contact interactions between the robot, object, and environment is notoriously challenging due to the scene diversity, large action space, and difficult contact dynamics. We observe that most extrinsic manipulation are combinations of short-horizon primitives, each of which depend strongly on initializing from a desirable contact configuration to succeed. Therefore, we propose to generalize one extrinsic manipulation trajectory to diverse objects and environments by retargeting contact requirements. We prepare a single library of robust short-horizon, goal-conditioned primitive policies, and design a framework to compose state constraints stemming from contacts specifications of each primitive. Given a test scene and a single demo prescribing the primitive sequence, our method enforces the state constraints on the test scene and find intermediate goal states using inverse kinematics. The goals are then tracked by the primitive policies. Using a 7+1 DoF robotic arm-gripper system, we achieved an overall success rate of 80.5% on hardware over 4 long-horizon extrinsic manipulation tasks, each with up to 4 primitives. Our experiments cover 10 objects and 6 environment configurations. We further show empirically that our method admits a wide range of demonstrations, and that contact retargeting is indeed the key to successfully combining primitives for long-horizon extrinsic manipulation. Code and additional details are available at stanford-tml.github.io/extrinsic-manipulation."
"Design, Prototype, and Performance Assessment of an Autonomous Manipulation System for Mars Sample Recovery Helicopter","Kalantari, Arash; Brinkman, Alexander; Carpenter, Kalind; Gildner, Matthew; Jenkins, Justin; Newill-Smith, David; Seiden, Jeffrey; Umali, Allen; McCormick, Ryan",,
GESCE: Graph-based Ergodic Search in Cluttered Environments,"Shirose, Burhanuddin; Johnson, Adam; Vundurthy, Bhaskar; Choset, Howie; Travers, Matthew",,
Design of Stickbug: a Six-Armed Precision Pollination Robot,"Smith, Trevor; Rijal, Madhav; Arend Tatsch, Christopher Alexander; Butts, R. Michael; Beard, Jared; Robert Cook, Tyler; Chu, Andy; Gross, Jason; Gu, Yu",https://arxiv.org/abs/2404.03489,"This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses. Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability. Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity. Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination. Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 50% success rate. Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug's software and design files."
Shape-prior Free Space-time Neural Radiance Field for 4D Semantic Reconstruction of Dynamic Scene from Sparse-View RGB Videos,"Biswas, Sandika; Banerjee, Biplab; Rezatofighi, Hamid",,
FC-Hetero: Fast and Autonomous Aerial Reconstruction Using a LiDAR-Visual Heterogeneous Multi-UAV System,"Zhang, Mingjie; Feng, Chen; Li, Zengzhi; Zheng, Guiyong; Luo, Yiming; Wang, Zhu; ZHOU, Jinni; Shen, Shaojie; Zhou, Boyu",,
Force-Triggered Control Design for User Intent-Driven Assistive Upper-Limb Robots,"Manzano, Maxime; Guegan, Sylvain; Le Breton, Ronan; Devigne, Louise; Babel, Marie",,
Importance of Translational Velocity for Bird-scale Flapping Wing Vehicles Incapable of Hovering,"Zhou, Shijun; Orr, Aidan; Hyun, Nak-seung Patrick",,
Learned Sensor Fusion For Robust Human Activity Recognition in Challenging Environments,"Conway, Max; Reily, Brian; Reardon, Christopher M.",,
Open Human-Robot Collaborations using Decentralized Inverse Reinforcement Learning,"SENGADU SURESH, PRASANTH; Jain, Siddarth; Doshi, Prashant; Romeres, Diego",,
Roofus: Learning-based Robotic Moisture Mapping on Flat Rooftops with Ground Penetrating Radar,"Lee, Kevin; Lin, Wei-Heng; Javed, Talha; Madhusudhan, Sruti; Sher, Bilal; Feng, Chen",,
Feeling Optimistic? Ambiguity Attitudes for Online Decision Making,"Beard, Jared; Butts, R. Michael; Gu, Yu",https://arxiv.org/abs/2303.04225,"Due to the complexity of many decision making problems, tree search algorithms often have inadequate information to produce accurate transition models. This results in ambiguities (uncertainties for which there are multiple plausible models). Faced with ambiguities, robust methods have been used to produce safe solutions--often by maximizing the lower bound over the set of plausible transition models. However, they often overlook how much the representation of uncertainty can impact how a decision is made. This work introduces the Ambiguity Attitude Graph Search (AAGS), advocating for more comprehensive representations of ambiguities in decision making. Additionally, AAGS allows users to adjust their ambiguity attitude (or preference), promoting exploration and improving users' ability to control how an agent should respond when faced with a set of plausible alternatives. Simulation in a dynamic sailing environment shows how environments with high entropy transition models can lead robust methods to fail. Results further demonstrate how adjusting ambiguity attitudes better fulfills objectives while mitigating this failure mode of robust approaches. Because this approach is a generalization of the robust framework, these results further demonstrate how algorithms focused on ambiguity have applicability beyond safety-critical systems."
PAAMP: Polytopic Action-Set And Motion Planning For Long Horizon Dynamic Motion Planning via Mixed Integer Linear Programming,"Jaitly, Akshay; Farzan, Siavash",https://arxiv.org/abs/2403.10924,"Optimization methods for long-horizon, dynamically feasible motion planning in robotics tackle challenging non-convex and discontinuous optimization problems. Traditional methods often falter due to the nonlinear characteristics of these problems. We introduce a technique that utilizes learned representations of the system, known as Polytopic Action Sets, to efficiently compute long-horizon trajectories. By employing a suitable sequence of Polytopic Action Sets, we transform the long-horizon dynamically feasible motion planning problem into a Linear Program. This reformulation enables us to address motion planning as a Mixed Integer Linear Program (MILP). We demonstrate the effectiveness of a Polytopic Action-Set and Motion Planning (PAAMP) approach by identifying swing-up motions for a torque-constrained pendulum as fast as 0.75 milliseconds. This approach is well-suited for solving complex motion planning and long-horizon Constraint Satisfaction Problems (CSPs) in dynamic and underactuated systems such as legged and aerial robots."
LiDAR-based 4D Occupancy Completion and Forecasting,"Liu, Xinhao; Gong, Moonjun; Fang, Qi; Xie, Haoyu; LI, YIMING; Zhao, Hang; Feng, Chen",https://arxiv.org/abs/2310.11239,"Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baseline models and our own ones on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast."
Learning Generalizable Manipulation Policy with Adapter-Based Parameter Fine-Tuning,"Lu, Kai; Ly, Kim Tien; zhou, kaichen; Havoutis, Ioannis; Markham, Andrew",,
A Voxel-Enabled Robotic Assistant for Omnidirectional Conveyance,"Carvajal, Michael Angelo; Mabulu, Katiso; Lalji, Muneer; Flanagan, James; Hibbard, Sam; Luo, Rui; Chinthapatla, Tanav; Bettadpur, Rohan; Bazzi, Salah; Zolotas, Mark; Kloeckl, Kristian; Padir, Taskin",,
BOMP: Bin-Optimized Motion Planning,"Tam, Zachary; Dharmarajan, Karthik; Qiu, Tianshuang; Avigal, Yahav; Ichnowski, Jeffrey; Goldberg, Ken",,
Renderable Street View Map-based Localization: Leveraging 3D Gaussian Splatting for Street-level Positioning,"Jun, Howoong; Yu, Hyeonwoo; Oh, Songhwai",,
Improving Legged Robot Locomotion by Quantifying Morphological Computation,"Chandiramani, Vijay; Hauser, Helmut; Conn, Andrew",,
Bifurcation Identification for Ultrasound-driven Robotic Cannulation,"Morales, Cecilia; Srikanth, Dhruv; Good, Jack; Goswami, Mononito; Dufendach, Keith; Dubrawski, Artur",https://arxiv.org/abs/2409.06817,"In trauma and critical care settings, rapid and precise intravascular access is key to patients' survival. Our research aims at ensuring this access, even when skilled medical personnel are not readily available. Vessel bifurcations are anatomical landmarks that can guide the safe placement of catheters or needles during medical procedures. Although ultrasound is advantageous in navigating anatomical landmarks in emergency scenarios due to its portability and safety, to our knowledge no existing algorithm can autonomously extract vessel bifurcations using ultrasound images. This is primarily due to the limited availability of ground truth data, in particular, data from live subjects, needed for training and validating reliable models. Researchers often resort to using data from anatomical phantoms or simulations. We introduce BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a novel algorithm that identifies vessel bifurcations and provides optimal needle insertion sites for an autonomous robotic cannulation system. BIFURC integrates expert knowledge with deep learning techniques to efficiently detect vessel bifurcations within the femoral region and can be trained on a limited amount of in-vivo data. We evaluated our algorithm using a medical phantom as well as real-world experiments involving live pigs. In all cases, BIFURC consistently identified bifurcation points and needle insertion locations in alignment with those identified by expert clinicians."
Dynamic Model and Experimental Validation of a Haptic Robot based on a Flexible Antenna mounted on an Omnidirectional Platform,"Merida-Calvo, Luis; Haro-Olmo, Maria Isabel; Feliu, Vicente",,
VLPG-Nav: Object Navigation Using Visual Language Pose Graph and Object Localization Probability Maps,"Arul, Senthil Hariharan; Kumar, Dhruva; Sugirtharaj, Vivek; Kim, Richard; Qi, Xuewei; Madhivanan, Rajasimman; Sen, Arnab; Manocha, Dinesh",https://arxiv.org/abs/2408.08301,"We present VLPG-Nav, a visual language navigation method for guiding robots to specified objects within household scenes. Unlike existing methods primarily focused on navigating the robot toward objects, our approach considers the additional challenge of centering the object within the robot's camera view. Our method builds a visual language pose graph (VLPG) that functions as a spatial map of VL embeddings. Given an open vocabulary object query, we plan a viewpoint for object navigation using the VLPG. Despite navigating to the viewpoint, real-world challenges like object occlusion, displacement, and the robot's localization error can prevent visibility. We build an object localization probability map that leverages the robot's current observations and prior VLPG. When the object isn't visible, the probability map is updated and an alternate viewpoint is computed. In addition, we propose an object-centering formulation that locally adjusts the robot's pose to center the object in the camera view. We evaluate the effectiveness of our approach through simulations and real-world experiments, evaluating its ability to successfully view and center the object within the camera field of view. VLPG-Nav demonstrates improved performance in locating the object, navigating around occlusions, and centering the object within the robot's camera view, outperforming the selected baselines in the evaluation metrics."
Enhancing Object Grasping Efficiency with Deep Learning and Post-processing for Multi-finger Robotic Hands,"Samandi, Pouya; Gupta, Kamal; mehrandezh, mehran",,
Commonsense Scene Graph-based Target Localization for Object Search,"Ge, Wenqi; Tang, Chao; Zhang, Hong",https://arxiv.org/abs/2404.00343,"Object search is a fundamental skill for household robots, yet the core problem lies in the robot's ability to locate the target object accurately. The dynamic nature of household environments, characterized by the arbitrary placement of daily objects by users, makes it challenging to perform target localization. To efficiently locate the target object, the robot needs to be equipped with knowledge at both the object and room level. However, existing approaches rely solely on one type of knowledge, leading to unsatisfactory object localization performance and, consequently, inefficient object search processes. To address this problem, we propose a commonsense scene graph-based target localization, CSG-TL, to enhance target object search in the household environment. Given the pre-built map with stationary items, the robot models the room-level knowledge with object-level commonsense knowledge generated by a large language model (LLM) to a commonsense scene graph (CSG), supporting both types of knowledge for CSG-TL. To demonstrate the superiority of CSG-TL on target localization, extensive experiments are performed on the real-world ScanNet dataset and the AI2THOR simulator. Moreover, we have extended CSG-TL to an object search framework, CSG-OS, validated in both simulated and real-world environments. Code and videos are available at https://sites.google.com/view/csg-os."
Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots,"Lee, Connor; Soedarmadji, Saraswati; Anderson, Matthew; Clark, Anthony; Chung, Soon-Jo",https://arxiv.org/abs/2403.14056,"We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates. This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale. By incorporating a thermal-conditioned refinement step with visual foundation models, our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-to-no cost. It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular zero-shot semantic segmentation methods based on large vision-language models currently used for generating annotations for RGB imagery. Code will be available at: https://github.com/connorlee77/aerial-auto-segment."
Single-Shot 6DoF Pose and 3D Size Estimation for Robotic Strawberry Harvesting,"Li, Lun; Kasaei, Hamidreza",,
Integrating Interactive Perception into Long-Horizon Robot Manipulation Systems,"Sadjadpour, Tara; Yu, Justin; O'Neill, Abigail; Khfifi, Mehdi; Chen, Lawrence Yunliang; Cheng, Richard; Balakrishna, Ashwin; Kollar, Thomas; Goldberg, Ken",https://arxiv.org/abs/2107.00683,"Long horizon sequential manipulation tasks are effectively addressed hierarchically: at a high level of abstraction the planner searches over abstract action sequences, and when a plan is found, lower level motion plans are generated. Such a strategy hinges on the ability to reliably predict that a feasible low level plan will be found which satisfies the abstract plan. However, computing Abstract Plan Feasibility (APF) is difficult because the outcome of a plan depends on real-world phenomena that are difficult to model, such as noise in estimation and execution. In this work, we present an active learning approach to efficiently acquire an APF predictor through task-independent, curious exploration on a robot. The robot identifies plans whose outcomes would be informative about APF, executes those plans, and learns from their successes or failures. Critically, we leverage an infeasible subsequence property to prune candidate plans in the active learning strategy, allowing our system to learn from less data. We evaluate our strategy in simulation and on a real Franka Emika Panda robot with integrated perception, experimentation, planning, and execution. In a stacking domain where objects have non-uniform mass distributions, we show that our system permits real robot learning of an APF model in four hundred self-supervised interactions, and that our learned model can be used effectively in multiple downstream tasks."
Learning-on-the-Drive: Self-supervised Adaptive Long-range Perception for High-speed Offroad Driving,"Ho, Cherie; Chen, Eric; Maulimov, Mukhtar; Wang, Chen; Scherer, Sebastian",https://arxiv.org/abs/2306.15226,"Autonomous off-road driving requires understanding traversability, which refers to the suitability of a given terrain to drive over. When offroad vehicles travel at high speed ($>10m/s$), they need to reason at long-range ($50m$-$100m$) for safe and deliberate navigation. Moreover, vehicles often operate in new environments and under different weather conditions. LiDAR provides accurate estimates robust to visual appearances, however, it is often too noisy beyond 30m for fine-grained estimates due to sparse measurements. Conversely, visual-based models give dense predictions at further distances but perform poorly at all ranges when out of training distribution. To address these challenges, we present ALTER, an offroad perception module that adapts-on-the-drive to combine the best of both sensors. Our visual model continuously learns from new near-range LiDAR measurements. This self-supervised approach enables accurate long-range traversability prediction in novel environments without hand-labeling. Results on two distinct real-world offroad environments show up to 52.5% improvement in traversability estimation over LiDAR-only estimates and 38.1% improvement over non-adaptive visual baseline."
Task-Driven Manipulation with Reconfigurable Parallel Robots,"Morton, Daniel; Cutkosky, Mark; Pavone, Marco",https://arxiv.org/abs/2403.10768,"ReachBot, a proposed robotic platform, employs extendable booms as limbs for mobility in challenging environments, such as martian caves. When attached to the environment, ReachBot acts as a parallel robot, with reconfiguration driven by the ability to detach and re-place the booms. This ability enables manipulation-focused scientific objectives: for instance, through operating tools, or handling and transporting samples. To achieve these capabilities, we develop a two-part solution, optimizing for robustness against task uncertainty and stochastic failure modes. First, we present a mixed-integer stance planner to determine the positioning of ReachBot's booms to maximize the task wrench space about the nominal point(s). Second, we present a convex tension planner to determine boom tensions for the desired task wrenches, accounting for the probabilistic nature of microspine grasping. We demonstrate improvements in key robustness metrics from the field of dexterous manipulation, and show a large increase in the volume of the manipulation workspace. Finally, we employ Monte-Carlo simulation to validate the robustness of these methods, demonstrating good performance across a range of randomized tasks and environments, and generalization to cable-driven morphologies. We make our code available at our project webpage, https://stanfordasl.github.io/reachbot_manipulation/"
Learned Regions of Attraction for Safe Motion Primitive Transitions,"Ubellacker, Wyatt; Ames, Aaron",,
Multi-Modal Representation Learning with Tactile Data,"Chi, Hyung-gun; Mercat, Jean; Barreiros, Jose; Ramani, Karthik; Kollar, Thomas",https://arxiv.org/abs/1802.07490,"Vision and touch are two of the important sensing modalities for humans and they offer complementary information for sensing the environment. Robots could also benefit from such multi-modal sensing ability. In this paper, addressing for the first time (to the best of our knowledge) texture recognition from tactile images and vision, we propose a new fusion method named Deep Maximum Covariance Analysis (DMCA) to learn a joint latent space for sharing features through vision and tactile sensing. The features of camera images and tactile data acquired from a GelSight sensor are learned by deep neural networks. But the learned features are of a high dimensionality and are redundant due to the differences between the two sensing modalities, which deteriorates the perception performance. To address this, the learned features are paired using maximum covariance analysis. Results of the algorithm on a newly collected dataset of paired visual and tactile data relating to cloth textures show that a good recognition performance of greater than 90\% can be achieved by using the proposed DMCA framework. In addition, we find that the perception performance of either vision or tactile sensing can be improved by employing the shared representation space, compared to learning from unimodal data."
Infrastructure-less UWB-based Active Relative Localization,"Brunacci, Valerio; Dionigi, Alberto; De Angelis, Alessio; Costante, Gabriele",,
Steering Decision Transformers via Temporal Difference Learning,"Hsu, Hao-Lun; Bozkurt, Alper Kamil; Dong, Juncheng; Gao, Qitong; Tarokh, Vahid; Pajic, Miroslav",,
Low-Cost Urban Localization with Magnetometer and LoRa Technology,"Benham, Derek; Palacios, Ashton; Lundrigan, Philip; Mangelson, Joshua",,
Hierarchical Large Scale Multirobot Path (Re)Planning,"Pan, Lishuo; Hsu, Kevin; Ayanian, Nora",https://arxiv.org/abs/2407.02777,"We consider a large-scale multi-robot path planning problem in a cluttered environment. Our approach achieves real-time replanning by dividing the workspace into cells and utilizing a hierarchical planner. Specifically, multi-commodity flow-based high-level planners route robots through the cells to reduce congestion, while an anytime low-level planner computes collision-free paths for robots within each cell in parallel. Despite resulting in longer paths compared to the baseline multi-agent pathfinding algorithm, our method produces a solution with significant improvement in computation time. Specifically, we show empirical results of a 500-times speedup in computation time compared to the baseline multi-agent pathfinding approach on the environments we study. We account for the robot's embodiment and support non-stop execution when replanning continuously. We demonstrate the real-time performance of our algorithm with up to 142 robots in simulation, and a representative 32 physical Crazyflie nano-quadrotor experiment."
Learning Dynamic Tasks on a Large-scale Soft Robot in a Handful of Trials,"Zwane, Sicelukwanda Njabuliso Tunner; Cheney, Daniel G.; Johnson, Curtis C; Luo, Yicheng; Bekiroglu, Yasemin; Killpack, Marc; Deisenroth, Marc Peter",,
Towards the New Generation of Smart Home-Care with Cloud-Based Internet of Humans and Robotic Things,"Zhang, Dandan; Zheng, Jin",,
Masked Mutual Guidance Transformer Tracking,"Wang, Zhiquan; fan, baojie; Gao, Xuan; zhou, yuhan; zhang, caiyu",,
Accelerating Model Predictive Control for Legged Robots through Distributed Optimization,"Amatucci, Lorenzo; Turrisi, Giulio; Bratta, Angelo; Barasuol, Victor; Semini, Claudio",https://arxiv.org/abs/2403.11742,"This paper presents a novel approach to enhance Model Predictive Control (MPC) for legged robots through Distributed Optimization. Our method focuses on decomposing the robot dynamics into smaller, parallelizable subsystems, and utilizing the Alternating Direction Method of Multipliers (ADMM) to ensure consensus among them. Each subsystem is managed by its own Optimal Control Problem, with ADMM facilitating consistency between their optimizations. This approach not only decreases the computational time but also allows for effective scaling with more complex robot configurations, facilitating the integration of additional subsystems such as articulated arms on a quadruped robot. We demonstrate, through numerical evaluations, the convergence of our approach on two systems with increasing complexity. In addition, we showcase that our approach converges towards the same solution when compared to a state-of-the-art centralized whole-body MPC implementation. Moreover, we quantitatively compare the computational efficiency of our method to the centralized approach, revealing up to a 75% reduction in computational time. Overall, our approach offers a promising avenue for accelerating MPC solutions for legged robots, paving the way for more effective utilization of the computational performance of modern hardware."
Continuum Robot Shape Estimation Using Magnetic Ball Chains,"Pittiglio, Giovanni; Donder, Abdulhamit; Dupont, Pierre",,
Automating Trophectoderm Cells Aspiration and Separation in Embryo Biopsy at the Blastocyst Stage: A Vision-Based Control Approach,"Abu Ajamieh, Ihab; Al Saaideh, Mohammad; Al Janaideh, Mohammad; Mills, James K.",,
A Real-time Filter for Human Pose Estimation based on Denoising Diffusion Models for Edge Devices,"Bozzini, Chiara; Boldo, Michele; Martini, Enrico; Bombieri, Nicola",,
Diffusion-PbD: Generalizable Robot Programming by Demonstration with Diffusion Features,"Murray, Michael; Su, Entong; Cakmak, Maya",,
Evaluating Dynamic Environment Difficulty for Obstacle Avoidance Benchmarking,"Shi, Moji; Chen, Gang; Serra-Gómez, Álvaro; Wu, Siyuan; Alonso-Mora, Javier",https://arxiv.org/abs/2404.14848,"Dynamic obstacle avoidance is a popular research topic for autonomous systems, such as micro aerial vehicles and service robots. Accurately evaluating the performance of dynamic obstacle avoidance methods necessitates the establishment of a metric to quantify the environment's difficulty, a crucial aspect that remains unexplored. In this paper, we propose four metrics to measure the difficulty of dynamic environments. These metrics aim to comprehensively capture the influence of obstacles' number, size, velocity, and other factors on the difficulty. We compare the proposed metrics with existing static environment difficulty metrics and validate them through over 1.5 million trials in a customized simulator. This simulator excludes the effects of perception and control errors and supports different motion and gaze planners for obstacle avoidance. The results indicate that the survivability metric outperforms and establishes a monotonic relationship between the success rate, with a Spearman's Rank Correlation Coefficient (SRCC) of over 0.9. Specifically, for every planner, lower survivability leads to a higher success rate. This metric not only facilitates fair and comprehensive benchmarking but also provides insights for refining collision avoidance methods, thereby furthering the evolution of autonomous systems in dynamic environments."
Efficient Target Singulation with Multi-Fingered Gripper using Propositional Logic,"Kim, Hyojeong; Jo, Jeong Yong; Lim, Myo-Taeg; Kim, ChangHwan",,
Online Optimization of Central Pattern Generators for Quadruped Locomotion,"Zhang, Zewei; Bellegarda, Guillaume; Shafiee, Milad; Ijspeert, Auke",https://arxiv.org/abs/2406.07065,"This study focuses on the locomotion capability improvement in a tendon-driven soft quadruped robot through an online adaptive learning approach. Leveraging the inverse kinematics model of the soft quadruped robot, we employ a central pattern generator to design a parametric gait pattern, and use Bayesian optimization (BO) to find the optimal parameters. Further, to address the challenges of modeling discrepancies, we implement a multi-fidelity BO approach, combining data from both simulation and physical experiments throughout training and optimization. This strategy enables the adaptive refinement of the gait pattern and ensures a smooth transition from simulation to real-world deployment for the controller. Moreover, we integrate a computational task off-loading architecture by edge computing, which reduces the onboard computational and memory overhead, to improve real-time control performance and facilitate an effective online learning process. The proposed approach successfully achieves optimal walking gait design for physical deployment with high efficiency, effectively addressing challenges related to the reality gap in soft robotics."
Dynamic Reconfiguration Integrated Nested A*: A Path Planner for Reconfigurable Robot to Improve Performance in Confined Spaces,"Rishan Sachinthana, Wijenayaka Kankanamge; Samarakoon Mudiyanselage, Bhagya Prasangi Samarakoon; Muthugala Arachchige, Viraj Jagathpriya Muthugala; Elara, Mohan Rajesh",,
CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning,"Chane-Sane, Elliot; Leziart, Pierre-Alexandre; Flayols, Thomas; Stasse, Olivier; Soueres, Philippe; Mansard, Nicolas",https://arxiv.org/abs/2403.18765,"Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption. Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks. Videos and code are available at https://constraints-as-terminations.github.io."
Enabling Maintainablity of Robot Programs in Assembly by Extracting Compositions of Force- and Position-Based Robot Skills from Learning-from-Demonstration Models,"Bargmann, Daniel; Kraus, Werner; Huber, Marco F.",,
SuPerPM: A Surgical Perception Framework Based on Deep Point Matching Learned from Physical Constrained Simulation Data,"Lin, Shan; Miao, Albert; Alabiad, Ali; LIU, FEI; Wang, Kaiyuan; Lu, Jingpei; Richter, Florian; Yip, Michael C.",https://arxiv.org/abs/2309.13863,"Manipulation of tissue with surgical tools often results in large deformations that current methods in tracking and reconstructing algorithms have not effectively addressed. A major source of tracking errors during large deformations stems from wrong data association between observed sensor measurements with previously tracked scene. To mitigate this issue, we present a surgical perception framework, SuPerPM, that leverages learning-based non-rigid point cloud matching for data association, thus accommodating larger deformations. The learning models typically require training data with ground truth point cloud correspondences, which is challenging or even impractical to collect in surgical environments. Thus, for tuning the learning model, we gather endoscopic data of soft tissue being manipulated by a surgical robot and then establish correspondences between point clouds at different time points to serve as ground truth. This was achieved by employing a position-based dynamics (PBD) simulation to ensure that the correspondences adhered to physical constraints. The proposed framework is demonstrated on several challenging surgical datasets that are characterized by large deformations, achieving superior performance over state-of-the-art surgical scene tracking algorithms."
Neural Control Barrier Functions for Safe Navigation,"Harms, Marvin Chayton; Kulkarni, Mihir; Khedekar, Nikhil Vijay; Jacquet, Martin; Alexis, Kostas",https://arxiv.org/abs/2407.19907,"Autonomous robot navigation can be particularly demanding, especially when the surrounding environment is not known and safety of the robot is crucial. This work relates to the synthesis of Control Barrier Functions (CBFs) through data for safe navigation in unknown environments. A novel methodology to jointly learn CBFs and corresponding safe controllers, in simulation, inspired by the State Dependent Riccati Equation (SDRE) is proposed. The CBF is used to obtain admissible commands from any nominal, possibly unsafe controller. An approach to apply the CBF inside a safety filter without the need for a consistent map or position estimate is developed. Subsequently, the resulting reactive safety filter is deployed on a multirotor platform integrating a LiDAR sensor both in simulation and real-world experiments."
DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability,"Fang, Xiaolin; Garrett, Caelan; Eppner, Clemens; Lozano-Perez, Tomas; Kaelbling, Leslie; Fox, Dieter",https://arxiv.org/abs/2306.13196,"Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constraint-based reasoning. We also apply the learned sampler in the real world. More details are available at https://sites.google.com/view/dimsam-tamp"
Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot,"Lorimer, Philip; Saunders, Jack; Hunter, Alan Joseph; Li, Wenbin",,
Visual Imitation Learning of Task-Oriented Object Grasping and Rearrangement,"Cai, Yichen; Gao, Jianfeng; Pohl, Christoph; Asfour, Tamim",https://arxiv.org/abs/2403.14000,"Task-oriented object grasping and rearrangement are critical skills for robots to accomplish different real-world manipulation tasks. However, they remain challenging due to partial observations of the objects and shape variations in categorical objects. In this paper, we propose the Multi-feature Implicit Model (MIMO), a novel object representation that encodes multiple spatial features between a point and an object in an implicit neural field. Training such a model on multiple features ensures that it embeds the object shapes consistently in different aspects, thus improving its performance in object shape reconstruction from partial observation, shape similarity measure, and modeling spatial relations between objects. Based on MIMO, we propose a framework to learn task-oriented object grasping and rearrangement from single or multiple human demonstration videos. The evaluations in simulation show that our approach outperforms the state-of-the-art methods for multi- and single-view observations. Real-world experiments demonstrate the efficacy of our approach in one- and few-shot imitation learning of manipulation tasks."
ARCADE: Scalable Demonstration Collection and Generation via Augmented Reality for Imitation Learning,"Yang, Yue; Ikeda, Bryce; Bertasius, Gedas; Szafir, Daniel J.",,
Real-Time Truly-Coupled Lidar-Inertial Motion Correction and Spatiotemporal Dynamic Object Detection,"Le Gentil, Cedric; Falque, Raphael; Vidal-Calleja, Teresa A.",,
Where and when should the teleoperated avatar look: Gaze Instruction Dataset for Enhanced Teleoperated Avatar Communication,"Hoshimure, Kenya; Baba, Jun; Nakanishi, Junya; Yoshikawa, Yuichiro; Ishiguro, Hiroshi",,
Robust Partitioned Visual Servoing for Aerial Manipulation Utilizing Controllable-space Image Planning and Adaptive Image Representation,"Soltanshah, Mohammad; eskandarpour, abolfazl; mehrandezh, mehran; Gupta, Kamal",,
Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation,"Di Felice, Francesco; Remus, Alberto; Gasperini, Stefano; Busam, Benjamin; Ott, Lionel; Tombari, Federico; Siegwart, Roland; Avizzano, Carlo Alberto",https://arxiv.org/abs/2403.14279,"Estimating the pose of objects through vision is essential to make robotic platforms interact with the environment. Yet, it presents many challenges, often related to the lack of flexibility and generalizability of state-of-the-art solutions. Diffusion models are a cutting-edge neural architecture transforming 2D and 3D computer vision, outlining remarkable performances in zero-shot novel-view synthesis. Such a use case is particularly intriguing for reconstructing 3D objects. However, localizing objects in unstructured environments is rather unexplored. To this end, this work presents Zero123-6D, the first work to demonstrate the utility of Diffusion Model-based novel-view-synthesizers in enhancing RGB 6D pose estimation at category-level, by integrating them with feature extraction techniques. Novel View Synthesis allows to obtain a coarse pose that is refined through an online optimization method introduced in this work to deal with intra-category geometric differences. In such a way, the outlined method shows reduction in data requirements, removal of the necessity of depth information in zero-shot category-level 6D pose estimation task, and increased performance, quantitatively demonstrated through experiments on the CO3D dataset."
IMU Based Pose Reconstruction and Closed-loop Control for Soft Robotic Arms,"Pei, Guanran; Stella, Francesco; Meebed, Omar Hani Mokhtar Ahmed; Bing, Zhenshan; Della Santina, Cosimo; Hughes, Josie",,
Koopman Dynamic Modeling for Global and Unified Representations of Rigid Body Systems Making and Breaking Contact,"O'Neill, Cormac; Asada, Harry",,
Valuing Attrition in a Fleet of Robots Used as Path-Based Sensors for Gathering Information in a Communications Restricted Environment,"McGuire, Loy; Otte, Michael W.; Sofge, Donald",,
Design of Upper-Limb Exoskeleton with Distal Branching Link Mechanism for Bilateral Operation of Humanoid Robots,"Yoshioka, Hiroki; Hiraoka, Naoki; Kojima, Kunio; Okada, Kei; Inaba, Masayuki",,
Dragtraffic: Interactive and Controllable Traffic Scene Generation for Autonomous Driving,"WANG, Sheng",https://arxiv.org/abs/2404.12624,"The evaluation and training of autonomous driving systems require diverse and scalable corner cases. However, most existing scene generation methods lack controllability, accuracy, and versatility, resulting in unsatisfactory generation results. To address this problem, we propose Dragtraffic, a generalized, point-based, and controllable traffic scene generation framework based on conditional diffusion. Dragtraffic enables non-experts to generate a variety of realistic driving scenarios for different types of traffic agents through an adaptive mixture expert architecture. We use a regression model to provide a general initial solution and a refinement process based on the conditional diffusion model to ensure diversity. User-customized context is introduced through cross-attention to ensure high controllability. Experiments on a real-world driving dataset show that Dragtraffic outperforms existing methods in terms of authenticity, diversity, and freedom."
Enhancing Surgical Precision in Autonomous Robotic Incisions via Physics-Based Tissue Cutting Simulation,"Ge, Jiawei; Kilmer, Ethan; Mady, Leila; Opfermann, Justin; Krieger, Axel",,
Recommendations on joint capabilities for transhumeral prosthetics,"Herneth, Christopher; Ganguly, Amartya; Haddadin, Sami",,
Goal Estimation-based Adaptive Shared Control for Brain-Machine Interfaces Remote Robot Navigation,"Muraoka, Tomoka; Aoki, Tatsuya; Hirata, Masayuki; Taniguchi, Tadahiro; Horii, Takato; Nagai, Takayuki",https://arxiv.org/abs/2407.17936,"In this study, we propose a shared control method for teleoperated mobile robots using brain-machine interfaces (BMI). The control commands generated through BMI for robot operation face issues of low input frequency, discreteness, and uncertainty due to noise. To address these challenges, our method estimates the user's intended goal from their commands and uses this goal to generate auxiliary commands through the autonomous system that are both at a higher input frequency and more continuous. Furthermore, by defining the confidence level of the estimation, we adaptively calculated the weights for combining user and autonomous commands, thus achieving shared control."
On the Benefits of GPU Sample-Based Stochastic Predictive Controllers for Legged Locomotion,"Turrisi, Giulio; Modugno, Valerio; Amatucci, Lorenzo; Kanoulas, Dimitrios; Semini, Claudio",https://arxiv.org/abs/2403.11383,"Quadrupedal robots excel in mobility, navigating complex terrains with agility. However, their complex control systems present challenges that are still far from being fully addressed. In this paper, we introduce the use of Sample-Based Stochastic control strategies for quadrupedal robots, as an alternative to traditional optimal control laws. We show that Sample-Based Stochastic methods, supported by GPU acceleration, can be effectively applied to real quadruped robots. In particular, in this work, we focus on achieving gait frequency adaptation, a notable challenge in quadrupedal locomotion for gradient-based methods. To validate the effectiveness of Sample-Based Stochastic controllers we test two distinct approaches for quadrupedal robots and compare them against a conventional gradient-based Model Predictive Control system. Our findings, validated both in simulation and on a real 21Kg Aliengo quadruped, demonstrate that our method is on par with a traditional Model Predictive Control strategy when the robot is subject to zero or moderate disturbance, while it surpasses gradient-based methods in handling sustained external disturbances, thanks to the straightforward gait adaptation strategy that is possible to achieve within their formulation."
Multistable Soft Actuator for Physical Human-robot Interaction,"Long, Juncai; Li, Jituo; Diao, Xiaojie; Zhou, Chengdi; Lu, GuoDong; Feng, Yixiong",,
HS3-Bench: A Benchmark and Strong Baseline for Hyperspectral Semantic Segmentation in Driving Scenarios,"Theisen, Nick; Bartsch, Robin; Paulus, Dietrich; Neubert, Peer",,
Sequential Discrete Action Selection via Blocking Conditions and Resolutions,"Merz Hoffmeister, Liam; Scassellati, Brian; Rakita, Daniel",https://arxiv.org/abs/2409.08410,"In this work, we introduce a strategy that frames the sequential action selection problem for robots in terms of resolving \textit{blocking conditions}, i.e., situations that impede progress on an action en route to a goal. This strategy allows a robot to make one-at-a-time decisions that take in pertinent contextual information and swiftly adapt and react to current situations. We present a first instantiation of this strategy that combines a state-transition graph and a zero-shot Large Language Model (LLM). The state-transition graph tracks which previously attempted actions are currently blocked and which candidate actions may resolve existing blocking conditions. This information from the state-transition graph is used to automatically generate a prompt for the LLM, which then uses the given context and set of possible actions to select a single action to try next. This selection process is iterative, with each chosen and executed action further refining the state-transition graph, continuing until the agent either fulfills the goal or encounters a termination condition. We demonstrate the effectiveness of our approach by comparing it to various LLM and traditional task-planning methods in a testbed of simulation experiments. We discuss the implications of our work based on our results."
AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans,"Perauer, Cedric; Zhang, Haifan; Heidrich, Laurenz Adrian; Niessner, Matthias; Kornilova, Anastasiia; Artemov, Alexey",https://arxiv.org/abs/2403.16318,"Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments. Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations. Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain. To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations. To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals. To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image- and point-based self-supervised features, and perform graph-cuts to isolate individual pseudo-instances. We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals. To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations. Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline. The code will be made publicly available at https://github.com/artonson/autoinst."
Beyond Feasibility: Efficiently Planning Robotic Assembly Sequences That Minimize Assembly Path Lengths,"Cebulla, Alexander; Asfour, Tamim; Kroeger, Torsten",,
Search-based Strategy for Spatio-Temporal Environmental Property Restoration,"Docena, Amel Nestor; Quattrini Li, Alberto",,
Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers,"Krnjaic, Aleksandar; Steleac, Raul Dacian; Thomas, Jonathan David; Papoudakis, Georgios; Schäfer, Lukas; To, Andrew; Lao, Kuan-Ho; Cubuktepe, Murat; Haley, Matthew; Börsting, Peter; Albrecht, Stefano V.",https://arxiv.org/abs/2212.11498,"We consider a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance in this task. Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), and different types of order-picking paradigms (e.g. Goods-to-Person and Person-to-Goods), as the agents can learn how to cooperate optimally through experience. We develop hierarchical MARL algorithms in which a manager agent assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorithms achieve significant gains in sample efficiency over baseline MARL algorithms and overall pick rates over multiple established industry heuristics in a diverse set of warehouse configurations and different order-picking paradigms."
LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement,"Chang, Haonan; Gao, Kai; Boyalakuntla, Kowndinya; Lee, Alex; Huang, Baichuan; Yu, Jingjin; Boularias, Abdeslam",https://arxiv.org/abs/2309.15821,"We introduce a novel approach to the executable semantic object rearrangement problem. In this challenge, a robot seeks to create an actionable plan that rearranges objects within a scene according to a pattern dictated by a natural language description. Unlike existing methods such as StructFormer and StructDiffusion, which tackle the issue in two steps by first generating poses and then leveraging a task planner for action plan formulation, our method concurrently addresses pose generation and action planning. We achieve this integration using a Language-Guided Monte-Carlo Tree Search (LGMCTS). Quantitative evaluations are provided on two simulation datasets, and complemented by qualitative tests with a real robot."
Communication-Constrained Multi-Robot Exploration with Intermittent Rendezvous,"Ribeiro da Silva, Alysson; Chaimowicz, Luiz; Costa Silva, Thales; Hsieh, M. Ani",https://arxiv.org/abs/2309.13494,"Communication constraints can significantly impact robots' ability to share information, coordinate their movements, and synchronize their actions, thus limiting coordination in Multi-Robot Exploration (MRE) applications. In this work, we address these challenges by modeling the MRE application as a DEC-POMDP and designing a joint policy that follows a rendezvous plan. This policy allows robots to explore unknown environments while intermittently sharing maps opportunistically or at rendezvous locations without being constrained by joint path optimizations. To generate the rendezvous plan, robots represent the MRE task as an instance of the Job Shop Scheduling Problem (JSSP) and minimize JSSP metrics. They aim to reduce waiting times and increase connectivity, which correlates to the DEC-POMDP rewards and time to complete the task. Our simulation results suggest that our method is more efficient than using relays or maintaining intermittent communication with a base station, being a suitable approach for Multi-Robot Exploration. We developed a proof-of-concept using the Robot Operating System (ROS) that is available at: https://github.com/multirobotplayground/ROS-Noetic-Multi-robot-Sandbox."
Diagrammatic Instructions to Specify Spatial Objectives and Constraints with Applications to Mobile Base Placement,"Sun, Qilin; Zhi, Weiming; Zhang, Tianyi; Johnson-Roberson, Matthew",https://arxiv.org/abs/2403.12465,"This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach for human operators to specify objectives and constraints that are related to spatial regions in the working environment. Human operators are enabled to sketch out regions directly on camera images that correspond to the objectives and constraints. These sketches are projected to 3D spatial coordinates, and continuous Spatial Instruction Maps (SIMs) are learned upon them. These maps can then be integrated into optimization problems for tasks of robots. In particular, we demonstrate how Spatial Diagrammatic Instructions can be applied to solve the Base Placement Problem of mobile manipulators, which concerns the best place to put the manipulator to facilitate a certain task. Human operators can specify, via sketch, spatial regions of interest for a manipulation task and permissible regions for the mobile manipulator to be at. Then, an optimization problem that maximizes the manipulator's reachability, or coverage, over the designated regions of interest while remaining in the permissible regions is solved. We provide extensive empirical evaluations, and show that our formulation of Spatial Instruction Maps provides accurate representations of user-specified diagrammatic instructions. Furthermore, we demonstrate that our diagrammatic approach to the Mobile Base Placement Problem enables higher quality solutions and faster run-time."
An Observability Constrained Downward-Facing Optical-Flow-Aided Visual-Inertial Odometry,"Liu, Dandi; Mei, Jiahao; Zhou, Jin; Li, Shuo",,
An Efficient Coverage Method for Irregularly Shaped Terrains,"Tang, Yuxuan; Wu, Qizhen; Zhu, Chunli; Chen, Lei",,
Probabilistic Homotopy Optimization for Dynamic Motion Planning,"Chignoli, Matthew; Pardis, Shayan; Kim, Sangbae",https://arxiv.org/abs/2408.12490,"We present a homotopic approach to solving challenging, optimization-based motion planning problems. The approach uses Homotopy Optimization, which, unlike standard continuation methods for solving homotopy problems, solves a sequence of constrained optimization problems rather than a sequence of nonlinear systems of equations. The insight behind our proposed algorithm is formulating the discovery of this sequence of optimization problems as a search problem in a multidimensional homotopy parameter space. Our proposed algorithm, the Probabilistic Homotopy Optimization algorithm, switches between solve and sample phases, using solutions to easy problems as initial guesses to more challenging problems. We analyze how our algorithm performs in the presence of common challenges to homotopy methods, such as bifurcation, folding, and disconnectedness of the homotopy solution manifold. Finally, we demonstrate its utility via a case study on two dynamic motion planning problems: the cart-pole and the MIT Humanoid."
Identifying Optimal Launch Sites of High-Altitude Latex-Balloons using Bayesian Optimisation for the Task of Station-Keeping,"Saunders, Jack; Saeedi, Sajad; Hartshorne, Adam; Xu, Binbin; &#350;im&#351;ek, Özgür; Hunter, Alan Joseph; Li, Wenbin",https://arxiv.org/abs/2403.10784,"Station-keeping tasks for high-altitude balloons show promise in areas such as ecological surveys, atmospheric analysis, and communication relays. However, identifying the optimal time and position to launch a latex high-altitude balloon is still a challenging and multifaceted problem. For example, tasks such as forest fire tracking place geometric constraints on the launch location of the balloon. Furthermore, identifying the most optimal location also heavily depends on atmospheric conditions. We first illustrate how reinforcement learning-based controllers, frequently used for station-keeping tasks, can exploit the environment. This exploitation can degrade performance on unseen weather patterns and affect station-keeping performance when identifying an optimal launch configuration. Valuing all states equally in the region, the agent exploits the region's geometry by flying near the edge, leading to risky behaviours. We propose a modification which compensates for this exploitation and finds this leads to, on average, higher steps within the target region on unseen data. Then, we illustrate how Bayesian Optimisation (BO) can identify the optimal launch location to perform station-keeping tasks, maximising the expected undiscounted return from a given rollout. We show BO can find this launch location in fewer steps compared to other optimisation methods. Results indicate that, surprisingly, the most optimal location to launch from is not commonly within the target region. Please find further information about our project at https://sites.google.com/view/bo-lauch-balloon/."
"D2SR: Decentralized Detection, De-Synchronization, and Recovery of LiDAR Interference","Rathnayake, Darshana; Sabbella, Hemanth; Radhakrishnan, Meera; Misra, Archan",,
"MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements","Sun, Lisong C.; Bhatt, Neel P.; Liu, Jonathan C.; Fan, Zhiwen; Wang, Zhangyang (Atlas); Humphreys, Todd E.; Topcu, Ufuk",https://arxiv.org/abs/2404.00923,"Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: https://vita-group.github.io/MM3DGS-SLAM"
Prediction of Acoustic Communication Performance for AUVs using Gaussian Process Classification,"GAO, YIFEI; Yetkin, Harun; Stilwell, Daniel; McMahon, James",,
Preserving Relative Localization of FoV-Limited Drone Swarm via Active Mutual Observation,"Gongye, Zaitian; Guo, Lianjie; Xu, Ziyi; Wang, Yingjian; Zhou, Xin; ZHOU, Jinni; Gao, Fei",https://arxiv.org/abs/2407.01292,"Relative state estimation is crucial for vision-based swarms to estimate and compensate for the unavoidable drift of visual odometry. For autonomous drones equipped with the most compact sensor setting -- a stereo camera that provides a limited field of view (FoV), the demand for mutual observation for relative state estimation conflicts with the demand for environment observation. To balance the two demands for FoV limited swarms by acquiring mutual observations with a safety guarantee, this paper proposes an active localization correction system, which plans camera orientations via a yaw planner during the flight. The yaw planner manages the contradiction by calculating suitable timing and yaw angle commands based on the evaluation of localization uncertainty estimated by the Kalman Filter. Simulation validates the scalability of our algorithm. In real-world experiments, we reduce positioning drift by up to 65% and managed to maintain a given formation in both indoor and outdoor GPS-denied flight, from which the accuracy, efficiency, and robustness of the proposed system are verified."
AEGO: Modeling Attention for HRI in Ego-Sphere Neural Networks,"Ferreira Chame, Hendry; Alami, Rachid",,
HM3D-OVON: A Dataset and Benchmark for Open-Vocabulary Object Goal Navigation,"Yokoyama, Naoki; Ramrakhya, Ram; Das, Abhishek; Batra, Dhruv; Ha, Sehoon",,
HeteroLight: A General and Efficient Learning Approach for Heterogeneous Traffic Signal Control,"ZHANG, YIFENG; LI, Peizhuo; Fan, Mingfeng; Sartoretti, Guillaume Adrien",,
Tree-Based Reconfiguration of Metamorphic Robots,"Ondika, Patrick; Mrázek, Jan; Barnat, Jiri",,
BEV-CV: Birds-Eye-View Transform for Cross-View Geo-Localisation,"Shore, Tavis George; Hadfield, Simon; Mendez, Oscar",https://arxiv.org/abs/2312.15363,"Cross-view image matching for geo-localisation is a challenging problem due to the significant visual difference between aerial and ground-level viewpoints. The method provides localisation capabilities from geo-referenced images, eliminating the need for external devices or costly equipment. This enhances the capacity of agents to autonomously determine their position, navigate, and operate effectively in environments where GPS signals are unavailable. Current research employs a variety of techniques to reduce the domain gap such as applying polar transforms to aerial images or synthesising between perspectives. However, these approaches generally rely on having a 360° field of view, limiting real-world feasibility. We propose BEV-CV, an approach which introduces two key novelties. Firstly we bring ground-level images into a semantic Birds-Eye-View before matching embeddings, allowing for direct comparison with aerial segmentation representations. Secondly, we introduce the use of a Normalised Temperature-scaled Cross Entropy Loss to the sub-field, achieving faster convergence than with the standard triplet loss. BEV-CV achieves state-of-the-art recall accuracies, improving feature extraction Top-1 rates by more than 300%, and Top-1% rates by approximately 150% for 70° crops, and for orientation-aware application we achieve a 35% Top-1 accuracy increase with 70° crops."
Transformer-based Multi-Agent Reinforcement Learning for Generalization of Heterogeneous Multi-Robot Cooperation,"Cai, Yuxin; He, Xiangkun; Guo, Hongliang; Yau, Wei-Yun; Lv, Chen",,
Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of Vision Algorithms,"Gamache, Olivier; Fortin, Jean-Michel; Boxan, Matej; Vaidis, Maxime; Pomerleau, Francois; Giguère, Philippe",https://arxiv.org/abs/2309.13139,"Visual Odometry (VO) is one of the fundamental tasks in computer vision for robotics. However, its performance is deeply affected by High Dynamic Range (HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches to mitigate this have appeared, their comparison in a reproducible manner is problematic. This stems from the fact that the behavior of AE depends on the environment, and it affects the image acquisition process. Consequently, AE has traditionally only been benchmarked in an online manner, making the experiments non-reproducible. To solve this, we propose a new methodology based on an emulator that can generate images at any exposure time. It leverages BorealHDR, a unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories with challenging illumination conditions. Moreover, it includes lidar-inertial-based global maps with pose estimation for each image frame as well as Global Navigation Satellite System (GNSS) data, for comparison. We show that using these images acquired at different exposure times, we can emulate realistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared to ground truth images. To demonstrate the practicality of our approach for offline benchmarking, we compared three state-of-the-art AE algorithms on key elements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline, against four baselines. Consequently, reproducible evaluation of AE is now possible, speeding up the development of future approaches. Our code and dataset are available online at this link: https://github.com/norlab-ulaval/BorealHDR"
Miniaturisation and Evaluation of the SoftSCREEN System in Colon Phantoms,"Consumi, Vanni; Dei, Neri Niccolò; Ciuti, Gastone; Stoyanov, Danail; Stilli, Agostino",,
Real-time Coordinated Motion Generation: A Hierarchical Deep Predictive Learning Model for Bimanual Tasks,"Shikada, Genki; Armleder, Simon; Ito, Hiroshi; Cheng, Gordon; Ogata, Tetsuya",,
An LSTM-based Model to Recognize Driving Style and Predict Acceleration,"Lu, Jiaxing; Hossain, Sanzida; Sheng, Weihua; BAI, HE",,
A Cascaded Broad Learning System for Manipulator Motion Control,"Zuo, Guoyu; Dong, Shuaifeng; Zhou, Jiyong; Yu, Shuangyue",,
DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World Representation and Label Optimization Techniques,"HU, Tianshuai; JIAO, Jianhao; Xu, Yucheng; Liu, Hongji; WANG, Sheng; Liu, Ming",https://arxiv.org/abs/2403.16880,"Maps provide robots with crucial environmental knowledge, thereby enabling them to perform interactive tasks effectively. Easily accessing accurate abstract-to-detailed geometric and semantic concepts from maps is crucial for robots to make informed and efficient decisions. To comprehensively model the environment and effectively manage the map data structure, we propose DHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed Distance Field (TSDF) submaps and panoptic labels to hierarchically model the environment. The output map is able to maintain both voxel- and submap-level metric and semantic information. Two modules are presented to enhance the mapping efficiency and label consistency: (1) an inter-submaps label fusion strategy to eliminate duplicate points across submaps and (2) a conditional random field (CRF) based approach to enhance panoptic labels through object label comprehension and contextual information. We conducted experiments with two public datasets including indoor and outdoor scenarios. Our system performs comparably to state-of-the-art (SOTA) methods across geometry and label accuracy evaluation metrics. The experiment results highlight the effectiveness and scalability of our system, as it is capable of constructing precise geometry and maintaining consistent panoptic labels. Our code is publicly available at https://github.com/hutslib/DHP-Mapping."
SePaint: Semantic Map Inpainting via Multinomial Diffusion,"Chen, Zheng; Duggirala, Deepak; Crandall, David; Jiang, Lei; Liu, Lantao",https://arxiv.org/abs/2303.02737,"Prediction beyond partial observations is crucial for robots to navigate in unknown environments because it can provide extra information regarding the surroundings beyond the current sensing range or resolution. In this work, we consider the inpainting of semantic Bird's-Eye-View maps. We propose SePaint, an inpainting model for semantic data based on generative multinomial diffusion. To maintain semantic consistency, we need to condition the prediction for the missing regions on the known regions. We propose a novel and efficient condition strategy, Look-Back Condition (LB-Con), which performs one-step look-back operations during the reverse diffusion process. By doing so, we are able to strengthen the harmonization between unknown and known parts, leading to better completion performance. We have conducted extensive experiments on different datasets, showing our proposed model outperforms commonly used interpolation methods in various robotic applications."
Incremental Learning of Robotic Manipulation Tasks through Virtual Reality Demonstrations,"Rauso, Giuseppe; Caccavale, Riccardo; Finzi, Alberto",,
LiDAR-Visual-Inertial Tightly-coupled Odometry with Adaptive Learnable Fusion Weights,"Hulchuk, Vsevolod; Bayer, Jan; Faigl, Jan",,
MultipleCupSuctionNet: Deep Neural Network for Detecting Grasp Pose of a Vacuum Gripper with Multiple Suction Cups based on YOLO Feature Map Affine Transformation,"Jiang, Ping; Komoda, Kazuma; Han, Haifeng; Ooga, Jun'ichiro",,
Exploring Modal Switch in Metamaterial-Based Robots,"Jordan, Britton; Esser, Daniel; Kim, Jeonghyeon; Cho, Brian Y; Webster III, Robert James; Kuntz, Alan",,
Ensuring Joint Constraints of Torque-Controlled Robot Manipulators under Bounded Jerk,"Ko, Dongwoo; Kim, Jonghyeok; Chung, Wan Kyun",,
CBFkit: A Control Barrier Function Toolbox for Robotics Applications,"Black, Mitchell; Fainekos, Georgios; Hoxha, Bardh; OKAMOTO, HIDEKI; Prokhorov, Danil",https://arxiv.org/abs/2404.07158,"This paper introduces CBFKit, a Python/ROS toolbox for safe robotics planning and control under uncertainty. The toolbox provides a general framework for designing control barrier functions for mobility systems within both deterministic and stochastic environments. It can be connected to the ROS open-source robotics middleware, allowing for the setup of multi-robot applications, encoding of environments and maps, and integrations with predictive motion planning algorithms. Additionally, it offers multiple CBF variations and algorithms for robot control. The CBFKit is demonstrated on the Toyota Human Support Robot (HSR) in both simulation and in physical experiments."
Design of a Soft Shell for a Spherical Exploration Robot Traversing Varying Terrain,"Dravid, Meghali Prashant; Oevermann, Micah; McDougall, David; Dugas, David; Ambrose, Robert",,
Imitation learning for sim-to-real adaptation of robotic cutting policies based on residual Gaussian process disturbance force model,"Hathaway, Jamie; Rastegarpanah, Alireza; Stolkin, Rustam",,
Learning Coordinated Maneuver in Adversarial Environments,"Hu, Zechen; Limbu, Manshi; Shishika, Daigo; Xiao, Xuesu; Wang, Xuan",https://arxiv.org/abs/2407.09469,"This paper aims to solve the coordination of a team of robots traversing a route in the presence of adversaries with random positions. Our goal is to minimize the overall cost of the team, which is determined by (i) the accumulated risk when robots stay in adversary-impacted zones and (ii) the mission completion time. During traversal, robots can reduce their speed and act as a `guard' (the slower, the better), which will decrease the risks certain adversary incurs. This leads to a trade-off between the robots' guarding behaviors and their travel speeds. The formulated problem is highly non-convex and cannot be efficiently solved by existing algorithms. Our approach includes a theoretical analysis of the robots' behaviors for the single-adversary case. As the scale of the problem expands, solving the optimal solution using optimization approaches is challenging, therefore, we employ reinforcement learning techniques by developing new encoding and policy-generating methods. Simulations demonstrate that our learning methods can efficiently produce team coordination behaviors. We discuss the reasoning behind these behaviors and explain why they reduce the overall team cost."
"Extensive, Long-term Task and Motion Planning with Signal Temporal Logic Specification for Autonomous Construction","Satoh, Mineto; Takano, Rin; Oyama, Hiroyuki",,
Collaboration Strategies for Two Heterogeneous Pursuers in a Pursuit-evasion Game Using Deep Reinforcement Learning,"Zhong, Zhanping; Dong, Zhuoning; Duan, Xiaoming; He, Jianping",,
SNU-Avatar Haptic Glove: Novel Modularized Haptic Glove via Trigonometric Series Elastic Actuators,"Sung, Eunho; You, Seungbin; Moon, Seongkyeong; Kim, Juhyun; Park, Jaeheung",,
Volumetric Mapping with Panoptic Refinement using Kernel Density Estimation for Mobile Robots,"Nguyen, Khang; Dang, Tuan; Huber, Manfred",,
HP3: Hierarchical Prediction-Pretrained Planning for Unprotected Left Turn,"Ou, Zhihao; Wang, Zhibo; Hua, Yue; Dou, Jinsheng; Feng, Di; Pu, Jian",,
Learning When to Stop: Efficient Active Tactile Perception with Deep Reinforcement Learning,"Niemann, Christopher; Lach, Luca; Leins, David Philip",,
"EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic Under Real-World Perturbations Via Reinforcement Learning","Poudel, Bibek; Li, Weizi; Heaslip, Kevin",https://arxiv.org/abs/2311.12261,"Human-driven vehicles (HVs) amplify naturally occurring perturbations in traffic, leading to congestion--a major contributor to increased fuel consumption, higher collision risks, and reduced road capacity utilization. While previous research demonstrates that Robot Vehicles (RVs) can be leveraged to mitigate these issues, most such studies rely on simulations with simplistic models of human car-following behaviors. In this work, we analyze real-world driving trajectories and extract a wide range of acceleration profiles. We then incorporates these profiles into simulations for training RVs to mitigate congestion. We evaluate the safety, efficiency, and stability of mixed traffic via comprehensive experiments conducted in two mixed traffic environments (Ring and Bottleneck) at various traffic densities, configurations, and RV penetration rates. The results show that under real-world perturbations, prior RV controllers experience performance degradation on all three objectives (sometimes even lower than 100% HVs). To address this, we introduce a reinforcement learning based RV that employs a congestion stage classifier to optimize the safety, efficiency, and stability of mixed traffic. Our RVs demonstrate significant improvements: safety by up to 66%, efficiency by up to 54%, and stability by up to 97%."
LF2SLAM: Learning-based Features For visual SLAM,"Legittimo, Marco; Crocetti, Francesco; Fravolini, Mario Luca; Mollica, Giuseppe; Costante, Gabriele",,
Interactive learning of physical object properties through robot manipulation and database of object measurements,"Kruliak, Andrej; Hartvich, Jiri; Patni, Shubhan; Rustler, Lukas; Behrens, Jan Kristof; Abu-Dakka, Fares; Mikolajczyk, Krystian; Kyrki, Ville; Hoffmann, Matej",https://arxiv.org/abs/2404.07344,"This work presents a framework for automatically extracting physical object properties, such as material composition, mass, volume, and stiffness, through robot manipulation and a database of object measurements. The framework involves exploratory action selection to maximize learning about objects on a table. A Bayesian network models conditional dependencies between object properties, incorporating prior probability distributions and uncertainty associated with measurement actions. The algorithm selects optimal exploratory actions based on expected information gain and updates object properties through Bayesian inference. Experimental evaluation demonstrates effective action selection compared to a baseline and correct termination of the experiments if there is nothing more to be learned. The algorithm proved to behave intelligently when presented with trick objects with material properties in conflict with their appearance. The robot pipeline integrates with a logging module and an online database of objects, containing over 24,000 measurements of 63 objects with different grippers. All code and data are publicly available, facilitating automatic digitization of objects and their physical properties through exploratory manipulations."
Agonist-Antagonist Pouch Motors: Bidirectional Soft Actuators Enhanced by Thermally Responsive Peltier Elements,"Exley, Trevor; Wijesundara Mudiyanselage, Rashmi Diviyanjali; Tan, Nathan; Sunkara, Akshay; He, Xinyu; Wang, Shuopu; Chan, Bonnie; Jain, Aditya Jain; Espinosa, Luis; Jafari, Amir",https://arxiv.org/abs/2403.10955,"In this study, we introduce a novel Mylar-based pouch motor design that leverages the reversible actuation capabilities of Peltier junctions to enable agonist-antagonist muscle mimicry in soft robotics. Addressing the limitations of traditional silicone-based materials, such as leakage and phase-change fluid degradation, our pouch motors filled with Novec 7000 provide a durable and leak-proof solution for geometric modeling. The integration of flexible Peltier junctions offers a significant advantage over conventional Joule heating methods by allowing active and reversible heating and cooling cycles. This innovation not only enhances the reliability and longevity of soft robotic applications but also broadens the scope of design possibilities, including the development of agonist-antagonist artificial muscles, grippers with can manipulate through flexion and extension, and an anchor-slip style simple crawler design. Our findings indicate that this approach could lead to more efficient, versatile, and durable robotic systems, marking a significant advancement in the field of soft robotics."
Gradual Receptive Expansion Using Vision Transformer for Online 3D Bin Packing,"Kang, Minjae; Kee, Hogun; Park, Yoseph; Kim, Junseok; Jeong, Jaeyeon; Cheon, Geunje; Lee, Jaewon; Oh, Songhwai",,
"Seven Benefits of Using Series Elastic Actuators in Design of an Affordable, Simple Controlled, and Functional Prosthetic Hand","Koochakzadeh, Erfan; Kargar, Alireza; Sattari, Parsa; Ravanshid, Diba; Nasiri, Rezvan",,
A Novel MR Safe Double-Arch Needle Insertion Robot with Scissor-Folding Mechanism for Abdominal Interventions,"Liang, Ziting; Chuang, Lu; Yang, Haoqian; Hashem, Ryman; Abdelaziz, Mohamed Essam Mohamed Kassem; Lindenroth, Lukas; Bandula, Steve; Stoyanov, Danail; Stilli, Agostino",,
This is the Way: Mitigating the Roll of an Autonomous Uncrewed Surface Vessel in Wavy Conditions Using Model Predictive Control,"Jenkins, Daniel; Marshall, Joshua A.",https://arxiv.org/abs/2408.15349,"Though larger vessels may be well-equipped to deal with wavy conditions, smaller vessels are often more susceptible to disturbances. This paper explores the development of a nonlinear model predictive control (NMPC) system for Uncrewed Surface Vessels (USVs) in wavy conditions to minimize average roll. The NMPC is based on a prediction method that uses information about the vessel's dynamics and an assumed wave model. This method is able to mitigate the roll of an under-actuated USV in a variety of conditions by adjusting the weights of the cost function. The results show a reduction of 39% of average roll with a tuned controller in conditions with 1.75-metre sinusoidal waves. A general and intuitive tuning strategy is established. This preliminary work is a proof of concept which sets the stage for the leveraging of wave prediction methodologies to perform planning and control in real time for USVs in real-world scenarios and field trials."
A Geometry-based Approach for Support-free Additive Manufacturing of Structures with Large Overhang Angles and Closed Features,"Liu, Jitian; Cohen, Zachary; Kim, Jin Seob; Armand, Mehran; Kutzer, Michael Dennis Mays",,
Multi-Uncertainty Aware Autonomous Cooperative Planning,"Zhang, Shiyao; Li, He; Zhang, Shengyu; Wang, Shuai; Ng, Derrick Wing Kwan; Xu, Chengzhong",https://arxiv.org/abs/2110.06401,"Multi-agent mapping is a fundamentally important capability for autonomous robot task coordination and execution in complex environments. While successful algorithms have been proposed for mapping using individual platforms, cooperative online mapping for teams of robots remains largely a challenge. We focus on probabilistic variants of mapping due to its potential utility in downstream tasks such as uncertainty-aware path-planning. A critical question to enabling this capability is how to process and aggregate incrementally observed local information among individual platforms, especially when their ability to communicate is intermittent. We put forth an Incremental Sparse Gaussian Process (GP) methodology for multi-robot mapping, where the regression is over a truncated signed-distance field (TSDF). Doing so permits each robot in the network to track a local estimate of a pseudo-point approximation GP posterior and perform weighted averaging of its parameters with those of its (possibly time-varying) set of neighbors. We establish conditions on the pseudo-point representation, as well as communication protocol, such that robots' local GPs converge to the one with globally aggregated information. We further provide experiments that corroborate our theoretical findings for probabilistic multi-robot mapping."
ROS-lite2: Autonomous-driving Software Platform for Clustered Many-core Processor,"Tajima, Yuta; Azumi, Takuya",,
Head-Mounted Hydraulic Needle Driver for Targeted Interventions in Neurosurger,"FANG, Zhiwei; Xu, Chao; Gao, Huxin; Chan, Tat-Ming; Yuan, Wu; Ren, Hongliang",,
Reactive Temporal Logic-based Planning and Control for Interactive Robotic Tasks,"Savvas Sadiq Ali, Farhad Nawaz; Peng, Shaoting; Lindemann, Lars; Figueroa, Nadia; Matni, Nikolai",https://arxiv.org/abs/2404.19594,"Robots interacting with humans must be safe, reactive and adapt online to unforeseen environmental and task changes. Achieving these requirements concurrently is a challenge as interactive planners lack formal safety guarantees, while safe motion planners lack flexibility to adapt. To tackle this, we propose a modular control architecture that generates both safe and reactive motion plans for human-robot interaction by integrating temporal logic-based discrete task level plans with continuous Dynamical System (DS)-based motion plans. We formulate a reactive temporal logic formula that enables users to define task specifications through structured language, and propose a planning algorithm at the task level that generates a sequence of desired robot behaviors while being adaptive to environmental changes. At the motion level, we incorporate control Lyapunov functions and control barrier functions to compute stable and safe continuous motion plans for two types of robot behaviors: (i) complex, possibly periodic motions given by autonomous DS and (ii) time-critical tasks specified by Signal Temporal Logic~(STL). Our methodology is demonstrated on the Franka robot arm performing wiping tasks on a whiteboard and a mannequin that is compliant to human interactions and adaptive to environmental changes."
Loco-Manipulation with Nonimpulsive Contact-Implicit Planning in a Slithering Robot,"Salagame, Adarsh; Gangaraju, Kruthika; Nallaguntla, Harin Kumar; Sihite, Eric; Schirner, Gunar; Ramezani, Alireza",https://arxiv.org/abs/2404.08174,"Object manipulation has been extensively studied in the context of fixed base and mobile manipulators. However, the overactuated locomotion modality employed by snake robots allows for a unique blend of object manipulation through locomotion, referred to as loco-manipulation. The following work presents an optimization approach to solving the loco-manipulation problem based on non-impulsive implicit contact path planning for our snake robot COBRA. We present the mathematical framework and show high-fidelity simulation results and experiments to demonstrate the effectiveness of our approach."
AdvDiffuser: Generating Adversarial Safety-Critical Driving Scenarios via Guided Diffusion,"Xie, yuting; Guo, Xianda; Wang, Cong; Kunhua, Liu; Chen, Long",,
"Harnessing Natural Oscillations for High-Speed, Efficient Asymmetrical Locomotion in Quadrupedal Robots","Cheng, Jing; Alqaham, Yasser G.; Gan, Zhenyu",https://arxiv.org/abs/2405.17579,"This study explores the dynamics of asymmetrical bounding gaits in quadrupedal robots, focusing on the integration of torso pitching and hip motion to enhance speed and stability. Traditional control strategies often enforce a fixed posture, minimizing natural body movements to simplify the control problem. However, this approach may overlook the inherent dynamical advantages found in natural locomotion. By considering the robot as two interconnected segments, we concentrate on stance leg motion while allowing passive torso oscillation, drawing inspiration from natural dynamics and underactuated robotics principles. Our control scheme employs Linear Inverted Pendulum (LIP) and Spring-Loaded Inverted Pendulum (SLIP) models to govern front and rear leg movements independently. This approach has been validated through extensive simulations and hardware experiments, demonstrating successful high-speed locomotion with top speeds nearing 4 m/s and reduced ground reaction forces, indicating a more efficient gait. Furthermore, unlike conventional methods, our strategy leverages natural torso oscillations to aid leg circulation and stride length, aligning robot dynamics more closely with biological counterparts. Our findings suggest that embracing the natural dynamics of quadrupedal movement, particularly in asymmetrical gaits like bounding, can lead to more stable, efficient, and high-speed robotic locomotion. This investigation lays the groundwork for future studies on versatile and dynamic quadrupedal gaits and their potential applications in scenarios demanding rapid and effective locomotion."
Spatial Spinal Fixation: A Transformative Approach Using a Unique Robot-Assisted Steerable Drilling System and Flexible Pedicle Screw,"Sharma, Susheela; Kulkarni, Yash; Go, Sarah; Bonyun, Jeff; Amadio, Jordan P.; Rajebi, Mohammad; Khadem, Mohsen; Alambeigi, Farshid",https://arxiv.org/abs/2405.17600,"Spinal fixation procedures are currently limited by the rigidity of the existing instruments and pedicle screws leading to fixation failures and rigid pedicle screw pull out. Leveraging our recently developed Concentric Tube Steerable Drilling Robot (CT-SDR) in integration with a robotic manipulator, to address the aforementioned issue, here we introduce the transformative concept of Spatial Spinal Fixation (SSF) using a unique Flexible Pedicle Screw (FPS). The proposed SSF procedure enables planar and out-of-plane placement of the FPS throughout the full volume of the vertebral body. In other words, not only does our fixation system provide the option of drilling in-plane and out-of-plane trajectories, it also enables implanting the FPS inside linear (represented by an I-shape) and/or non-linear (represented by J-shape) trajectories. To thoroughly evaluate the functionality of our proposed robotic system and the SSF procedure, we have performed various experiments by drilling different I-J and J-J drilling trajectory pairs into our custom-designed L3 vertebral phantoms and analyzed the accuracy of the procedure using various metrics."
Learning Safe Locomotion for Quadrupedal Robots by Derived-Action Optimization,"Zhu, Deye; Zhu, Chengrui; Zhang, Zhen; Xin, Shuo; Liu, Yong",,
Online Rotor Fault Detection and Isolation for Vertical Takeoff and Landing Vehicles,"Lian, Jiaqi; Gandhi, Neeraj; Wang, Yifan; Phan, Linh Thi Xuan",,
LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight Loco-Manipulators,"Lin, Changyi; Liu, Xingyu; yang, Yuxiang; Niu, Yaru; Yu, Wenhao; Zhang, Tingnan; Tan, Jie; Boots, Byron; ZHAO, DING",https://arxiv.org/abs/2403.18197,"Quadrupedal robots have emerged as versatile agents capable of locomoting and manipulating in complex environments. Traditional designs typically rely on the robot's inherent body parts or incorporate top-mounted arms for manipulation tasks. However, these configurations may limit the robot's operational dexterity, efficiency and adaptability, particularly in cluttered or constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal robot with a novel morphology to perform versatile manipulation in diverse constrained environments. By equipping a Unitree Go1 robot with two low-cost and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan leverages the combined mobility and functionality of the legs and grippers for complex manipulation tasks that require precise 6D positioning of the end effector in a wide workspace. To harness the loco-manipulation capabilities of LocoMan, we introduce a unified control framework that extends the whole-body controller (WBC) to integrate the dynamics of loco-manipulators. Through experiments, we validate that the proposed whole-body controller can accurately and stably follow desired 6D trajectories of the end effector and torso, which, when combined with the large workspace from our design, facilitates a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation."
"RoboGuardZ: A Scalable, Lightweight Zero-Shot Learning Framework for Zero-Day Malware-Driven Controller Attack Detection in Robots","KAUR, UPINDER; Celik, Berkay; Voyles, Richard",,
Design and Control of an Ultra-Slender Push-Pull Multisection Continuum Manipulator for In-Situ Inspection of Aeroengine,"Zhong, Weiheng; Huang, Yuancan; Hong, Da; Shao, Nianfeng",,
Design of a Variable Wheel-propeller Integrated Mechanism for Amphibious Robots,"Lu, Liang; Gao, Xiangquan; Xiang, Ming; Yan, Zefeng; Han, Bin",,
Disp2Depth: Hybrid Stereo Dense Depth Estimation for Robotics Tasks in Industrial Automation,"Singh, Suhani; Suppa, Michael; Suarez, Raul; Rosell, Jan",,
Heading Control for Obstacle Avoidance using Dynamic Posture Manipulation during Tumbling Locomotion,"Salagame, Adarsh; Gangaraju, Kruthika; Sihite, Eric; Schirner, Gunar; Ramezani, Alireza",,
Adaptive multi-altitude search and sampling of sparsely distributed natural phenomena,"Todd, Jessica; McCammon, Seth; Girdhar, Yogesh; Roy, Nicholas; Yoerger, Dana",,
Loss Distillation via Gradient Matching for Point Cloud Completion with Weighted Chamfer Distance,"Lin, Fangzhou; Liu, Haotian; Zhou, Haoying; Hou, Songlin; Yamada, Kazunori; Fischer, Gregory Scott; Li, Yanhua; Zhang, Haichong; Zhang, Ziming",https://arxiv.org/abs/2409.06171,"3D point clouds enhanced the robot's ability to perceive the geometrical information of the environments, making it possible for many downstream tasks such as grasp pose detection and scene understanding. The performance of these tasks, though, heavily relies on the quality of data input, as incomplete can lead to poor results and failure cases. Recent training loss functions designed for deep learning-based point cloud completion, such as Chamfer distance (CD) and its variants (\eg HyperCD ), imply a good gradient weighting scheme can significantly boost performance. However, these CD-based loss functions usually require data-related parameter tuning, which can be time-consuming for data-extensive tasks. To address this issue, we aim to find a family of weighted training losses ({\em weighted CD}) that requires no parameter tuning. To this end, we propose a search scheme, {\em Loss Distillation via Gradient Matching}, to find good candidate loss functions by mimicking the learning behavior in backpropagation between HyperCD and weighted CD. Once this is done, we propose a novel bilevel optimization formula to train the backbone network based on the weighted CD loss. We observe that: (1) with proper weighted functions, the weighted CD can always achieve similar performance to HyperCD, and (2) the Landau weighted CD, namely {\em Landau CD}, can outperform HyperCD for point cloud completion and lead to new state-of-the-art results on several benchmark datasets. {\it Our demo code is available at \url{https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD}.}"
Incrementally Building Room-Scale Language-Embedded Gaussian Splats (LEGS) with a Mobile Robot,"Yu, Justin; Hari, Kush; Srinivas, Kishore; El-Refai, Karim; Rashid, Adam; Kim, Chung Min; Kerr, Justin; Cheng, Richard; Balakrishna, Ashwin; Kollar, Thomas; Goldberg, Ken",,
Adaptive Planning with Generative Models under Uncertainty,"Jutras-Dube, Pascal; Zhang, Ruqi; Bera, Aniket",https://arxiv.org/abs/2408.01510,"Planning with generative models has emerged as an effective decision-making paradigm across a wide range of domains, including reinforcement learning and autonomous navigation. While continuous replanning at each timestep might seem intuitive because it allows decisions to be made based on the most recent environmental observations, it results in substantial computational challenges, primarily due to the complexity of the generative model's underlying deep learning architecture. Our work addresses this challenge by introducing a simple adaptive planning policy that leverages the generative model's ability to predict long-horizon state trajectories, enabling the execution of multiple actions consecutively without the need for immediate replanning. We propose to use the predictive uncertainty derived from a Deep Ensemble of inverse dynamics models to dynamically adjust the intervals between planning sessions. In our experiments conducted on locomotion tasks within the OpenAI Gym framework, we demonstrate that our adaptive planning policy allows for a reduction in replanning frequency to only about 10% of the steps without compromising the performance. Our results underscore the potential of generative modeling as an efficient and effective tool for decision-making."
Motion Planning for Object Manipulation by Edge-Rolling,"Boroji, Maede; Danesh, Vahid; Kao, Imin; Fakhari, Amin",,
Learning autonomous driving from aerial imagery,"Murali, Varun; Rosman, Guy; Karaman, Sertac; Rus, Daniela",https://arxiv.org/abs/1810.10438,"Semantic segmentation has been one of the leading research interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the top views of the objects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more information for object recognition. In this paper, we introduce our UAVid dataset, a new high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction. Our UAVid website and the labeling tool have been published https://uavid.nl/."
AirShot: Efficient Few-Shot Detection for Autonomous Exploration,"Wang, Zihan; Li, Bowen; Wang, Chen; Scherer, Sebastian",https://arxiv.org/abs/2404.05069,"Few-shot object detection has drawn increasing attention in the field of robotic exploration, where robots are required to find unseen objects with a few online provided examples. Despite recent efforts have been made to yield online processing capabilities, slow inference speeds of low-powered robots fail to meet the demands of real-time detection-making them impractical for autonomous exploration. Existing methods still face performance and efficiency challenges, mainly due to unreliable features and exhaustive class loops. In this work, we propose a new paradigm AirShot, and discover that, by fully exploiting the valuable correlation map, AirShot can result in a more robust and faster few-shot object detection system, which is more applicable to robotics community. The core module Top Prediction Filter (TPF) can operate on multi-scale correlation maps in both the training and inference stages. During training, TPF supervises the generation of a more representative correlation map, while during inference, it reduces looping iterations by selecting top-ranked classes, thus cutting down on computational costs with better performance. Surprisingly, this dual functionality exhibits general effectiveness and efficiency on various off-the-shelf models. Exhaustive experiments on COCO2017, VOC2014, and SubT datasets demonstrate that TPF can significantly boost the efficacy and efficiency of most off-the-shelf models, achieving up to 36.4% precision improvements along with 56.3% faster inference speed. Code and Data are at: https://github.com/ImNotPrepared/AirShot."
Magnetic Field Aided Vehicle Localization with Acceleration Correction,"Deshpande, Mrunmayee; Majji, Manoranjan; Ramos, J Humberto",,
Design and implementation of a novel wheel-based cable inspection robot,"Hou, Mengqi; Li, Jie; Xu, Fengyu; HU, LeZhi",,
"A Feasibility Study of a Soft, Low-Cost, 6-Axis Load Cell for Haptics","Veliky, Madison; Johnston, Garrison; Yildiz, Ahmet; Simaan, Nabil",,
ActiveRIR: Active Audio-Visual Exploration for Acoustic Environment Modeling,"Somayazulu, Arjun; Majumder, Sagnik; Chen, Changan; Grauman, Kristen",https://arxiv.org/abs/2404.16216,"An environment acoustic model represents how sound is transformed by the physical characteristics of an indoor environment, for any given source/receiver location. Traditional methods for constructing acoustic models involve expensive and time-consuming collection of large quantities of acoustic data at dense spatial locations in the space, or rely on privileged knowledge of scene geometry to intelligently select acoustic data sampling locations. We propose active acoustic sampling, a new task for efficiently building an environment acoustic model of an unmapped environment in which a mobile agent equipped with visual and acoustic sensors jointly constructs the environment acoustic model and the occupancy map on-the-fly. We introduce ActiveRIR, a reinforcement learning (RL) policy that leverages information from audio-visual sensor streams to guide agent navigation and determine optimal acoustic data sampling positions, yielding a high quality acoustic model of the environment from a minimal set of acoustic samples. We train our policy with a novel RL reward based on information gain in the environment acoustic model. Evaluating on diverse unseen indoor environments from a state-of-the-art acoustic simulation platform, ActiveRIR outperforms an array of methods--both traditional navigation agents based on spatial novelty and visual exploration as well as existing state-of-the-art methods."
ManiFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots,"Xu, Zhixuan; Gao, Chongkai; Liu, Zixuan; Yang, Gang; Tie, Chenrui; Zheng, Haozhuo; Zhou, Haoyu; Weikun, Peng; Wang, Debang; Chen, Tianyi; Yu, Zhouliang; Shao, Lin",https://arxiv.org/abs/2405.06964,"To substantially enhance robot intelligence, there is a pressing need to develop a large model that enables general-purpose robots to proficiently undertake a broad spectrum of manipulation tasks, akin to the versatile task-planning ability exhibited by LLMs. The vast diversity in objects, robots, and manipulation tasks presents huge challenges. Our work introduces a comprehensive framework to develop a foundation model for general robotic manipulation that formalizes a manipulation task as contact synthesis. Specifically, our model takes as input object and robot manipulator point clouds, object physical attributes, target motions, and manipulation region masks. It outputs contact points on the object and associated contact forces or post-contact motions for robots to achieve the desired manipulation task. We perform extensive experiments both in the simulation and real-world settings, manipulating articulated rigid objects, rigid objects, and deformable objects that vary in dimensionality, ranging from one-dimensional objects like ropes to two-dimensional objects like cloth and extending to three-dimensional objects such as plasticine. Our model achieves average success rates of around 90\%. Supplementary materials and videos are available on our project website at https://manifoundationmodel.github.io/."
RoboCop: A Robust Zero-Day Cyber-Physical Attack Detection Framework for Robots,"KAUR, UPINDER; Celik, Berkay; Voyles, Richard",,
ROG-Map: An Efficient Robocentric Occupancy Grid Map for Large-scene and High-resolution LiDAR-based Motion Planning,"Ren, Yunfan; Cai, Yixi; Zhu, Fangcheng; Liang, Siqi; Zhang, Fu",https://arxiv.org/abs/2302.14819,"Recent advances in LiDAR technology have opened up new possibilities for robotic navigation. Given the widespread use of occupancy grid maps (OGMs) in robotic motion planning, this paper aims to address the challenges of integrating LiDAR with OGMs. To this end, we propose ROG-Map, a uniform grid-based OGM that maintains a local map moving along with the robot to enable efficient map operation and reduce memory costs for large-scene autonomous flight. Moreover, we present a novel incremental obstacle inflation method that significantly reduces the computational cost of inflation. The proposed method outperforms state-of-the-art (SOTA) methods on various public datasets. To demonstrate the effectiveness and efficiency of ROG-Map, we integrate it into a complete quadrotor system and perform autonomous flights against both small obstacles and large-scale scenes. During real-world flight tests with a 0.05 m resolution local map and 30mx30mx12m local map size, ROG-Map takes only 29.8% of frame time on average to update the map at a frame rate of 50 Hz (\ie, 5.96 ms in 20 ms), including 0.33% (i.e., 0.66 ms) to perform obstacle inflation, demonstrating outstanding real-world performance. We release ROG-Map as an open-source ROS package to promote the development of LiDAR-based motion planning."
CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers,"Ranne, Alex; Kuang, Liming; Velikova, Yordanka; Navab, Nassir; Rodriguez y Baena, Ferdinando",https://arxiv.org/abs/2403.14465,"In minimally invasive endovascular procedures, contrast-enhanced angiography remains the most robust imaging technique. However, it is at the expense of the patient and clinician's health due to prolonged radiation exposure. As an alternative, interventional ultrasound has notable benefits such as being radiation-free, fast to deploy, and having a small footprint in the operating room. Yet, ultrasound is hard to interpret, and highly prone to artifacts and noise. Additionally, interventional radiologists must undergo extensive training before they become qualified to diagnose and treat patients effectively, leading to a shortage of staff, and a lack of open-source datasets. In this work, we seek to address both problems by introducing a self-supervised deep learning architecture to segment catheters in longitudinal ultrasound images, without demanding any labeled data. The network architecture builds upon AiAReSeg, a segmentation transformer built with the Attention in Attention mechanism, and is capable of learning feature changes across time and space. To facilitate training, we used synthetic ultrasound data based on physics-driven catheter insertion simulations, and translated the data into a unique CT-Ultrasound common domain, CACTUSS, to improve the segmentation performance. We generated ground truth segmentation masks by computing the optical flow between adjacent frames using FlowNet2, and performed thresholding to obtain a binary map estimate. Finally, we validated our model on a test dataset, consisting of unseen synthetic data and images collected from silicon aorta phantoms, thus demonstrating its potential for applications to clinical data in the future."
PathFormer: A Transformer-Based Framework for Vision-Centric Autonomous Navigation in Off-Road Environments,"Hassan, Bilal; Abdel Madjid, Nadya; Kashwani, Fatima; Alansari, Mohamad; Khonji, Majid; Dias, Jorge",,
BEV-ODOM: Reducing Scale Drift in Monocular Visual Odometry with BEV Representation,"Wei, Yufei; Lu, Sha; Xiong, Rong; Wang, Yue",,
Ground-Density Clustering for Approximate Agricultural Field Segmentation,"Nelson, Henry J.; Papanikolopoulos, Nikos",,
Advancing ASV Autonomy for Environmental Cleanup: A Deep Reinforcement Learning Framework for Floating Waste Capture,"Wolf Batista, Luis Felipe; Ro, Junghwan; Richard, Antoine; Schroepfer, Pete; Hutchinson, Seth; Pradalier, Cedric",,
PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving,"Zhao, Zhipeng; Li, Bowen; Du, Yi; Fu, Taimeng; Wang, Chen",https://arxiv.org/abs/2404.01596,"Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equation, into data-driven neural models for motion prediction in off-road driving. Our experiments showed that PhysORD can accurately predict vehicle motion and tolerate external disturbance by modeling uncertainties. It outperforms existing methods both in accuracy and efficiency and demonstrates data-efficient learning and generalization ability in long-term prediction."
Scalable Networked Feature Selection with Randomized Algorithm for Robot Navigation,"Pandey, Vivek; Amini, Arash; Liu, Guangyi; Topcu, Ufuk; Sun, Qiyu; Daniilidis, Kostas; Motee, Nader",https://arxiv.org/abs/2403.12279,"We address the problem of sparse selection of visual features for localizing a team of robots navigating an unknown environment, where robots can exchange relative position measurements with neighbors. We select a set of the most informative features by anticipating their importance in robots localization by simulating trajectories of robots over a prediction horizon. Through theoretical proofs, we establish a crucial connection between graph Laplacian and the importance of features. We show that strong network connectivity translates to uniformity in feature importance, which enables uniform random sampling of features and reduces the overall computational complexity. We leverage a scalable randomized algorithm for sparse sums of positive semidefinite matrices to efficiently select the set of the most informative features and significantly improve the probabilistic performance bounds. Finally, we support our findings with extensive simulations."
Spike-based high energy efficiency and accuracy tracker for Robot,"Qu, Jinye; Gao, Zeyu; Yi, Li; Lu, Yanfeng; Qiao, Hong",,
MAP-NBV: Multi-agent Prediction-guided Next-Best-View Planning for Active 3D Object Reconstruction,"Dhami, Harnaik; Sharma, Vishnu D.; Tokekar, Pratap",https://arxiv.org/abs/2307.04004,"Next-Best View (NBV) planning is a long-standing problem of determining where to obtain the next best view of an object from, by a robot that is viewing the object. There are a number of methods for choosing NBV based on the observed part of the object. In this paper, we investigate how predicting the unobserved part helps with the efficiency of reconstructing the object. We present, Multi-Agent Prediction-Guided NBV (MAP-NBV), a decentralized coordination algorithm for active 3D reconstruction with multi-agent systems. Prediction-based approaches have shown great improvement in active perception tasks by learning the cues about structures in the environment from data. However, these methods primarily focus on single-agent systems. We design a decentralized next-best-view approach that utilizes geometric measures over the predictions and jointly optimizes the information gain and control effort for efficient collaborative 3D reconstruction of the object. Our method achieves 19% improvement over the non-predictive multi-agent approach in simulations using AirSim and ShapeNet. We make our code publicly available through our project website: http://raaslab.org/projects/MAPNBV/."
Learning for Kinodynamic Tree Expansion,"Lai, Tin; Zhi, Weiming; Hermans, Tucker; Ramos, Fabio",https://arxiv.org/abs/2203.00975,"We present the Learning for KinoDynamic Tree Expansion (L4KDE) method for kinodynamic planning. Tree-based planning approaches, such as rapidly exploring random tree (RRT), are the dominant approach to finding globally optimal plans in continuous state-space motion planning. Central to these approaches is tree-expansion, the procedure in which new nodes are added into an ever-expanding tree. We study the kinodynamic variants of tree-based planning, where we have known system dynamics and kinematic constraints. In the interest of quickly selecting nodes to connect newly sampled coordinates, existing methods typically cannot optimise to find nodes that have low cost to transition to sampled coordinates. Instead, they use metrics like Euclidean distance between coordinates as a heuristic for selecting candidate nodes to connect to the search tree. We propose L4KDE to address this issue. L4KDE uses a neural network to predict transition costs between queried states, which can be efficiently computed in batch, providing much higher quality estimates of transition cost compared to commonly used heuristics while maintaining almost-surely asymptotic optimality guarantee. We empirically demonstrate the significant performance improvement provided by L4KDE on a variety of challenging system dynamics, with the ability to generalise across different instances of the same model class, and in conjunction with a suite of modern tree-based motion planners."
Visual Forecasting as a Mid-level Representation for Avoidance,"Yang, Hsuan-Kung; Chiang, Tsung-Chih; Liu, Ting-Ru; Liu, Jou-Min; Huang, Chun-Wei; Lee, Chun-Yi",https://arxiv.org/abs/2310.07724,"The challenge of navigation in environments with dynamic objects continues to be a central issue in the study of autonomous agents. While predictive methods hold promise, their reliance on precise state information makes them less practical for real-world implementation. This study presents visual forecasting as an innovative alternative. By introducing intuitive visual cues, this approach projects the future trajectories of dynamic objects to improve agent perception and enable anticipatory actions. Our research explores two distinct strategies for conveying predictive information through visual forecasting: (1) sequences of bounding boxes, and (2) augmented paths. To validate the proposed visual forecasting strategies, we initiate evaluations in simulated environments using the Unity engine and then extend these evaluations to real-world scenarios to assess both practicality and effectiveness. The results confirm the viability of visual forecasting as a promising solution for navigation and obstacle avoidance in dynamic environments."
Accurate and Efficient Loop Closure Detection With Deep Binary Image Descriptor and Augmented Point Cloud Registration,"Wang, Jialiang; Gao, Zhi; LIN, Zhipeng; Zhou, Zhiyu; Wang, Xiaonan; CHENG, Jianhua; Zhang, Hao; Liu, Xinyi; Chen, Ben M.",,
BEVLoc: Off-Road Aerial-to-Ground Localization and Matching via Birds-Eye-View Synthesis,"Klammer, Christopher; Kaess, Michael",,
CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration,"Ionova, Marina; Behrens, Jan Kristof",https://arxiv.org/abs/2403.18459,"Assembly processes involving humans and robots are challenging scenarios because the individual activities and access to shared workspace have to be coordinated. Fixed robot programs leave no room to diverge from a fixed protocol. Working on such a process can be stressful for the user and lead to ineffective behavior or failure. We propose a novel approach of online constraint-based scheduling in a reactive execution control framework facilitating behavior trees called CoBOS. This allows the robot to adapt to uncertain events such as delayed activity completions and activity selection (by the human). The user will experience less stress as the robotic coworkers adapt their behavior to best complement the human-selected activities to complete the common task. In addition to the improved working conditions, our algorithm leads to increased efficiency, even in highly uncertain scenarios. We evaluate our algorithm using a probabilistic simulation study with 56000 experiments. We outperform all baselines by a margin of 4-10%. Initial real robot experiments using a Franka Emika Panda robot and human tracking based on HTC Vive VR gloves look promising."
Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint,"Ma, Haitong; Ren, Zhaolin; Dai, Bo; Li, Na",https://arxiv.org/abs/2404.05051,"We study sim-to-real skill transfer and discovery in the context of robotics control using representation learning. We draw inspiration from spectral decomposition of Markov decision processes. The spectral decomposition brings about representation that can linearly represent the state-action value function induced by any policies, thus can be regarded as skills. The skill representations are transferable across arbitrary tasks with the same transition dynamics. Moreover, to handle the sim-to-real gap in the dynamics, we propose a skill discovery algorithm that learns new skills caused by the sim-to-real gap from real-world data. We promote the discovery of new skills by enforcing orthogonal constraints between the skills to learn and the skills from simulators, and then synthesize the policy using the enlarged skill sets. We demonstrate our methodology by transferring quadrotor controllers from simulators to Crazyflie 2.1 quadrotors. We show that we can learn the skill representations from a single simulator task and transfer these to multiple different real-world tasks including hovering, taking off, landing and trajectory tracking. Our skill discovery approach helps narrow the sim-to-real gap and improve the real-world controller performance by up to 30.2%."
RMap: Millimeter-Wave Radar Mapping Through Volumetric UpSampling,"Mopidevi, Ajay Narasimha; Harlow, Kyle; Heckman, Christoffer",https://arxiv.org/abs/2310.13188,"Millimeter Wave Radar is being adopted as a viable alternative to lidar and radar in adverse visually degraded conditions, such as the presence of fog and dust. However, this sensor modality suffers from severe sparsity and noise under nominal conditions, which makes it difficult to use in precise applications such as mapping. This work presents a novel solution to generate accurate 3D maps from sparse radar point clouds. RMap uses a custom generative transformer architecture, UpPoinTr, which upsamples, denoises, and fills the incomplete radar maps to resemble lidar maps. We test this method on the ColoRadar dataset to demonstrate its efficacy."
Toward Perpetual Occlusion-Aware Observation of Comb States in Living Honeybee Colonies,"Blaha, Jan; Vintr, Tomas; Mikula, Jan; Janota, Ji&#345;í; Rou&#269;ek, Tomá; Ulrich, Jiri; Rekabi Bana, Fatemeh; Arvin, Farshad; Kulich, Miroslav; Krajnik, Tomas",,
"X-neuron: Interpreting, Locating and Editing of Neurons in Reinforcement Learning Policy","Ge, Yuhong; Zhao, Xun; Pang, Jiangmiao; Zhao, Mingguo; Lin, Dahua",,
Event-intensity Stereo with Cross-modal Fusion and Contrast,"Wang, Yuanbo; Qu, Shanglai; Meng, Tianyu; Cui, Yan; Piao, Haiyin; Wei, Xiaopeng; Yang, Xin",,
Data-Driven Koopman Operator-Based Error-State Kalman Filter for Enhancing State Estimation of Quadrotors in Agile Flight,"Huang, Peng; Zheng, Ketong; Fettweis, Gerhard",,
Self Supervised Detection of Incorrect Human Demonstrations: A Path Toward Safe Imitation Learning by Robots in the Wild,"Sojib, Noushad; Begum, Momotaz",,
Identification and validation of the dynamic model of a tendon-driven anthropomorphic finger,"Li, Junnan; Chen, Lingyun; Ringwald, Johannes; Pozo Fortuni&#263;, Edmundo; Ganguly, Amartya; Haddadin, Sami",https://arxiv.org/abs/2408.13044,"This study addresses the absence of an identification framework to quantify a comprehensive dynamic model of human and anthropomorphic tendon-driven fingers, which is necessary to investigate the physiological properties of human fingers and improve the control of robotic hands. First, a generalized dynamic model was formulated, which takes into account the inherent properties of such a mechanical system. This includes rigid-body dynamics, coupling matrix, joint viscoelasticity, and tendon friction. Then, we propose a methodology comprising a series of experiments, for step-wise identification and validation of this dynamic model. Moreover, an experimental setup was designed and constructed that features actuation modules and peripheral sensors to facilitate the identification process. To verify the proposed methodology, a 3D-printed robotic finger based on the index finger design of the Dexmart hand was developed, and the proposed experiments were executed to identify and validate its dynamic model. This study could be extended to explore the identification of cadaver hands, aiming for a consistent dataset from a single cadaver specimen to improve the development of musculoskeletal hand models."
Neuro-Explorer: Efficient and Scalable Exploration Planning via Learned Frontier Regions,"Han, Kyung Min; Kim, Young J.",,
Constrained 6-DoF Grasp Generation on Complex Shapes for Improved Dual-Arm Manipulation,"Singh, Gaurav; Kalwar, Sanket; Karim, Md Faizal; Sen, Bipasha; Govindan, Nagamanikandan; Sridhar, Srinath; Krishna, Madhava",https://arxiv.org/abs/2404.04643,"Efficiently generating grasp poses tailored to specific regions of an object is vital for various robotic manipulation tasks, especially in a dual-arm setup. This scenario presents a significant challenge due to the complex geometries involved, requiring a deep understanding of the local geometry to generate grasps efficiently on the specified constrained regions. Existing methods only explore settings involving table-top/small objects and require augmented datasets to train, limiting their performance on complex objects. We propose CGDF: Constrained Grasp Diffusion Fields, a diffusion-based grasp generative model that generalizes to objects with arbitrary geometries, as well as generates dense grasps on the target regions. CGDF uses a part-guided diffusion approach that enables it to get high sample efficiency in constrained grasping without explicitly training on massive constraint-augmented datasets. We provide qualitative and quantitative comparisons using analytical metrics and in simulation, in both unconstrained and constrained settings to show that our method can generalize to generate stable grasps on complex objects, especially useful for dual-arm manipulation settings, while existing methods struggle to do so."
ODD-diLLMma: Driving Automation System ODD Compliance Checking using LLMs,"Hildebrandt, Carl; Woodlief, Trey; Elbaum, Sebastian",,
PickScan: Object discovery and reconstruction from handheld interactions,"van der Brugge, Vincent Daniel; Pollefeys, Marc; Tenenbaum, Joshua; Jatavallabhula, Krishna Murthy; Tewari, Ayush",,
Roadmaps with Gaps over Controllers: Achieving Efficiency in Planning under Dynamics,"Sivaramakrishnan, Aravind; Tangirala, Sumanth; Granados, Edgar; Carver, Noah; Bekris, Kostas E.",https://arxiv.org/abs/2310.03239,"This paper aims to improve the computational efficiency of motion planning for mobile robots with non-trivial dynamics through the use of learned controllers. It adopts a decoupled strategy, where a system-specific controller is first trained offline in an empty environment to deal with the robot's dynamics. For a target environment, the proposed approach constructs offline a data structure, a ""Roadmap with Gaps,"" to approximately learn how to solve planning queries in this environment using the learned controller. The nodes of the roadmap correspond to local regions. Edges correspond to applications of the learned control policy that approximately connect these regions. Gaps arise because the controller does not perfectly connect pairs of individual states along edges. Online, given a query, a tree sampling-based motion planner uses the roadmap so that the tree's expansion is informed towards the goal region. The tree expansion selects local subgoals given a wavefront on the roadmap that guides towards the goal. When the controller cannot reach a subgoal region, the planner resorts to random exploration to maintain probabilistic completeness and asymptotic optimality. The accompanying experimental evaluation shows that the approach significantly improves the computational efficiency of motion planning on various benchmarks, including physics-based vehicular models on uneven and varying friction terrains as well as a quadrotor under air pressure effects."
Differential-Algebraic Equation Control Barrier Function for Flexible Link Manipulator,"Park, Younghwa; Sloth, Christoffer",,
Targeted Image Transformation for Improving Robustness in Long Range Aircraft Detection,"Martin, Rebecca; Fung, Clement; Keetha, Nikhil Varma; Bauer, Lujo; Scherer, Sebastian",,
Unsupervised 3D Part Decomposition via Leveraged Gaussian Splatting,"Choy, Jae Goo; Cha, Geonho; Kee, Hogun; Oh, Songhwai",,
A 6-DOF Double-layer Programmable Remote Center of Motion Robot for Vitreoretinal Surgery,"Wang, Chenyu; Ko, Seong Young",,
Towards Long Term SLAM on Thermal Imagery,"Keil, Colin; Gupta, Aniket; Kaveti, Pushyami; Singh, Hanumant",https://arxiv.org/abs/2403.19885,"Visual SLAM with thermal imagery, and other low contrast visually degraded environments such as underwater, or in areas dominated by snow and ice, remain a difficult problem for many state of the art (SOTA) algorithms. In addition to challenging front-end data association, thermal imagery presents an additional difficulty for long term relocalization and map reuse. The relative temperatures of objects in thermal imagery change dramatically from day to night. Feature descriptors typically used for relocalization in SLAM are unable to maintain consistency over these diurnal changes. We show that learned feature descriptors can be used within existing Bag of Word based localization schemes to dramatically improve place recognition across large temporal gaps in thermal imagery. In order to demonstrate the effectiveness of our trained vocabulary, we have developed a baseline SLAM system, integrating learned features and matching into a classical SLAM algorithm. Our system demonstrates good local tracking on challenging thermal imagery, and relocalization that overcomes dramatic day to night thermal appearance changes. Our code and datasets are available here: https://github.com/neufieldrobotics/IRSLAM_Baseline"
Model Predictive Trees: Sample-Efficient Receding-Horizon Planning with Reusable Tree Search,"Lathrop, John; Riviere, Benjamin; Alindogan, Jedidiah; Chung, Soon-Jo",,
Embedded Sensing-Enabled External Interaction Estimation of 6-PSS Parallel Robots,"Xia, Jingyuan; Lin, Zecai; Ai, Xiaojie; Gao, Anzhu",,
Architectural-Scale Artistic Brush Painting with a Hybrid Cable Robot,"Chen, Gerry; Al-Haddad, Tristan; Dellaert, Frank; Hutchinson, Seth",https://arxiv.org/abs/2403.12214,"Robot art presents an opportunity to both showcase and advance state-of-the-art robotics through the challenging task of creating art. Creating large-scale artworks in particular engages the public in a way that small-scale works cannot, and the distinct qualities of brush strokes contribute to an organic and human-like quality. Combining the large scale of murals with the strokes of the brush medium presents an especially impactful result, but also introduces unique challenges in maintaining precise, dextrous motion control of the brush across such a large workspace. In this work, we present the first robot to our knowledge that can paint architectural-scale murals with a brush. We create a hybrid robot consisting of a cable-driven parallel robot and 4 degree of freedom (DoF) serial manipulator to paint a 27m by 3.7m mural on windows spanning 2-stories of a building. We discuss our approach to achieving both the scale and accuracy required for brush-painting a mural through a combination of novel mechanical design elements, coordinated planning and control, and on-site calibration algorithms with experimental validations."
Learning Force-Based Control Policies via Differentiable Virtual Coupling (Diff-VC),"Galvan, Aldo; Majewicz Fey, Ann; Patel, Ravi",,
Robot Synesthesia: A Sound and Emotion Guided Robot Painter,"Misra, Vihaan; Schaldenbrand, Peter; Oh, Jean",https://arxiv.org/abs/2302.04850,"If a picture paints a thousand words, sound may voice a million. While recent robotic painting and image synthesis methods have achieved progress in generating visuals from text inputs, the translation of sound into images is vastly unexplored. Generally, sound-based interfaces and sonic interactions have the potential to expand accessibility and control for the user and provide a means to convey complex emotions and the dynamic aspects of the real world. In this paper, we propose an approach for using sound and speech to guide a robotic painting process, known here as robot synesthesia. For general sound, we encode the simulated paintings and input sounds into the same latent space. For speech, we decouple speech into its transcribed text and the tone of the speech. Whereas we use the text to control the content, we estimate the emotions from the tone to guide the mood of the painting. Our approach has been fully integrated with FRIDA, a robotic painting framework, adding sound and speech to FRIDA's existing input modalities, such as text and style. In two surveys, participants were able to correctly guess the emotion or natural sound used to generate a given painting more than twice as likely as random chance. On our sound-guided image manipulation and music-guided paintings, we discuss the results qualitatively."
Learning Fine Pinch-Grasp Skills using Tactile Sensing from A Few Real-world Demonstrations,"Mao, Xiaofeng; Xu, Yucheng; Wen, Ruoshi; Kasaei, Mohammadreza; Yu, Wanming; Psomopoulou, Efi; Lepora, Nathan; Li, Zhibin (Alex)",https://arxiv.org/abs/2307.04619,"Imitation learning for robot dexterous manipulation, especially with a real robot setup, typically requires a large number of demonstrations. In this paper, we present a data-efficient learning from demonstration framework which exploits the use of rich tactile sensing data and achieves fine bimanual pinch grasping. Specifically, we employ a convolutional autoencoder network that can effectively extract and encode high-dimensional tactile information. Further, We develop a framework that achieves efficient multi-sensor fusion for imitation learning, allowing the robot to learn contact-aware sensorimotor skills from demonstrations. Our comparision study against the framework without using encoded tactile features highlighted the effectiveness of incorporating rich contact information, which enabled dexterous bimanual grasping with active contact searching. Extensive experiments demonstrated the robustness of the fine pinch grasp policy directly learned from few-shot demonstration, including grasping of the same object with different initial poses, generalizing to ten unseen new objects, robust and firm grasping against external pushes, as well as contact-aware and reactive re-grasping in case of dropping objects under very large perturbations. Furthermore, the saliency map analysis method is used to describe weight distribution across various modalities during pinch grasping, confirming the effectiveness of our framework at leveraging multimodal information."
Abstraction of the Body Ability of the Transformer Robot System for the Transportation and Installation of Heavy Objects in Land and Underwater Environments,"Makabe, Tasuku; Okada, Kei; Inaba, Masayuki",,
Semantic Layering in Room Segmentation via LLMs,"Kim, Taehyeon; Min, Byung-Cheol",https://arxiv.org/abs/2403.12920,"In this paper, we introduce Semantic Layering in Room Segmentation via LLMs (SeLRoS), an advanced method for semantic room segmentation by integrating Large Language Models (LLMs) with traditional 2D map-based segmentation. Unlike previous approaches that solely focus on the geometric segmentation of indoor environments, our work enriches segmented maps with semantic data, including object identification and spatial relationships, to enhance robotic navigation. By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation. Furthermore, SeLRoS overcomes the limitations of existing algorithms by using a semantic evaluation method to accurately distinguish true room divisions from those erroneously generated by furniture and segmentation inaccuracies. The effectiveness of SeLRoS is verified through its application across 30 different 3D environments. Source code and experiment videos for this work are available at: https://sites.google.com/view/selros."
Integrating Online Learning and Connectivity Maintenance for Communication-Aware Multi-Robot Coordination,"Yang, Yupeng; Lyu, Yiwei; Zhang, Yanze; Gao, Ian; Luo, Wenhao",,
Map-based Modular Approach for Zero-shot Embodied Question Answering,"Sakamoto, Koya; Azuma, Daichi; Miyanishi, Taiki; Kurita, Shuhei; Kawanabe, Motoaki",https://arxiv.org/abs/2405.16559,"Building robots capable of interacting with humans through natural language in the visual world presents a significant challenge in the field of robotics. To overcome this challenge, Embodied Question Answering (EQA) has been proposed as a benchmark task to measure the ability to identify an object navigating through a previously unseen environment in response to human-posed questions. Although some methods have been proposed, their evaluations have been limited to simulations, without experiments in real-world scenarios. Furthermore, all of these methods are constrained by a limited vocabulary for question-and-answer interactions, making them unsuitable for practical applications. In this work, we propose a map-based modular EQA method that enables real robots to navigate unknown environments through frontier-based map creation and address unknown QA pairs using foundation models that support open vocabulary. Unlike the questions of the previous EQA dataset on Matterport 3D (MP3D), questions in our real-world experiments contain various question formats and vocabularies not included in the training data. We conduct comprehensive experiments on virtual environments (MP3D-EQA) and two real-world house environments and demonstrate that our method can perform EQA even in the real world."
Enhancing Nighttime UAV Tracking with Light Distribution Suppression,"Yao, Liangliang; Fu, Changhong; Wang, Yiheng; Zuo, Haobo; Lu, Kunhan",,
Grow-to-Shape Control of Variable Length Continuum Robots via Adaptive Visual Servoing,"Gandhi, Abhinav; Chiang, Shou-Shan; Onal, Cagdas; Calli, Berk",,
Graph Neural Network-based Multi-agent Reinforcement Learning for Resilient Distributed Coordination of Multi-Robot Systems,"Goeckner, Anthony; Sui, Yueyuan; Martinet, Nicolas; Li, Xinliang; Zhu, Qi",https://arxiv.org/abs/2403.13093,"Existing multi-agent coordination techniques are often fragile and vulnerable to anomalies such as agent attrition and communication disturbances, which are quite common in the real-world deployment of systems like field robotics. To better prepare these systems for the real world, we present a graph neural network (GNN)-based multi-agent reinforcement learning (MARL) method for resilient distributed coordination of a multi-robot system. Our method, Multi-Agent Graph Embedding-based Coordination (MAGEC), is trained using multi-agent proximal policy optimization (PPO) and enables distributed coordination around global objectives under agent attrition, partial observability, and limited or disturbed communications. We use a multi-robot patrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator and then compare its performance with prior coordination approaches. Results demonstrate that MAGEC outperforms existing methods in several experiments involving agent attrition and communication disturbance, and provides competitive results in scenarios without such anomalies."
LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion,"Zhang, Jinyu; Gu, Yongchong; Gao, Jianxiong; Lin, Haitao; Sun, Qiang; Sun, Xinwei; Xue, Xiangyang; Fu, Yanwei",https://arxiv.org/abs/2408.03238,"This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-modal data, and then uses the prior visible mask as attention map to guide the network to focus on target feature locations for further complete mask recovery. Using the amodal mask of the target object provides advantages in selecting more accurate and robust grasp points compared to relying solely on the visible segments. The results on different datasets show that our method achieves state-of-the-art performance. Furthermore, the robot experiments validate the feasibility and robustness of this method in the real world. Our code and demonstrations are available on the project page: https://jrryzh.github.io/LAC-Net."
Coarse-to-Fine Detection of Multiple Seams for Robotic Welding,"Wei, Pengkun; Cheng, Shuo; Li, Dayou; Song, Ran; Zhang, Yipeng; Zhang, Wei",https://arxiv.org/abs/2408.10710,"Efficiently detecting target weld seams while ensuring sub-millimeter accuracy has always been an important challenge in autonomous welding, which has significant application in industrial practice. Previous works mostly focused on recognizing and localizing welding seams one by one, leading to inferior efficiency in modeling the workpiece. This paper proposes a novel framework capable of multiple weld seams extraction using both RGB images and 3D point clouds. The RGB image is used to obtain the region of interest by approximately localizing the weld seams, and the point cloud is used to achieve the fine-edge extraction of the weld seams within the region of interest using region growth. Our method is further accelerated by using a pre-trained deep learning model to ensure both efficiency and generalization ability. The performance of the proposed method has been comprehensively tested on various workpieces featuring both linear and curved weld seams and in physical experiment systems. The results showcase considerable potential for real-world industrial applications, emphasizing the method's efficiency and effectiveness. Videos of the real-world experiments can be found at https://youtu.be/pq162HSP2D4."
Robust Multi-Camera BEV Perception: An Image-Perceptive Approach to Counter Imprecise Camera Calibration,"Sun, Rundong; Fu, Mengyin; Liang, Hao; Zhu, Chunhui; Dong, Zhipeng; Yang, Yi",,
Versatile Variable-Stiffness Scooping End-Effector: Tilting-Scooping-Transfer Mechanism for Objects with Various Properties,"Takahashi, Yuta; Tadakuma, Kenjiro; Abe, Kazuki; Watanabe, Masahiro; Shimizu, Shoya; Tadokoro, Satoshi",,
CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments,"Sathyamoorthy, Adarsh Jagan; Kulathun Mudiyanselage, Kasun Weerakoon; Elnoor, Mohamed; Zore, Anuj; Ichter, Brian; Xia, Fei; Tan, Jie; Yu, Wenhao; Manocha, Dinesh",https://arxiv.org/abs/2403.15637,"We present ConVOI, a novel method for autonomous robot navigation in real-world indoor and outdoor environments using Vision Language Models (VLMs). We employ VLMs in two ways: first, we leverage their zero-shot image classification capability to identify the context or scenario (e.g., indoor corridor, outdoor terrain, crosswalk, etc) of the robot's surroundings, and formulate context-based navigation behaviors as simple text prompts (e.g. ``stay on the pavement""). Second, we utilize their state-of-the-art semantic understanding and logical reasoning capabilities to compute a suitable trajectory given the identified context. To this end, we propose a novel multi-modal visual marking approach to annotate the obstacle-free regions in the RGB image used as input to the VLM with numbers, by correlating it with a local occupancy map of the environment. The marked numbers ground image locations in the real-world, direct the VLM's attention solely to navigable locations, and elucidate the spatial relationships between them and terrains depicted in the image to the VLM. Next, we query the VLM to select numbers on the marked image that satisfy the context-based behavior text prompt, and construct a reference path using the selected numbers. Finally, we propose a method to extrapolate the reference trajectory when the robot's environmental context has not changed to prevent unnecessary VLM queries. We use the reference trajectory to guide a motion planner, and demonstrate that it leads to human-like behaviors (e.g. not cutting through a group of people, using crosswalks, etc.) in various real-world indoor and outdoor scenarios."
Practical Framework for Path Representation and Following Control in Mobile Industrial Robots,"Koh, Youngil; Kim, WooJeong; Choi, MidEum",,
Adaptive Trajectory Database Learning for Nonlinear Control with Hybrid Gradient Optimization,"Tseng, Kuan-Yu; Zhang, Mengchao; Hauser, Kris; Dullerud, Geir E.",,
A Novel Variable Stiffness Suspension System for Improved Stability and Control of Tactile Mobile Manipulators,"Kuhn, Sebastian; Yildirim, Mehmet Can; Pozo Fortuni&#263;, Edmundo; Karacan, Kübra; Swikir, Abdalla; Haddadin, Sami",,
Aligning Learning with Communication in Shared Autonomy,"Hoegerman, Joshua; Sagheb, Shahabedin; Christie, Benjamin; Losey, Dylan",https://arxiv.org/abs/2403.12023,"Assistive robot arms can help humans by partially automating their desired tasks. Consider an adult with motor impairments controlling an assistive robot arm to eat dinner. The robot can reduce the number of human inputs -- and how precise those inputs need to be -- by recognizing what the human wants (e.g., a fork) and assisting for that task (e.g., moving towards the fork). Prior research has largely focused on learning the human's task and providing meaningful assistance. But as the robot learns and assists, we also need to ensure that the human understands the robot's intent (e.g., does the human know the robot is reaching for a fork?). In this paper, we study the effects of communicating learned assistance from the robot back to the human operator. We do not focus on the specific interfaces used for communication. Instead, we develop experimental and theoretical models of a) how communication changes the way humans interact with assistive robot arms, and b) how robots can harness these changes to better align with the human's intent. We first conduct online and in-person user studies where participants operate robots that provide partial assistance, and we measure how the human's inputs change with and without communication. With communication, we find that humans are more likely to intervene when the robot incorrectly predicts their intent, and more likely to release control when the robot correctly understands their task. We then use these findings to modify an established robot learning algorithm so that the robot can correctly interpret the human's inputs when communication is present. Our results from a second in-person user study suggest that this combination of communication and learning outperforms assistive systems that isolate either learning or communication."
Enhanced Spherical Omnidirectional Wheel: Achieving Passive Rollers with High Load Capacity and Smoothness through an Offset Rotational Axis,"Tadakuma, Kenjiro; Sakiyama, Seiji; Takane, Eri; Tadakuma, Riichiro; Tadokoro, Satoshi",,
Design of a Fully Actuated Drone With Non-Isotropic Wrench Shape,"Park, Seongsu; Kim, Min Jun",,
A New 10-mg SMA-Based Fast Bimorph Actuator for Microrobotics,"Trygstad, Conor; Blankenship, Elijah; Perez-Arancibia, Nestor O",,
FogROS2-FT: Fault Tolerant Cloud Robotics,"Chen, Kaiyuan; Hari, Kush; Chung, Trinity; Wang, Michael; Tian, Nan; Juette, Christian; Ichnowski, Jeffrey; Ren, Liu; Kubiatowicz, John; Stoica, Ion; Goldberg, Ken",,
Motion Planning for Automata-based Objectives using Efficient Gradient-based Methods,"Balakrishnan, Anand; Atasever, Merve; Deshmukh, Jyotirmoy",,
Self-Supervised Motion Segmentation with Confidence-Aware Loss Functions for Handling Occluded Pixels and Uncertain Optical Flow Predictions,"Chen, Chung-Yu; Lai, Bo-Yun; Huang, Ying-Shiuan; Lin, Wen-Chieh; Wang, Chieh-Chih",,
A Robotic Mediation Device for Skill Assessment and Training During Colonoscopy,"Richards, Olivia; Ahronovich, Elan; Shihora, Neel; Yildiz, Ahmet; Atoum, Jumana; Wu, Jie Ying; Obstein, Keith; Simaan, Nabil",,
BEV Image-based Lane Tracking Control System for Autonomous Lane Repainting Robot,"Seo, Junghyun; Jeon, Hyeonjae; Choi, Joonyoung; Kwangho, Woo; Lim, Yongseob; Jin, yongsik",,
NeRF-enabled Analysis-Through-Synthesis for ISAR Imaging of Small Everyday Objects with Sparse and Noisy UWB Radar Data,"Tasnim Oshim, Md Farhan; Reed, Albert; Jayasuriya, Suren; Rahman, Tauhidur",,
Few-shot Transparent Instance Segmentation,"Cherian, Anoop; Jain, Siddarth; Marks, Tim K.",,
Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models,"Seker, Muhammet Yunus; Kroemer, Oliver",https://arxiv.org/abs/2403.11313,"Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels of precision and generalization. Using a model-based approach, the robot must quickly reason about the outcomes of different actions using a predictive model to find a set of parameters that will have the desired effect. The model may need to capture the behaviors of rigid and deformable objects, as well as objects of various shapes and sizes. Predictive models often need to trade-off speed for prediction accuracy and generalization. This paper proposes a framework that leverages the strengths of multiple predictive models, including analytical, learned, and simulation-based models, to enhance the efficiency and accuracy of action parameter optimization. Our approach uses Model Deviation Estimators (MDEs) to determine the most suitable predictive model for any given state-action parameters, allowing the robot to select models to make fast and precise predictions. We extend the MDE framework by not only learning sim-to-real MDEs, but also sim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide significantly faster parameter optimization as well as a basis for efficiently learning sim-to-real MDEs through finetuning. The ease of collecting sim-to-sim training data also allows the robot to learn MDEs based directly on visual inputs and local material properties."
Two-stage pose optimization algorithm using color information for underwater SLAM with light-sectioning-based 3D scanning method,"Ikeda, Takaki; Iwaguchi, Takafumi; Thomas, Diego; Kawasaki, Hiroshi",,
Highly Efficient Observation Process based on FFT Filtering for Robot Swarm Collaborative Navigation in Unknown Environments,"Li, Chenxi; Lu, Weining; Ma, Zhihao; Meng, Litong; Liang, Bin",https://arxiv.org/abs/2405.07687,"Collaborative path planning for robot swarms in complex, unknown environments without external positioning is a challenging problem. This requires robots to find safe directions based on real-time environmental observations, and to efficiently transfer and fuse these observations within the swarm. This study presents a filtering method based on Fast Fourier Transform (FFT) to address these two issues. We treat sensors' environmental observations as a digital sampling process. Then, we design two different types of filters for safe direction extraction, as well as for the compression and reconstruction of environmental data. The reconstructed data is mapped to probabilistic domain, achieving efficient fusion of swarm observations and planning decision. The computation time is only on the order of microseconds, and the transmission data in communication systems is in bit-level. The performance of our algorithm in sensor data processing was validated in real world experiments, and the effectiveness in swarm path optimization was demonstrated through extensive simulations."
MPP: Multiscale Path Planning for UGV Navigationin Semi-structured Environments,"Cao, Rui; Yang, Zhiqiang; Song, Ran; Meng, Ziyu; Wang, Ruifeng; Zhang, Wei",,
Task-Based Design and Policy Co-Optimization for Tendon-driven Underactuated Kinematic Chains,"Islam, Sharfin; He, Zhanpeng; Ciocarlie, Matei",https://arxiv.org/abs/2405.14566,"Underactuated manipulators reduce the number of bulky motors, thereby enabling compact and mechanically robust designs. However, fewer actuators than joints means that the manipulator can only access a specific manifold within the joint space, which is particular to a given hardware configuration and can be low-dimensional and/or discontinuous. Determining an appropriate set of hardware parameters for this class of mechanisms, therefore, is difficult - even for traditional task-based co-optimization methods. In this paper, our goal is to implement a task-based design and policy co-optimization method for underactuated, tendon-driven manipulators. We first formulate a general model for an underactuated, tendon-driven transmission. We then use this model to co-optimize a three-link, two-actuator kinematic chain using reinforcement learning. We demonstrate that our optimized tendon transmission and control policy can be transferred reliably to physical hardware with real-world reaching experiments."
AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for Quadruped Robot Navigation in Outdoor Environments,"Elnoor, Mohamed; Kulathun Mudiyanselage, Kasun Weerakoon; Sathyamoorthy, Adarsh Jagan; Guan, Tianrui; Rajagopal, Vignesh; Manocha, Dinesh",https://arxiv.org/abs/2403.13235,"We present AMCO, a novel navigation method for quadruped robots that adaptively combines vision-based and proprioception-based perception capabilities. Our approach uses three cost maps: general knowledge map; traversability history map; and current proprioception map; which are derived from a robot's vision and proprioception data, and couples them to obtain a coupled traversability cost map for navigation. The general knowledge map encodes terrains semantically segmented from visual sensing, and represents a terrain's typically expected traversability. The traversability history map encodes the robot's recent proprioceptive measurements on a terrain and its semantic segmentation as a cost map. Further, the robot's present proprioceptive measurement is encoded as a cost map in the current proprioception map. As the general knowledge map and traversability history map rely on semantic segmentation, we evaluate the reliability of the visual sensory data by estimating the brightness and motion blur of input RGB images and accordingly combine the three cost maps to obtain the coupled traversability cost map used for navigation. Leveraging this adaptive coupling, the robot can depend on the most reliable input modality available. Finally, we present a novel planner that selects appropriate gaits and velocities for traversing challenging outdoor environments using the coupled traversability cost map. We demonstrate AMCO's navigation performance in different real-world outdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability metrics, and up to 50% improvement in terms of success rate compared to current navigation methods."
Learning Bimanual Manipulation Policies for Bathing Bed-bound People,"Gu, Yijun; Demiris, Yiannis",,
"NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications","Cui, Li; Ding, Yang; Hartley, Richard; Xie, Zirui; Kneip, Laurent; YU, ZHENGHUA",,
Safety-critical Autonomous Inspection of Distillation Columns using Quadrupedal Robots Equipped with Roller Arms,"Lee, Jaemin; Kim, Jeeseop; Ames, Aaron",https://arxiv.org/abs/2404.10938,"This paper proposes a comprehensive framework designed for the autonomous inspection of complex environments, with a specific focus on multi-tiered settings such as distillation column trays. Leveraging quadruped robots equipped with roller arms, and through the use of onboard perception, we integrate essential motion components including: locomotion, safe and dynamic transitions between trays, and intermediate motions that bridge a variety of motion primitives. Given the slippery and confined nature of column trays, it is critical to ensure safety of the robot during inspection, therefore we employ a safety filter and footstep re-planning based upon control barrier function representations of the environment. Our framework integrates all system components into a state machine encoding the developed safety-critical planning and control elements to guarantee safety-critical autonomy, enabling autonomous and safe navigation and inspection of distillation columns. Experimental validation in an environment, consisting of industrial-grade chemical distillation trays, highlights the effectiveness of our multi-layered architecture."
Versatile Locomotion Skills for Hexapod Robots,"Qu, Tomson; Li, Dichen; Zakhor, Avideh; Yu, Wenhao; Zhang, Tingnan",,
Streamlining Forest Wildfire Surveillance: AI-Enhanced UAVs Utilizing the FLAME Aerial Video Dataset for Lightweight and Efficient Monitoring,"Zhao, Lemeng; Hu, Junjie; Bi, Jianchao; Bai, Yanbing; Erick, Mas; Koshimura, Shunichi",https://arxiv.org/abs/2409.00510,"In recent years, unmanned aerial vehicles (UAVs) have played an increasingly crucial role in supporting disaster emergency response efforts by analyzing aerial images. While current deep-learning models focus on improving accuracy, they often overlook the limited computing resources of UAVs. This study recognizes the imperative for real-time data processing in disaster response scenarios and introduces a lightweight and efficient approach for aerial video understanding. Our methodology identifies redundant portions within the video through policy networks and eliminates this excess information using frame compression techniques. Additionally, we introduced the concept of a `station point,' which leverages future information in the sequential policy network, thereby enhancing accuracy. To validate our method, we employed the wildfire FLAME dataset. Compared to the baseline, our approach reduces computation costs by more than 13 times while boosting accuracy by 3$\%$. Moreover, our method can intelligently select salient frames from the video, refining the dataset. This feature enables sophisticated models to be effectively trained on a smaller dataset, significantly reducing the time spent during the training process."
EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union,"Liao, Brian Hsuan-Cheng; Cheng, Chih-Hong; Esen, Hasan; Knoll, Alois",https://arxiv.org/abs/2403.15474,"This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well."
SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models,"Kannan, Shyam Sundar; Venkatesh, L.N Vishnunandan; Min, Byung-Cheol",https://arxiv.org/abs/2309.10062,"In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/."
GSLoc: Visual Localization with 3D Gaussian Splatting,"Botashev, Kazii; Pyatov, Vladislav; Ferrer, Gonzalo; Lefkimmiatis, Stamatios",https://arxiv.org/abs/2408.11085,"We leverage 3D Gaussian Splatting (3DGS) as a scene representation and propose a novel test-time camera pose refinement framework, GSLoc. This framework enhances the localization accuracy of state-of-the-art absolute pose regression and scene coordinate regression methods. The 3DGS model renders high-quality synthetic images and depth maps to facilitate the establishment of 2D-3D correspondences. GSLoc obviates the need for training feature extractors or descriptors by operating directly on RGB images, utilizing the 3D vision foundation model, MASt3R, for precise 2D matching. To improve the robustness of our model in challenging outdoor environments, we incorporate an exposure-adaptive module within the 3DGS framework. Consequently, GSLoc enables efficient pose refinement given a single RGB query and a coarse initial pose estimation. Our proposed approach surpasses leading NeRF-based optimization methods in both accuracy and runtime across indoor and outdoor visual localization benchmarks, achieving state-of-the-art accuracy on two indoor datasets."
EMPOWER: Embodied Multi-role Open-vocabulary Planning with Online Grounding and Execution,"Argenziano, Francesco; Brienza, Michele; Suriani, Vincenzo; Nardi, Daniele; Bloisi, Domenico",https://arxiv.org/abs/2408.17379,"Task planning for robots in real-life settings presents significant challenges. These challenges stem from three primary issues: the difficulty in identifying grounded sequences of steps to achieve a goal; the lack of a standardized mapping between high-level actions and low-level commands; and the challenge of maintaining low computational overhead given the limited resources of robotic hardware. We introduce EMPOWER, a framework designed for open-vocabulary online grounding and planning for embodied agents aimed at addressing these issues. By leveraging efficient pre-trained foundation models and a multi-role mechanism, EMPOWER demonstrates notable improvements in grounded planning and execution. Quantitative results highlight the effectiveness of our approach, achieving an average success rate of 0.73 across six different real-life scenarios using a TIAGo robot."
A General Formulation for Path Constrained Time-Optimized Trajectory Planning with Environmental and Object Contacts,"Mahalingam, Dasharadhan; Patankar, Aditya; Laha, Riddhiman; Lakshminarayanan, Srinivasan; Haddadin, Sami; Chakraborty, Nilanjan",,
OBHMR: Partial Generalized Point Set Registration with Overlap Bidirectional Hybrid Mixture Model,"Du, xinzhe; Zhang, Zhengyan; Min, Zhe; Zhang, Ang; Song, Rui; Li, Yibin; Meng, Max Q.-H.",,
EverySync: An Open Hardware Time Synchronization Sensor Suite for Common Sensors in SLAM,"Wu, Xuankang; Sun, Haoxiang; Wu, Rongguang; Fang, Zheng",,
GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment,"Ying, Lance; Jha, Kunal; Aarya, Shivam; Tenenbaum, Joshua; Torralba, Antonio; Shu, Tianmin",https://arxiv.org/abs/2403.11075,"Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other's mental state. In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents' mental states that are relevant to the goals. This approach enables an embodied assistant to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation. We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator). Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context. In contrast, our approach can successfully generate concise verbal communication for the embodied assistant to effectively boost the performance of the cooperation as well as human users' perception of the assistant."
Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability,"Muenprasitivej, Kasdiit; Jiang, Jesse; Shamsah, Abdulaziz; Coogan, Samuel; Zhao, Ye",https://arxiv.org/abs/2403.16356,"We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain. In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation. Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations. Thus, the problems of terrain mapping and locomotion stability are intertwined. We evaluate three different kernels for Gaussian process (GP) regression to learn the terrain elevation. We also learn the motion deviation resulting from both the terrain as well as the discrepancy between the reduced-order Prismatic Inverted Pendulum Model used for planning and the full-order locomotion dynamics. We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner. The global navigation planner plans a series of local waypoints to reach the desired goal locations while respecting locomotion stability constraints. Then, a local navigation planner is used to generate a sequence of dynamically feasible footsteps to reach local waypoints. We develop a novel trajectory evaluation metric to minimize motion deviation and maximize information gain of the terrain elevation map. We evaluate the efficacy of our planning framework on Digit bipedal robot simulation in MuJoCo."
Discretizing SO(2)-Equivariant Features for Robotic Kitting,"Zhou, Jiadong; Zeng, Yadan; Dong, Huixu; Chen, I-Ming",https://arxiv.org/abs/2403.13336,"Robotic kitting has attracted considerable attention in logistics and industrial settings. However, existing kitting methods encounter challenges such as low precision and poor efficiency, limiting their widespread applications. To address these issues, we present a novel kitting framework that improves both the precision and computational efficiency of complex kitting tasks. Firstly, our approach introduces a fine-grained orientation estimation technique in the picking module, significantly enhancing orientation precision while effectively decoupling computational load from orientation granularity. This approach combines an SO(2)-equivariant network with a group discretization operation to preciously predict discrete orientation distributions. Secondly, we develop the Hand-tool Kitting Dataset (HKD) to evaluate the performance of different solutions in handling orientation-sensitive kitting tasks. This dataset comprises a diverse collection of hand tools and synthetically created kits, which reflects the complexities encountered in real-world kitting scenarios. Finally, a series of experiments are conducted to evaluate the performance of the proposed method. The results demonstrate that our approach offers remarkable precision and enhanced computational efficiency in robotic kitting tasks."
SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants,"Moghani, Masoud; Doorenbos, Lars; Panitch, William; Huver, Sean; Azizian, Mahdi; Goldberg, Ken; Garg, Animesh",https://arxiv.org/abs/2405.05226,"In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants. SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution. This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives. SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks. We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions. Project website: orbit-surgical.github.io/sufia"
A Novel Vitreoretinal Surgical Robot System to Maximize the Internal Reachable Workspace and Minimize the External Link Motion,"Jeong, Gowoon; Ko, Seong Young",,
TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments,"Hossain, Jumman; Faridee, Abu-Zaher; Roy, Nirmalya; Freeman, Jade; Gregory, Timothy; Trout, Theron T.",https://arxiv.org/abs/2402.04061,"Autonomous robots exploring unknown environments face a significant challenge: navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we present TopoNav, a novel topological navigation framework that integrates active mapping, hierarchical reinforcement learning, and intrinsic motivation to enable efficient goal-oriented exploration and navigation in sparse-reward settings. TopoNav dynamically constructs a topological map of the environment, capturing key locations and pathways. A two-level hierarchical policy architecture, comprising a high-level graph traversal policy and low-level motion control policies, enables effective navigation and obstacle avoidance while maintaining focus on the overall goal. Additionally, TopoNav incorporates intrinsic motivation to guide exploration toward relevant regions and frontier nodes in the topological map, addressing the challenges of sparse extrinsic rewards. We evaluate TopoNav both in the simulated and real-world off-road environments using a Clearpath Jackal robot, across three challenging navigation scenarios: goal-reaching, feature-based navigation, and navigation in complex terrains. We observe an increase in exploration coverage by 7- 20%, in success rates by 9-19%, and reductions in navigation times by 15-36% across various scenarios, compared to state-of-the-art methods"
Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual and Action Representations,"Li, Puhao; Liu, Tengyu; Li, Yuyang; Han, Muzhi; Geng, Haoran; Wang, Shu; Zhu, Yixin; Zhu, Song-Chun; Huang, Siyuan",https://arxiv.org/abs/2404.17521,"Autonomous robotic systems capable of learning novel manipulation tasks are poised to transform industries from manufacturing to service automation. However, modern methods (e.g., VIP and R3M) still face significant hurdles, notably the domain gap among robotic embodiments and the sparsity of successful task executions within specific action spaces, resulting in misaligned and ambiguous task representations. We introduce Ag2Manip (Agent-Agnostic representations for Manipulation), a framework aimed at surmounting these challenges through two key innovations: a novel agent-agnostic visual representation derived from human manipulation videos, with the specifics of embodiments obscured to enhance generalizability; and an agent-agnostic action representation abstracting a robot's kinematics to a universal agent proxy, emphasizing crucial interactions between end-effector and object. Ag2Manip's empirical validation across simulated benchmarks like FrankaKitchen, ManiSkill, and PartManip shows a 325% increase in performance, achieved without domain-specific demonstrations. Ablation studies underline the essential contributions of the visual and action representations to this success. Extending our evaluations to the real world, Ag2Manip significantly improves imitation learning success rates from 50% to 77.5%, demonstrating its effectiveness and generalizability across both simulated and physical environments."
In-Hand Singulation and Scooping Manipulation with a 5 DOF Tactile Gripper,"Zhou, Yuhao; Zhou, Pokuang; Wang, Shaoxiong; She, Yu",https://arxiv.org/abs/2408.00610,"Manipulation tasks often require a high degree of dexterity, typically necessitating grippers with multiple degrees of freedom (DoF). While a robotic hand equipped with multiple fingers can execute precise and intricate manipulation tasks, the inherent redundancy stemming from its extensive DoF often adds unnecessary complexity. In this paper, we introduce the design of a tactile sensor-equipped gripper with two fingers and five DoF. We present a novel design integrating a GelSight tactile sensor, enhancing sensing capabilities and enabling finer control during specific manipulation tasks. To evaluate the gripper's performance, we conduct experiments involving two challenging tasks: 1) retrieving, singularizing, and classification of various objects embedded in granular media, and 2) executing scooping manipulations of credit cards in confined environments to achieve precise insertion. Our results demonstrate the efficiency of the proposed approach, with a high success rate for singulation and classification tasks, particularly for spherical objects at high as 94.3%, and a 100% success rate for scooping and inserting credit cards."
Reinforce actions with half of the dynamics,"Wang, Shuyuan",https://arxiv.org/abs/1910.05313,"Buildings sector is one of the major consumers of energy in the United States. The buildings HVAC (Heating, Ventilation, and Air Conditioning) systems, whose functionality is to maintain thermal comfort and indoor air quality (IAQ), account for almost half of the energy consumed by the buildings. Thus, intelligent scheduling of the building HVAC system has the potential for tremendous energy and cost savings while ensuring that the control objectives (thermal comfort, air quality) are satisfied. Recently, several works have focused on model-free deep reinforcement learning based techniques such as Deep Q-Network (DQN). Such methods require extensive interactions with the environment. Thus, they are impractical to implement in real systems due to low sample efficiency. Safety-aware exploration is another challenge in real systems since certain actions at particular states may result in catastrophic outcomes. To address these issues and challenges, we propose a model-based reinforcement learning approach that learns the system dynamics using a neural network. Then, we adopt Model Predictive Control (MPC) using the learned system dynamics to perform control with random-sampling shooting method. To ensure safe exploration, we limit the actions within safe range and the maximum absolute change of actions according to prior knowledge. We evaluate our ideas through simulation using widely adopted EnergyPlus tool on a case study consisting of a two zone data-center. Experiments show that the average deviation of the trajectories sampled from the learned dynamics and the ground truth is below $20\%$. Compared with baseline approaches, we reduce the total energy consumption by $17.1\% \sim 21.8\%$. Compared with model-free reinforcement learning approach, we reduce the required number of training steps to converge by 10x."
"When, What, and with Whom to Communicate: Enhancing RL-based Multi-Robot Navigation through Selective Communication","Arul, Senthil Hariharan; Bedi, Amrit Singh; Manocha, Dinesh",,
An Ejecting System for Autonomous Takeoff of Flapping-Wing Robots,"jiang, xu; Zhang, Jun; Song, Aiguo",,
Learning-based Adaptive Control of Quadruped Robots for Active Stabilization on Moving Platforms,"Yoon, Minsung; Shin, Heechan; Jeong, Jeil; Yoon, Sung-eui",,
MUP-LIO: Mapping Uncertainty-aware Point-wise Lidar Inertial Odometry,"Yao, Hekai; Zhang, Xuetao; Sun, Gang; Liu, Yisha; Zhang, Xuebo; Zhuang, Yan",,
Novel design of Reconfigurable Tracked Robot with Geometry-Changing Tracks,"Xuan, Chice; Lu, Jiadong; Tian, Zhihao; Li, Jiacheng; Zhang, Mengke; Xie, Hanbin; Qiu, Jianxiong; Xu, Chao; Cao, Yanjun",,
"Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting without Accurate Pose Initialization","Schmidt, Christian; Piekenbrinck, Jens; Leibe, Bastian",,
Crowd-Aware Robot Navigation with Switching Between Learning-Based and Rule-Based Methods Using Normalizing Flows,"Matsumoto, Kohei; Hyodo, Yuki; Kurazume, Ryo",,
Visual-Geometry GP-based Navigable Space for Autonomous Navigation,"Ali, Mahmoud; Pushp, Durgakant; Chen, Zheng; Liu, Lantao",https://arxiv.org/abs/2407.06545,"Autonomous navigation in unknown environments is challenging and demands the consideration of both geometric and semantic information in order to parse the navigability of the environment. In this work, we propose a novel space modeling framework, Visual-Geometry Sparse Gaussian Process (VG-SGP), that simultaneously considers semantics and geometry of the scene. Our proposed approach can overcome the limitation of visual planners that fail to recognize geometry associated with the semantic and the geometric planners that completely overlook the semantic information which is very critical in real-world navigation. The proposed method leverages dual Sparse Gaussian Processes in an integrated manner; the first is trained to forecast geometrically navigable spaces while the second predicts the semantically navigable areas. This integrated model is able to pinpoint the overlapping (geometric and semantic) navigable space. The simulation and real-world experiments demonstrate that the ability of the proposed VG-SGP model, coupled with our innovative navigation strategy, outperforms models solely reliant on visual or geometric navigation algorithms, highlighting a superior adaptive behavior."
Is a Simulation better than Teleoperation for Acquiring Human Manipulation Data?,"Kim, Donghyeon; Park, Seong-Su; Lee, Kwang-Hyun; Lee, Dongheui; Ryu, Jee-Hwan",,
Bi-level Trajectory Optimization on Uneven Terrains with Differentiable Wheel-Terrain Interaction Model,"Manoharan, Amith; Sharma, Aditya; Belsare, Himani; Pal, Kaustab; Krishna, Madhava; Singh, Arun Kumar",https://arxiv.org/abs/2404.03307,"Navigation of wheeled vehicles on uneven terrain necessitates going beyond the 2D approaches for trajectory planning. Specifically, it is essential to incorporate the full 6dof variation of vehicle pose and its associated stability cost in the planning process. To this end, most recent works aim to learn a neural network model to predict the vehicle evolution. However, such approaches are data-intensive and fraught with generalization issues. In this paper, we present a purely model-based approach that just requires the digital elevation information of the terrain. Specifically, we express the wheel-terrain interaction and 6dof pose prediction as a non-linear least squares (NLS) problem. As a result, trajectory planning can be viewed as a bi-level optimization. The inner optimization layer predicts the pose on the terrain along a given trajectory, while the outer layer deforms the trajectory itself to reduce the stability and kinematic costs of the pose. We improve the state-of-the-art in the following respects. First, we show that our NLS based pose prediction closely matches the output from a high-fidelity physics engine. This result coupled with the fact that we can query gradients of the NLS solver, makes our pose predictor, a differentiable wheel-terrain interaction model. We further leverage this differentiability to efficiently solve the proposed bi-level trajectory optimization problem. Finally, we perform extensive experiments, and comparison with a baseline to showcase the effectiveness of our approach in obtaining smooth, stable trajectories."
SSCBench: Monocular 3D Semantic Scene Completion Benchmark for Autonomous Driving,"LI, YIMING; Li, Sihang; Liu, Xinhao; Gong, Moonjun; Li, Kenan; Nuo, Chen; Wang, Zijun; Li, Zhiheng; JIANG, TAO; Yu, Fisher; WANG, YUE; Zhao, Hang; Yu, Zhiding; Feng, Chen",https://arxiv.org/abs/2306.09001,"Monocular scene understanding is a foundational component of autonomous systems. Within the spectrum of monocular perception topics, one crucial and useful task for holistic 3D scene understanding is semantic scene completion (SSC), which jointly completes semantic information and geometric details from RGB input. However, progress in SSC, particularly in large-scale street views, is hindered by the scarcity of high-quality datasets. To address this issue, we introduce SSCBench, a comprehensive benchmark that integrates scenes from widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo). SSCBench follows an established setup and format in the community, facilitating the easy exploration of SSC methods in various street views. We benchmark models using monocular, trinocular, and point cloud input to assess the performance gap resulting from sensor coverage and modality. Moreover, we have unified semantic labels across diverse datasets to simplify cross-domain generalization testing. We commit to including more datasets and SSC models to drive further advancements in this field."
Self-assessment of Robotic Laboratory and Equipment Readiness Using Large Language Models and Robotic Data Capture,"Ili&#263;, Stefan; Hughes, Josie",,
LiDAR-camera Online Calibration by Representing Local Feature and Global Spatial Context,"Moon, SeongJoo; Lee, Sebin; He, Dong; Yoon, Sung-eui",,
REF^2-NeRF: Reflection and Refraction aware Neural Radiance Field,"Kim, Wooseok; Fukiage, Taiki; Oishi, Takeshi",https://arxiv.org/abs/2311.17116,"Recently, significant progress has been made in the study of methods for 3D reconstruction from multiple images using implicit neural representations, exemplified by the neural radiance field (NeRF) method. Such methods, which are based on volume rendering, can model various light phenomena, and various extended methods have been proposed to accommodate different scenes and situations. However, when handling scenes with multiple glass objects, e.g., objects in a glass showcase, modeling the target scene accurately has been challenging due to the presence of multiple reflection and refraction effects. Thus, this paper proposes a NeRF-based modeling method for scenes containing a glass case. In the proposed method, refraction and reflection are modeled using elements that are dependent and independent of the viewer's perspective. This approach allows us to estimate the surfaces where refraction occurs, i.e., glass surfaces, and enables the separation and modeling of both direct and reflected light components. The proposed method requires predetermined camera poses, but accurately estimating these poses in scenes with glass objects is difficult. Therefore, we used a robotic arm with an attached camera to acquire images with known poses. Compared to existing methods, the proposed method enables more accurate modeling of both glass refraction and the overall scene."
Long-Term Map-Maintenance in Changing Environments using Ray-Bundle-Impact-Factor Estimation,"Breitfuss, Matthias; Geimer, Marcus; Gruber, Christoph Johannes",,
Modeling and Analysis of Passive Quadruped Walker with Compliant Torso on Low-friction Environment,"Xiang, Yuxuan; Zheng, Yanqiu; Asano, Fumihiko",,
Occlusion Handling by Pushing for Enhanced Fruit Detection,"Gursoy, Ege; Kulic, Dana; Cherubini, Andrea",,
V2I-Calib: A Novel Calibration Approach for Collaborative Vehicle and Infrastructure LiDAR Systems,"Qu, Luca; Xiong, Yijin; wu, xin; Li, Hanyu; Guo, Shichun",https://arxiv.org/abs/2407.10195,"Cooperative vehicle and infrastructure LiDAR systems hold great potential, yet their implementation faces numerous challenges. Calibration of LiDAR systems across heterogeneous vehicle and infrastructure endpoints is a critical step to ensure the accuracy and consistency of perception system data, necessitating calibration methods that are real-time and stable. To this end, this paper introduces a novel calibration method for cooperative vehicle and road infrastructure LiDAR systems, which exploits spatial association information between detection boxes. The method centers around a novel Overall IoU metric that reflects the correlation of targets between vehicle and infrastructure, enabling real-time monitoring of calibration results. We search for common matching boxes between vehicle and infrastructure nodes by constructing an affinity matrix. Subsequently, these matching boxes undergo extrinsic parameter computation and optimization. Comparative and ablation experiments on the DAIR-V2X dataset confirm the superiority of our method. To better reflect the differences in calibration results, we have categorized the calibration tasks on the DAIR-V2X dataset based on their level of difficulty, enriching the dataset's utility for future research. Our project is available at https://github.com/MassimoQu/v2i-calib ."
IR2: Implicit Rendezvous for Robotic Exploration Teams under Sparse Intermittent Connectivity,"Tan, Derek Ming Siang; Yixiao, Ma; Liang, Jingsong; Chng, Yi Cheng; Cao, Yuhong; Sartoretti, Guillaume Adrien",https://arxiv.org/abs/2409.04730,"Information sharing is critical in time-sensitive and realistic multi-robot exploration, especially for smaller robotic teams in large-scale environments where connectivity may be sparse and intermittent. Existing methods often overlook such communication constraints by assuming unrealistic global connectivity. Other works account for communication constraints (by maintaining close proximity or line of sight during information exchange), but are often inefficient. For instance, preplanned rendezvous approaches typically involve unnecessary detours resulting from poorly timed rendezvous, while pursuit-based approaches often result in short-sighted decisions due to their greedy nature. We present IR2, a deep reinforcement learning approach to information sharing for multi-robot exploration. Leveraging attention-based neural networks trained via reinforcement and curriculum learning, IR2 allows robots to effectively reason about the longer-term trade-offs between disconnecting for solo exploration and reconnecting for information sharing. In addition, we propose a hierarchical graph formulation to maintain a sparse yet informative graph, enabling our approach to scale to large-scale environments. We present simulation results in three large-scale Gazebo environments, which show that our approach yields 6.6-34.1% shorter exploration paths and significantly improved mapped area consistency among robots when compared to state-of-the-art baselines. Our simulation training and testing code is available at https://github.com/marmotlab/IR2."
Robust Agility via Learned Zero Dynamics Policies,"Csomay-Shanklin, Noel; Compton, William; Jimenez Rodriguez, Ivan Dario; Ambrose, Eric; Yue, Yisong; Ames, Aaron",https://arxiv.org/abs/2409.06125,"We study the design of robust and agile controllers for hybrid underactuated systems. Our approach breaks down the task of creating a stabilizing controller into: 1) learning a mapping that is invariant under optimal control, and 2) driving the actuated coordinates to the output of that mapping. This approach, termed Zero Dynamics Policies, exploits the structure of underactuation by restricting the inputs of the target mapping to the subset of degrees of freedom that cannot be directly actuated, thereby achieving significant dimension reduction. Furthermore, we retain the stability and constraint satisfaction of optimal control while reducing the online computational overhead. We prove that controllers of this type stabilize hybrid underactuated systems and experimentally validate our approach on the 3D hopping platform, ARCHER. Over the course of 3000 hops the proposed framework demonstrates robust agility, maintaining stable hopping while rejecting disturbances on rough terrain."
Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models,"Nguyen, Phat; Wang, Tsun-Hsuan; Hong, Zhang-Wei; Karaman, Sertac; Rus, Daniela",https://arxiv.org/abs/2406.04300,"Generating varied scenarios through simulation is crucial for training and evaluating safety-critical systems, such as autonomous vehicles. Yet, the task of modeling the trajectories of other vehicles to simulate diverse and meaningful close interactions remains prohibitively costly. Adopting language descriptions to generate driving behaviors emerges as a promising strategy, offering a scalable and intuitive method for human operators to simulate a wide range of driving interactions. However, the scarcity of large-scale annotated language-trajectory data makes this approach challenging.   To address this gap, we propose Text-to-Drive (T2D) to synthesize diverse driving behaviors via Large Language Models (LLMs). We introduce a knowledge-driven approach that operates in two stages. In the first stage, we employ the embedded knowledge of LLMs to generate diverse language descriptions of driving behaviors for a scene. Then, we leverage LLM's reasoning capabilities to synthesize these behaviors in simulation. At its core, T2D employs an LLM to construct a state chart that maps low-level states to high-level abstractions. This strategy aids in downstream tasks such as summarizing low-level observations, assessing policy alignment with behavior description, and shaping the auxiliary reward, all without needing human supervision. With our knowledge-driven approach, we demonstrate that T2D generates more diverse trajectories compared to other baselines and offers a natural language interface that allows for interactive incorporation of human preference. Please check our website for more examples: https://text-to-drive.github.io/"
Underwater Hyperspectral Imaging for Measuring Seafloor Reflectance,"Zhang, Hongjie; Billings, Gideon; Shields, Jackson; Williams, Stefan Bernard",,
Ontology Based AI Planning and Scheduling for Robotic Assembly,"Zhao, Jingyun; Vogel-Heuser, Birgit; Ao, Jicong; Wu, Yansong; Zhang, Liding; Fandi, Bi; Hujo, Dominik; Bing, Zhenshan; Wu, Fan; Knoll, Alois; Haddadin, Sami; Vojanec, Bernd; Markert, Timo; Kraft, André",,
CAIS: Culvert Autonomous Inspection Robotic System,"Le, Chuong; Walunj, Pratik; Nguyen, An; Zhou, Yong; Nguyen, Thanh Binh; Nguyen, Thang; Netchaev, Anton; La, Hung",,
AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales,"Guan, Tianrui; Xian, Ruiqi; Wang, Xijun; Wu, Xiyang; Elnoor, Mohamed; Song, Daeun; Manocha, Dinesh",https://arxiv.org/abs/2404.03187,"We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment. The code and dataset will be made publicly available."
Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments,"Seong, Hyunki; Shim, David Hyunchul",https://arxiv.org/abs/2403.16664,"This paper focuses on the acquisition of mapless navigation skills within unknown environments. We introduce the Skill Q-Network (SQN), a novel reinforcement learning method featuring an adaptive skill ensemble mechanism. Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge. Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments. Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40\% higher performance compared to baseline models. Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios. Remarkably, our adaptive skill ensemble method enables zero-shot transfer to out-of-distribution domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments. The project page is available at https://sites.google.com/view/skill-q-net."
DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping,"Niessner, Matthias; Yilmaz, Kutay; Kornilova, Anastasiia; Artemov, Alexey",https://arxiv.org/abs/2403.17550,"Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as captured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks. The code of our approach will be made publicly available."
Learning Visual Quadrupedal Loco-Manipulation from Demonstrations,"He, Zhengmao; Lei, Kun; Ze, Yanjie; Sreenath, Koushil; Li, Zhongyu; Xu, Huazhe",https://arxiv.org/abs/2403.20328,"Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through simulations and real-world experiments, demonstrating the robot's ability to perform tasks that demand mobility and high precision, such as lifting a basket from the ground while moving, closing a dishwasher, pressing a button, and pushing a door. Project website: https://zhengmaohe.github.io/leg-manip"
Finetuning Pre-trained Model with Limited Data for LiDAR-based 3D Object Detection by Bridging Domain Gaps,"Jang, Jiyun; Chang, Mincheol; Kim, Jinkyu",,
STL-SLAM: A Structured-Constrained RGB-D SLAM Approach in Texture-Limited Environments,"Dong, Juan; Lu, Maobin; Deng, Fang; Chen, Jie",,
Optimizing Interaction Space: Enlarging the Capture Volume for Multiple Portable Motion Capture Devices,"Fatoni, Muhammad Hilman; Herneth, Christopher; Li, Junnan; Budiman, Fajar; Ganguly, Amartya; Haddadin, Sami",https://arxiv.org/abs/2408.17287,"Markerless motion capture devices such as the Leap Motion Controller (LMC) have been extensively used for tracking hand, wrist, and forearm positions as an alternative to Marker-based Motion Capture (MMC). However, previous studies have highlighted the subpar performance of LMC in reliably recording hand kinematics. In this study, we employ four LMC devices to optimize their collective tracking volume, aiming to enhance the accuracy and precision of hand kinematics. Through Monte Carlo simulation, we determine an optimized layout for the four LMC devices and subsequently conduct reliability and validity experiments encompassing 1560 trials across ten subjects. The combined tracking volume is validated against an MMC system, particularly for kinematic movements involving wrist, index, and thumb flexion. Utilizing calculation resources in one computer, our result of the optimized configuration has a better visibility rate with a value of 0.05 $\pm$ 0.55 compared to the initial configuration with -0.07 $\pm$ 0.40. Multiple Leap Motion Controllers (LMCs) have proven to increase the interaction space of capture volume but are still unable to give agreeable measurements from dynamic movement."
Look before you leap: Socially acceptable high-speed ground robot navigation in crowded hallways,"Sharma, Lakshay; How, Jonathan",https://arxiv.org/abs/2403.13284,"To operate safely and efficiently, autonomous warehouse/delivery robots must be able to accomplish tasks while navigating in dynamic environments and handling the large uncertainties associated with the motions/behaviors of other robots and/or humans. A key scenario in such environments is the hallway problem, where robots must operate in the same narrow corridor as human traffic going in one or both directions. Traditionally, robot planners have tended to focus on socially acceptable behavior in the hallway scenario at the expense of performance. This paper proposes a planner that aims to address the consequent ""robot freezing problem"" in hallways by allowing for ""peek-and-pass"" maneuvers. We then go on to demonstrate in simulation how this planner improves robot time to goal without violating social norms. Finally, we show initial hardware demonstrations of this planner in the real world."
LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving,"Paul, Pranjal; Garg, Anant; Choudhary, Tushar; Singh, Arun Kumar; Krishna, Madhava",https://arxiv.org/abs/2403.20116,"Existing Vision-Language models (VLMs) estimate either long-term trajectory waypoints or a set of control actions as a reactive solution for closed-loop planning based on their rich scene comprehension. However, these estimations are coarse and are subjective to their ""world understanding"" which may generate sub-optimal decisions due to perception errors. In this paper, we introduce LeGo-Drive, which aims to address this issue by estimating a goal location based on the given language command as an intermediate representation in an end-to-end setting. The estimated goal might fall in a non-desirable region, like on top of a car for a parking-like command, leading to inadequate planning. Hence, we propose to train the architecture in an end-to-end manner, resulting in iterative refinement of both the goal and the trajectory collectively. We validate the effectiveness of our method through comprehensive experiments conducted in diverse simulated environments. We report significant improvements in standard autonomous driving metrics, with a goal reaching Success Rate of 81%. We further showcase the versatility of LeGo-Drive across different driving scenarios and linguistic inputs, underscoring its potential for practical deployment in autonomous vehicles and intelligent transportation systems."
Whleaper: A 10-DOF High-Performance Bipedal Wheeled Robot,"Zhu, Yinglei; He, SiXiao; Qi, Zhenghao; Yong, Zhuoyuan; Qin, Yihua; Chen, Jianyu",,
LANCAR: Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments,"Shek, Chak Lam; Wu, Xiyang; Suttle, Wesley A.; Busart, Carl; zaroukian, erin; Manocha, Dinesh; Tokekar, Pratap; Bedi, Amrit Singh",https://arxiv.org/abs/2310.00481,"Navigating robots through unstructured terrains is challenging, primarily due to the dynamic environmental changes. While humans adeptly navigate such terrains by using context from their observations, creating a similar context-aware navigation system for robots is difficult. The essence of the issue lies in the acquisition and interpretation of contextual information, a task complicated by the inherent ambiguity of human language. In this work, we introduce LANCAR, which addresses this issue by combining a context translator with reinforcement learning (RL) agents for context-aware locomotion. LANCAR allows robots to comprehend contextual information through Large Language Models (LLMs) sourced from human observers and convert this information into actionable contextual embeddings. These embeddings, combined with the robot's sensor data, provide a complete input for the RL agent's policy network. We provide an extensive evaluation of LANCAR under different levels of contextual ambiguity and compare with alternative methods. The experimental results showcase the superior generalizability and adaptability across different terrains. Notably, LANCAR shows at least a 7.4% increase in episodic reward over the best alternatives, highlighting its potential to enhance robotic navigation in unstructured environments. More details and experiment videos could be found in http://raaslab.org/projects/LLM_Context_Estimation/."
Monocular Depth Estimation for Drone Obstacle Avoidance in Indoor Environments,"Zheng, Haokun; Rajadnya, Sidhant; Zakhor, Avideh",,
Absolute Pose Estimation for a Millimeter-Scale Vision System,"Ozturk, Derin; Wang, Zilin; Helbling, E. Farrell",,
Context-Generative Default Policy for Bounded Rational Agent,"Pushp, Durgakant; Xu, Junhong; Chen, Zheng; Liu, Lantao",,
EyeSight Hand: Design of a Fully-Actuated Dexterous Robot Hand with Integrated Vision-Based Tactile Sensors and Compliant Actuation,"Romero, Branden; Fang, Hao-Shu; Agrawal, Pulkit; Adelson, Edward",https://arxiv.org/abs/2408.06265,"In this work, we introduce the EyeSight Hand, a novel 7 degrees of freedom (DoF) humanoid hand featuring integrated vision-based tactile sensors tailored for enhanced whole-hand manipulation. Additionally, we introduce an actuation scheme centered around quasi-direct drive actuation to achieve human-like strength and speed while ensuring robustness for large-scale data collection. We evaluate the EyeSight Hand on three challenging tasks: bottle opening, plasticine cutting, and plate pick and place, which require a blend of complex manipulation, tool use, and precise force application. Imitation learning models trained on these tasks, with a novel vision dropout strategy, showcase the benefits of tactile feedback in enhancing task success rates. Our results reveal that the integration of tactile sensing dramatically improves task performance, underscoring the critical role of tactile information in dexterous manipulation."
Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization,"Idoko, Simon; sharma, basant; Singh, Arun Kumar",https://arxiv.org/abs/2403.19461,"Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds."
RISE: 3D Perception Makes Real-World Robot Imitation Simple and Effective,"Wang, Chenxi; Fang, Hongjie; Fang, Hao-Shu; Lu, Cewu",https://arxiv.org/abs/2404.12281,"Precise robot manipulations require rich spatial information in imitation learning. Image-based policies model object positions from fixed cameras, which are sensitive to camera view changes. Policies utilizing 3D point clouds usually predict keyframes rather than continuous actions, posing difficulty in dynamic and contact-rich scenarios. To utilize 3D perception efficiently, we present RISE, an end-to-end baseline for real-world imitation learning, which predicts continuous actions directly from single-view point clouds. It compresses the point cloud to tokens with a sparse 3D encoder. After adding sparse positional encoding, the tokens are featurized using a transformer. Finally, the features are decoded into robot actions by a diffusion head. Trained with 50 demonstrations for each real-world task, RISE surpasses currently representative 2D and 3D policies by a large margin, showcasing significant advantages in both accuracy and efficiency. Experiments also demonstrate that RISE is more general and robust to environmental change compared with previous baselines. Project website: rise-policy.github.io."
MADE: Malicious Agent Detection for Robust Multi-Agent Collaborative Perception,"Zhao, Yangheng; Xiang, Zhen; Yin, Sheng; Pang, Xianghe; Wang, Yanfeng; Chen, Siheng",https://arxiv.org/abs/2310.11901,"Recently, multi-agent collaborative (MAC) perception has been proposed and outperformed the traditional single-agent perception in many applications, such as autonomous driving. However, MAC perception is more vulnerable to adversarial attacks than single-agent perception due to the information exchange. The attacker can easily degrade the performance of a victim agent by sending harmful information from a malicious agent nearby. In this paper, we extend adversarial attacks to an important perception task -- MAC object detection, where generic defenses such as adversarial training are no longer effective against these attacks. More importantly, we propose Malicious Agent Detection (MADE), a reactive defense specific to MAC perception that can be deployed by each agent to accurately detect and then remove any potential malicious agent in its local collaboration network. In particular, MADE inspects each agent in the network independently using a semi-supervised anomaly detector based on a double-hypothesis test with the Benjamini-Hochberg procedure to control the false positive rate of the inference. For the two hypothesis tests, we propose a match loss statistic and a collaborative reconstruction loss statistic, respectively, both based on the consistency between the agent to be inspected and the ego agent where our detector is deployed. We conduct comprehensive evaluations on a benchmark 3D dataset V2X-sim and a real-road dataset DAIR-V2X and show that with the protection of MADE, the drops in the average precision compared with the best-case ""oracle"" defender against our attack are merely 1.28% and 0.34%, respectively, much lower than 8.92% and 10.00% for adversarial training, respectively."
OAS-GPUCB: On-the-way Adaptive Sampling Using GPUCB for Bathymetry Mapping,"AGRAWAL, RAJAT; Nambiar, Karthik; Chhaglani, Bhawana; PB, Sujit; Chitre, Mandar",,
Cooperative Path Planning for Four-Way Shuttle Vehicles in Storage and Retrieval Systems: A Hierarchically Dynamic Graph Based Approach,"Han, Xingyao; Tan, Yuhong; Chen, Siyuan; Liu, Zhe; Wang, Hesheng",,
MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes,"Wu, Chenyang; Duan, Yifan; Zhang, Xinran; Sheng, Yu; Ji, Jianmin; Zhang, Yanyong",https://arxiv.org/abs/2404.04026,"Localization and mapping are critical tasks for various applications such as autonomous vehicles and robotics. The challenges posed by outdoor environments present particular complexities due to their unbounded characteristics. In this work, we present MM-Gaussian, a LiDAR-camera multi-modal fusion system for localization and mapping in unbounded scenes. Our approach is inspired by the recently developed 3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering quality and fast rendering speed. Specifically, our system fully utilizes the geometric structure information provided by solid-state LiDAR to address the problem of inaccurate depth encountered when relying solely on visual solutions in unbounded, outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the assistance of pixel-level gradient descent, to fully exploit the color information in photos, thereby achieving realistic rendering effects. To further bolster the robustness of our system, we designed a relocalization module, which assists in returning to the correct trajectory in the event of a localization failure. Experiments conducted in multiple scenarios demonstrate the effectiveness of our method."
Online Refractive Camera Model Calibration in Visual Inertial Odometry,"Singh, Mohit; Alexis, Kostas",https://arxiv.org/abs/2310.16658,"This work presents a camera model for refractive media such as water and its application in underwater visual-inertial odometry. The model is self-calibrating in real-time and is free of known correspondences or calibration targets. It is separable as a distortion model (dependent on refractive index $n$ and radial pixel coordinate) and a virtual pinhole model (as a function of $n$). We derive the self-calibration formulation leveraging epipolar constraints to estimate the refractive index and subsequently correct for distortion. Through experimental studies using an underwater robot integrating cameras and inertial sensing, the model is validated regarding the accurate estimation of the refractive index and its benefits for robust odometry estimation in an extended envelope of conditions. Lastly, we show the transition between media and the estimation of the varying refractive index online, thus allowing computer vision tasks across refractive media."
Constrained Bootstrapped Learning for Few-Shot Robot Skill Adaptation,"Haque, A K M Nadimul; Sukkar, Fouad; Tanz, Lukas; Carmichael, Marc; Vidal-Calleja, Teresa A.",,
Multi-Goal Path Planning in Cluttered Environments with PRM-Guided Self-Organising Maps,"Davis, Benjamin R.; Bray, Edward; Best, Graeme",,
Active Neural Mapping at Scale,"Kuang, Zijia; Yan, Zike; Zhao, Hao; Zhou, Guyue; Zha, Hongbin",https://arxiv.org/abs/2206.08615,"Many feedforward neural networks (NNs) generate continuous and piecewise-linear (CPWL) mappings. Specifically, they partition the input domain into regions on which the mapping is affine. The number of these so-called linear regions offers a natural metric to characterize the expressiveness of CPWL NNs. The precise determination of this quantity is often out of reach in practice, and bounds have been proposed for specific architectures, including for ReLU and Maxout NNs. In this work, we generalize these bounds to NNs with arbitrary and possibly multivariate CPWL activation functions. We first provide upper and lower bounds on the maximal number of linear regions of a CPWL NN given its depth, width, and the number of linear regions of its activation functions. Our results rely on the combinatorial structure of convex partitions and confirm the distinctive role of depth which, on its own, is able to exponentially increase the number of regions. We then introduce a complementary stochastic framework to estimate the average number of linear regions produced by a CPWL NN. Under reasonable assumptions, the expected density of linear regions along any 1D path is bounded by the product of depth, width, and a measure of activation complexity (up to a scaling factor). This yields an identical role to the three sources of expressiveness: no exponential growth with depth is observed anymore."
Distributed Model Predictive Covariance Steering,"Saravanos, Augustinos; Balci, Isin; Bakolas, Efstathios; Theodorou, Evangelos",https://arxiv.org/abs/2212.00398,"This paper proposes Distributed Model Predictive Covariance Steering (DMPCS), a novel method for safe multi-robot control under uncertainty. The scope of our approach is to blend covariance steering theory, distributed optimization and model predictive control (MPC) into a single methodology that is safe, scalable and decentralized. Initially, we pose a problem formulation that uses the Wasserstein distance to steer the state distributions of a multi-robot team to desired targets, and probabilistic constraints to ensure safety. We then transform this problem into a finite-dimensional optimization one by utilizing a disturbance feedback policy parametrization for covariance steering and a tractable approximation of the safety constraints. To solve the latter problem, we derive a decentralized consensus-based algorithm using the Alternating Direction Method of Multipliers (ADMM). This method is then extended to a receding horizon form, which yields the proposed DMPCS algorithm. Simulation experiments on large-scale problems with up to hundreds of robots successfully demonstrate the effectiveness and scalability of DMPCS. Its superior capability in achieving safety is also highlighted through a comparison against a standard stochastic MPC approach. A video with all simulation experiments is available in https://youtu.be/Hks-0BRozxA."
Interpretation of Legged Locomotion in Underwater Robots based on Rimless Wheel Model,"HE, YUETONG; Asano, Fumihiko",,
Disturbance-Aware Model Predictive Control of Underactuated Robotics Systems,"Kim, Jiwon; Kim, Min Jun",,
Leveraging Symmetry in RL-based Legged Locomotion Control,"Su, Zhi; Huang, Xiaoyu; Ordonez Apraez, Daniel Felipe; Li, Yunfei; Li, Zhongyu; Liao, Qiayuan; Pontil, Massimiliano; Semini, Claudio; Turrisi, Giulio; Wu, Yi; Sreenath, Koushil",https://arxiv.org/abs/2403.17320,"Model-free reinforcement learning is a promising approach for autonomously solving challenging robotics control problems, but faces exploration difficulty without information of the robot's kinematics and dynamics morphology. The under-exploration of multiple modalities with symmetric states leads to behaviors that are often unnatural and sub-optimal. This issue becomes particularly pronounced in the context of robotic systems with morphological symmetries, such as legged robots for which the resulting asymmetric and aperiodic behaviors compromise performance, robustness, and transferability to real hardware. To mitigate this challenge, we can leverage symmetry to guide and improve the exploration in policy learning via equivariance/invariance constraints. In this paper, we investigate the efficacy of two approaches to incorporate symmetry: modifying the network architectures to be strictly equivariant/invariant, and leveraging data augmentation to approximate equivariant/invariant actor-critics. We implement the methods on challenging loco-manipulation and bipedal locomotion tasks and compare with an unconstrained baseline. We find that the strictly equivariant policy consistently outperforms other methods in sample efficiency and task performance in simulation. In addition, symmetry-incorporated approaches exhibit better gait quality, higher robustness and can be deployed zero-shot in real-world experiments."
Trajectory Planning for Non-Prehensile Object Transportation,"Chen, Lingyun; Yu, Haoyu; Naceri, Abdeldjallil; Swikir, Abdalla; Haddadin, Sami",https://arxiv.org/abs/2408.16420,"Non-prehensile object transportation offers a way to enhance robotic performance in object manipulation tasks, especially with unstable objects. Effective trajectory planning requires simultaneous consideration of robot motion constraints and object stability. Here, we introduce a physical model for object stability and propose a novel trajectory planning approach for non-prehensile transportation along arbitrary straight lines in 3D space. Validation with a 7-DoF Franka Panda robot confirms improved transportation speed via tray rotation integration while ensuring object stability and robot motion constraints."
Adversarial Attack on Trajectory Prediction for Autonomous Vehicles with Generative Adversarial Networks,"Fan, Jiping; Wang, Zhenpo; Li, Guoqiang",,
Spatiotemporal Co-Design Enabling Prioritized Multi-Agent Motion Planning,"Huang, Yunshen; He, Wenbo; Kantaros, Yiannis; Zeng, Shen",,
Design and Validation of Soft Flexible Aerial Robot for Safe Human-Robot Interaction,"Jia, Fuhua; Zheng, Zihao; LI, Cheng'ao; Li, Rui; XIAO, Junlin; YANG, Xiaoying; Rushworth, Adam; Ijaz, Salman",,
Grounding Spatio-temporal Navigation Commands Using Large Language and Vision Models,"Liu, Jason Xinyu; Shah, Ankit; Konidaris, George; Tellex, Stefanie; Paulius, David",,
Empathetic Response Generation System: Enhancing Photo Reminiscence Chatbot with Emotional Context Analysis,"Herrera Ruiz, Alberto; Qian, Xiaobei; Fu, Li-Chen",,
Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods,"Venkatesh, L.N Vishnunandan; Min, Byung-Cheol",https://arxiv.org/abs/2404.02324,"Learning from Demonstration (LfD) is a promising approach to enable Multi-Robot Systems (MRS) to acquire complex skills and behaviors. However, the intricate interactions and coordination challenges in MRS pose significant hurdles for effective LfD. In this paper, we present a novel LfD framework specifically designed for MRS, which leverages visual demonstrations to capture and learn from robot-robot and robot-object interactions. Our framework introduces the concept of Interaction Keypoints (IKs) to transform the visual demonstrations into a representation that facilitates the inference of various skills necessary for the task. The robots then execute the task using sensorimotor actions and reinforcement learning (RL) policies when required. A key feature of our approach is the ability to handle unseen contact-based skills that emerge during the demonstration. In such cases, RL is employed to learn the skill using a classifier-based reward function, eliminating the need for manual reward engineering and ensuring adaptability to environmental changes. We evaluate our framework across a range of mobile robot tasks, covering both behavior-based and contact-based domains. The results demonstrate the effectiveness of our approach in enabling robots to learn complex multi-robot tasks and behaviors from visual demonstrations."
A Novel Variable Step-size Path Planning Framework with Step Consistent Markov Decision Process For Large Scale UAV Swarm,"Xu, Dan; Guo, Yunxiao; Wang, Chang; Long, Han",,
A Piecewise-weighted RANSAC Method Utilizing Abandoned Hypothesis Model Information with a New Application on Robot Self-calibration,"He, Jianhui; Feng, Yiyang; Yang, Guilin; Shen, Wenjun; Chen, Silu; Zheng, Tianjiang; li, Junjie",,
ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models,"Huang, Siyuan; Ponomarenko, Iaroslav; Jiang, Zhengkai; Li, Xiaoqi; HU, XIAOBIN; Gao, Peng; Li, Hongsheng; Dong, Hao",https://arxiv.org/abs/2403.11289,"While the integration of Multi-modal Large Language Models (MLLMs) with robotic systems has significantly improved robots' ability to understand and execute natural language instructions, their performance in manipulation tasks remains limited due to a lack of robotics-specific knowledge. Conventional MLLMs are typically trained on generic image-text pairs, leaving them deficient in understanding affordances and physical concepts crucial for manipulation. To address this gap, we propose ManipVQA, a novel framework that infuses MLLMs with manipulation-centric knowledge through a Visual Question-Answering (VQA) format. This approach encompasses tool detection, affordance recognition, and a broader understanding of physical concepts. We curated a diverse dataset of images depicting interactive objects, to challenge robotic understanding in tool detection, affordance prediction, and physical concept comprehension. To effectively integrate this robotics-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we leverage a unified VQA format and devise a fine-tuning strategy. This strategy preserves the original vision-reasoning abilities while incorporating the newly acquired robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA. The code and dataset are publicly available at https://github.com/SiyuanHuang95/ManipVQA."
Understanding How a 3-dimensional ZMP Exactly Decouples the Horizontal and Vertical Dynamics of the CoM-ZMP Model,"Onishi, Yuki; Kajita, Shuuji",,
PS-Loc: Robust LiDAR Localization with Prior Structural Reference,"Li, Rui; Zhao, Wentao; Deng, Tianchen; Wang, Yanbo; Wang, Jingchuan",,
Harnessing Symmetry Breaking in Soft Robotics: A Novel Approach for Underactuated Fingers,"Hashem, Ryman; Howison, Toby; Stilli, Agostino; Stoyanov, Danail; XU, Peter; Iida, Fumiya",,
Event-based Few-shot Fine-grained Human Action Recognition,"Yang, Zonglin; Yang, Yan; Shi, Yuheng; Yang, Hao; Zhang, Ruikun; Liu, Liu; WU, Xinxiao; Pan, Liyuan",,
Pre-training on Synthetic Driving Data for Trajectory Prediction,"Li, Yiheng; Zhao, Zhihao; Xu, Chenfeng; TANG, CHEN; Li, Chenran; Ding, Mingyu; Tomizuka, Masayoshi; Zhan, Wei",https://arxiv.org/abs/2309.10121,"Accumulating substantial volumes of real-world driving data proves pivotal in the realm of trajectory forecasting for autonomous driving. Given the heavy reliance of current trajectory forecasting models on data-driven methodologies, we aim to tackle the challenge of learning general trajectory forecasting representations under limited data availability. We propose a pipeline-level solution to mitigate the issue of data scarcity in trajectory forecasting. The solution is composed of two parts: firstly, we adopt HD map augmentation and trajectory synthesis for generating driving data, and then we learn representations by pre-training on them. Specifically, we apply vector transformations to reshape the maps, and then employ a rule-based model to generate trajectories on both original and augmented scenes; thus enlarging the driving data without collecting additional real ones. To foster the learning of general representations within this augmented dataset, we comprehensively explore the different pre-training strategies, including extending the concept of a Masked AutoEncoder (MAE) for trajectory forecasting. Without bells and whistles, our proposed pipeline-level solution is general, simple, yet effective: we conduct extensive experiments to demonstrate the effectiveness of our data expansion and pre-training strategies, which outperform the baseline prediction model by large margins, e.g. 5.04%, 3.84% and 8.30% in terms of $MR_6$, $minADE_6$ and $minFDE_6$. The pre-training dataset and the codes for pre-training and fine-tuning are released at https://github.com/yhli123/Pretraining_on_Synthetic_Driving_Data_for_Trajectory_Prediction."
Self-Supervised Monocular Depth Estimation with Effective Feature Fusion and Self-Distillation,"Liu, ZhenFei; Song, Chengqun; Cheng, Jun; Luo, Jiefu; wang, xiaoyang",https://arxiv.org/abs/2212.05729,"The exploration of mutual-benefit cross-domains has shown great potential toward accurate self-supervised depth estimation. In this work, we revisit feature fusion between depth and semantic information and propose an efficient local adaptive attention method for geometric aware representation enhancement. Instead of building global connections or deforming attention across the feature space without restraint, we bound the spatial interaction within a learnable region of interest. In particular, we leverage geometric cues from semantic information to learn local adaptive bounding boxes to guide unsupervised feature aggregation. The local areas preclude most irrelevant reference points from attention space, yielding more selective feature learning and faster convergence. We naturally extend the paradigm into a multi-head and hierarchic way to enable the information distillation in different semantic levels and improve the feature discriminative ability for fine-grained depth estimation. Extensive experiments on the KITTI dataset show that our proposed method establishes a new state-of-the-art in self-supervised monocular depth estimation task, demonstrating the effectiveness of our approach over former Transformer variants."
Design and control of a novel multi-degree-of-freedom hybrid robotic arm,"Chen, Yang; Miao, Zhonghua; Xiong, Ya",,
LTL-D*: Incrementally Optimal Replanning for Feasible and Infeasible Tasks in Linear Temporal Logic Specifications,"Ren, Jiming; Miller, Haris; Feigh, Karen; Coogan, Samuel; Zhao, Ye",https://arxiv.org/abs/2404.01219,"This paper presents an incremental replanning algorithm, dubbed LTL-D*, for temporal-logic-based task planning in a dynamically changing environment. Unexpected changes in the environment may lead to failures in satisfying a task specification in the form of a Linear Temporal Logic (LTL). In this study, the considered failures are categorized into two classes: (i) the desired LTL specification can be satisfied via replanning, and (ii) the desired LTL specification is infeasible to meet strictly and can only be satisfied in a ""relaxed"" fashion. To address these failures, the proposed algorithm finds an optimal replanning solution that minimally violates desired task specifications. In particular, our approach leverages the D* Lite algorithm and employs a distance metric within the synthesized automaton to quantify the degree of the task violation and then replan incrementally. This ensures plan optimality and reduces planning time, especially when frequent replanning is required. Our approach is implemented in a robot navigation simulation to demonstrate a significant improvement in the computational efficiency for replanning by two orders of magnitude."
Development of a Mobile Reconfigurable Mecanum Robot with a Wheel Roller Locking Device,"Zakharov, Dmitriy; Iaremenko, Andrei; Kurovskii, Denis; Kurovskii, Artem; Borisov, Oleg",,
TriHelper: Zero-Shot Object Navigation with Dynamic Assistance,"Zhang, Lingfeng; Zhang, Qiang; WANG, Hao; Xiao, Erjia; Jiang, Zixuan; CHEN, Honglei; Xu, Renjing",https://arxiv.org/abs/2403.15223,"Navigating toward specific objects in unknown environments without additional training, known as Zero-Shot object navigation, poses a significant challenge in the field of robotics, which demands high levels of auxiliary information and strategic planning. Traditional works have focused on holistic solutions, overlooking the specific challenges agents encounter during navigation such as collision, low exploration efficiency, and misidentification of targets. To address these challenges, our work proposes TriHelper, a novel framework designed to assist agents dynamically through three primary navigation challenges: collision, exploration, and detection. Specifically, our framework consists of three innovative components: (i) Collision Helper, (ii) Exploration Helper, and (iii) Detection Helper. These components work collaboratively to solve these challenges throughout the navigation process. Experiments on the Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper significantly outperforms all existing baseline methods in Zero-Shot object navigation, showcasing superior success rates and exploration efficiency. Our ablation studies further underscore the effectiveness of each helper in addressing their respective challenges, notably enhancing the agent's navigation capabilities. By proposing TriHelper, we offer a fresh perspective on advancing the object navigation task, paving the way for future research in the domain of Embodied AI and visual-based navigation."
OPENGRASP-LITE Version 1.0: A Tactile Artificial Hand with a Compliant Linkage Mechanism,"Groß, Sonja; Ratzel, Michael; Welte, Edgar; Hidalgo Carvajal, Diego Xavier; Chen, Lingyun; Pozo Fortuni&#263;, Edmundo; Ganguly, Amartya; Swikir, Abdalla; Haddadin, Sami",https://arxiv.org/abs/2408.02293,"Recent research has seen notable progress in the development of linkage-based artificial hands. While previous designs have focused on adaptive grasping, dexterity and biomimetic artificial skin, only a few systems have proposed a lightweight, accessible solution integrating tactile sensing with a compliant linkage-based mechanism. This paper introduces OPENGRASP LITE, an open-source, highly integrated, tactile, and lightweight artificial hand. Leveraging compliant linkage systems and MEMS barometer-based tactile sensing, it offers versatile grasping capabilities with six degrees of actuation. By providing tactile sensors and enabling soft grasping, it serves as an accessible platform for further research in tactile artificial hands."
Pneumatic bladder links connected by joints with a wide range of motion for articulated inflatable robots,"Uchiyama, Katsu; Niiyama, Ryuma",,
Efficient-PIP: Large-Scale Pixel-level Aligned Image Pair Generation for Cross-time Infrared-RGB Translation,"Li, Jian; Fei, Kexin; Sun, Yi; Wang, Jie; Liu, Bokai; Zhou, Zongtan; Zheng, Yongbin; Sun, Zhenping",,
OmniRace: 6D Hand Pose Estimation for Intuitive Guidance of Racing Drone,"Serpiva, Valerii; Fedoseev, Aleksey; Karaf, Sausar; Abdulkarim, Ali Alridha; Dzmitry, Tsetserukou",https://arxiv.org/abs/2407.09841,"This paper presents the OmniRace approach to controlling a racing drone with 6-degree of freedom (DoF) hand pose estimation and gesture recognition. To our knowledge, it is the first-ever technology that allows for low-level control of high-speed drones using gestures. OmniRace employs a gesture interface based on computer vision and a deep neural network to estimate a 6-DoF hand pose. The advanced machine learning algorithm robustly interprets human gestures, allowing users to control drone motion intuitively. Real-time control of a racing drone demonstrates the effectiveness of the system, validating its potential to revolutionize drone racing and other applications. Experimental results conducted in the Gazebo simulation environment revealed that OmniRace allows the users to complite the UAV race track significantly (by 25.1%) faster and to decrease the length of the test drone path (from 102.9 to 83.7 m). Users preferred the gesture interface for attractiveness (1.57 UEQ score), hedonic quality (1.56 UEQ score), and lower perceived temporal demand (32.0 score in NASA-TLX), while noting the high efficiency (0.75 UEQ score) and low physical demand (19.0 score in NASA-TLX) of the baseline remote controller. The deep neural network attains an average accuracy of 99.75% when applied to both normalized datasets and raw datasets. OmniRace can potentially change the way humans interact with and navigate racing drones in dynamic and complex environments. The source code is available at https://github.com/SerValera/OmniRace.git."
Stein Movement Primitives for Adaptive Multi-Modal Trajectory Generation,"zeya, yin; Lai, Tin; Khan, Subhan; Jacob, Jayadeep; li, yong hui; Ramos, Fabio",,
Non-repetitive: A promising LiDAR scanning pattern,"Xie, Angchen; Qian, Yeqiang; Yan, Weihao; Wang, Chunxiang; Yang, Ming",https://arxiv.org/abs/2206.08517,"Prism-based LiDARs are more compact and cheaper than the conventional mechanical multi-line spinning LiDARs, which have become increasingly popular in robotics, recently. However, there are several challenges for these new LiDAR sensors, including small field of view, severe motion distortions, and irregular patterns, which hinder them from being widely used in LiDAR odometry, practically. To tackle these problems, we present an effective continuous-time LiDAR odometry (ECTLO) method for the Risley-prism-based LiDARs with non-repetitive scanning patterns. A single range image covering historical points in LiDAR's small FoV is adopted for efficient map representation. To account for the noisy data from occlusions after map updating, a filter-based point-to-plane Gaussian Mixture Model is used for robust registration. Moreover, a LiDAR-only continuous-time motion model is employed to relieve the inevitable distortions. Extensive experiments have been conducted on various testbeds using the prism-based LiDARs with different scanning patterns, whose promising results demonstrate the efficacy of our proposed approach."
Online Determination of Legged Kinematics,"Burgul, Chinmay; Lee, Woosik; Geneva, Patrick; Huang, Guoquan",,
EVSMap: An Efficient Volumetric-Semantic Mapping Approach for Embedded Systems,"Qiu, Jiyuan; Jiang, Chen; Zhang, Pengfei; Wang, Haowen",,
Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand Orthosis for Stroke,"La Rotta, Pedro Leandro; Xu, Jingxi; Chen, Ava; Winterbottom, Lauren; Chen, Wenxi; Nilsen, Dawn; Stein, Joel; Ciocarlie, Matei",https://arxiv.org/abs/2403.13147,"We propose MetaEMG, a meta-learning approach for fast adaptation in intent inferral on a robotic hand orthosis for stroke. One key challenge in machine learning for assistive and rehabilitative robotics with disabled-bodied subjects is the difficulty of collecting labeled training data. Muscle tone and spasticity often vary significantly among stroke subjects, and hand function can even change across different use sessions of the device for the same subject. We investigate the use of meta-learning to mitigate the burden of data collection needed to adapt high-capacity neural networks to a new session or subject. Our experiments on real clinical data collected from five stroke subjects show that MetaEMG can improve the intent inferral accuracy with a small session- or subject-specific dataset and very few fine-tuning epochs. To the best of our knowledge, we are the first to formulate intent inferral on stroke subjects as a meta-learning problem and demonstrate fast adaptation to a new session or subject for controlling a robotic hand orthosis with EMG signals."
Dynamic Walking on Highly Underactuated Point Foot Humanoids: Closing the Loop between HZD and HLIP,"Ghansah, Adrian; Kim, Jeeseop; Li, Kejun; Ames, Aaron",https://arxiv.org/abs/2406.13115,"Realizing bipedal locomotion on humanoid robots with point feet is especially challenging due to their highly underactuated nature, high degrees of freedom, and hybrid dynamics resulting from impacts. With the goal of addressing this challenging problem, this paper develops a control framework for realizing dynamic locomotion and implements it on a novel point foot humanoid: ADAM. To this end, we close the loop between Hybrid Zero Dynamics (HZD) and Hybrid linear inverted pendulum (HLIP) based step length regulation. To leverage the full-order hybrid dynamics of the robot, walking gaits are first generated offline by utilizing HZD. These trajectories are stabilized online through the use of a HLIP based regulator. Finally, the planned trajectories are mapped into the full-order system using a task space controller incorporating inverse kinematics. The proposed method is verified through numerical simulations and hardware experiments on the humanoid robot ADAM marking the first humanoid point foot walking. Moreover, we experimentally demonstrate the robustness of the realized walking via the ability to track a desired reference speed, robustness to pushes, and locomotion on uneven terrain."
Robust Two-View Geometry Estimation with Implicit Differentiation,"Pyatov, Vladislav; Koshelev, Iaroslav; Lefkimmiatis, Stamatios",,
"Team Coordination on Graphs: Problem, Analysis, and Algorithms","Limbu, Manshi; Zhou, Yanlin; Stein, Gregory; Wang, Xuan; Shishika, Daigo; Xiao, Xuesu",https://arxiv.org/abs/2403.15946,"Team Coordination on Graphs with Risky Edges (TCGRE) is a recently emerged problem, in which a robot team collectively reduces graph traversal cost through support from one robot to another when the latter traverses a risky edge. Resembling the traditional Multi-Agent Path Finding (MAPF) problem, both classical and learning-based methods have been proposed to solve TCGRE, however, they lacked either computational efficiency or optimality assurance. In this paper, we reformulate TCGRE as a constrained optimization problem and perform a rigorous mathematical analysis. Our theoretical analysis shows the NP-hardness of TCGRE by reduction from the Maximum 3D Matching problem and that efficient decomposition is a key to tackle this combinatorial optimization problem. Furthermore, we design three classes of algorithms to solve TCGRE, i.e., Joint State Graph (JSG) based, coordination based, and receding-horizon sub-team based solutions. Each of these proposed algorithms enjoy different provable optimality and efficiency characteristics that are demonstrated in our extensive experiments."
Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Baseline,"Ding, Yufei; Geng, Haoran; Xu, Chaoyi; Fang, Xiaomeng; Zhang, Jiazhao; Wei, Songlin; Zhang, Zhizheng; Wang, He",,
A Sampling Ensemble for Asymptotically Complete Motion Planning with Volume-Reducing Workspace Constraints,"Li, Sihui; Schack, Matthew; Upadhyay, Aakriti; Dantam, Neil",,
Towards Design and Development of a Soft Pressure Sensing Sleeve for Performing Safe Colonoscopic Procedures,"RafieeJavazm, Mohammad; Kiehler, Sonika; Kara, Ozdemir Can; Alambeigi, Farshid",,
PathFinder: Attention-Driven Dynamic Non-Line-of-Sight Tracking with a Mobile Robot,"Kannapiran, Shenbagaraj; Chandran, Sreenithy; Jayasuriya, Suren; Berman, Spring",https://arxiv.org/abs/2404.05024,"The study of non-line-of-sight (NLOS) imaging is growing due to its many potential applications, including rescue operations and pedestrian detection by self-driving cars. However, implementing NLOS imaging on a moving camera remains an open area of research. Existing NLOS imaging methods rely on time-resolved detectors and laser configurations that require precise optical alignment, making it difficult to deploy them in dynamic environments. This work proposes a data-driven approach to NLOS imaging, PathFinder, that can be used with a standard RGB camera mounted on a small, power-constrained mobile robot, such as an aerial drone. Our experimental pipeline is designed to accurately estimate the 2D trajectory of a person who moves in a Manhattan-world environment while remaining hidden from the camera's field-of-view. We introduce a novel approach to process a sequence of dynamic successive frames in a line-of-sight (LOS) video using an attention-based neural network that performs inference in real-time. The method also includes a preprocessing selection metric that analyzes images from a moving camera which contain multiple vertical planar surfaces, such as walls and building facades, and extracts planes that return maximum NLOS information. We validate the approach on in-the-wild scenes using a drone for video capture, thus demonstrating low-cost NLOS imaging in dynamic capture environments."
Optimal Integration of Hybrid FES-Exoskeleton for Precise Knee Trajectory Control,"Jafaripour, Masoud; Mushahwar, Vivian K.; Tavakoli, Mahdi",,
Stability of a Team of Tethered Ground Robots on Extreme Terrains,"Kumar, Rahul; Chipade, Vishnu S.; Yong, Sze Zheng",,
Joint Pedestrian Trajectory Prediction through Posterior Sampling,"Lin, Haotian; Wang, Yixiao; huo, mingxiao; Peng, Chensheng; Liu, Zhiyuan",https://arxiv.org/abs/2404.00237,"Joint pedestrian trajectory prediction has long grappled with the inherent unpredictability of human behaviors. Recent investigations employing variants of conditional diffusion models in trajectory prediction have exhibited notable success. Nevertheless, the heavy dependence on accurate historical data results in their vulnerability to noise disturbances and data incompleteness. To improve the robustness and reliability, we introduce the Guided Full Trajectory Diffuser (GFTD), a novel diffusion model framework that captures the joint full (historical and future) trajectory distribution. By learning from the full trajectory, GFTD can recover the noisy and missing data, hence improving the robustness. In addition, GFTD can adapt to data imperfections without additional training requirements, leveraging posterior sampling for reliable prediction and controllable generation. Our approach not only simplifies the prediction process but also enhances generalizability in scenarios with noise and incomplete inputs. Through rigorous experimental evaluation, GFTD exhibits superior performance in both trajectory prediction and controllable generation."
MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose and Size Estimation,"Yang, Jiaqi; Chen, Yucong; Meng, Xiangting; Yan, Chenxin; Li, Min; Cheng, Ran; Lige, Liu; Sun, Tao; Kneip, Laurent",https://arxiv.org/abs/2308.08856,"Recently there has been a growing interest in category-level object pose and size estimation, and prevailing methods commonly rely on single view RGB-D images. However, one disadvantage of such methods is that they require accurate depth maps which cannot be produced by consumer-grade sensors. Furthermore, many practical real-world situations involve a moving camera that continuously observes its surroundings, and the temporal information of the input video streams is simply overlooked by single-view methods. We propose a novel solution that makes use of RGB video streams. Our framework consists of three modules: a scale-aware monocular dense SLAM solution, a lightweight object pose predictor, and an object-level pose graph optimizer. The SLAM module utilizes a video stream and additional scale-sensitive readings to estimate camera poses and metric depth. The object pose predictor then generates canonical object representations from RGB images. The object pose is estimated through geometric registration of these canonical object representations with estimated object depth points. All per-view estimates finally undergo optimization within a pose graph, culminating in the output of robust and accurate canonical object poses. Our experimental results demonstrate that when utilizing public dataset sequences with high-quality depth information, the proposed method exhibits comparable performance to state-of-the-art RGB-D methods. We also collect and evaluate on new datasets containing depth maps of varying quality to further quantitatively benchmark the proposed method alongside previous RGB-D based methods. We demonstrate a significant advantage in scenarios where depth input is absent or the quality of depth sensing is limited."
DAP: Diffusion-based Affordance Prediction for Multi-modality Storage,"Chang, Haonan; Boyalakuntla, Kowndinya; Liu, Yuhan; Zhang, Xinyu; Schramm, Liam; Boularias, Abdeslam",https://arxiv.org/abs/2409.00499,"Solving storage problem: where objects must be accurately placed into containers with precise orientations and positions, presents a distinct challenge that extends beyond traditional rearrangement tasks. These challenges are primarily due to the need for fine-grained 6D manipulation and the inherent multi-modality of solution spaces, where multiple viable goal configurations exist for the same storage container. We present a novel Diffusion-based Affordance Prediction (DAP) pipeline for the multi-modal object storage problem. DAP leverages a two-step approach, initially identifying a placeable region on the container and then precisely computing the relative pose between the object and that region. Existing methods either struggle with multi-modality issues or computation-intensive training. Our experiments demonstrate DAP's superior performance and training efficiency over the current state-of-the-art RPDiff, achieving remarkable results on the RPDiff benchmark. Additionally, our experiments showcase DAP's data efficiency in real-world applications, an advancement over existing simulation-driven approaches. Our contribution fills a gap in robotic manipulation research by offering a solution that is both computationally efficient and capable of handling real-world variability. Code and supplementary material can be found at: https://github.com/changhaonan/DPS.git."
Feasibility-Guided Safety-Aware Model Predictive Control for Jump Markov Linear Systems,"Laouar, Zakariya; Ho, Qi Heng; Mazouz, Rayan; Becker, Tyler; Sunberg, Zachary",https://arxiv.org/abs/2310.14116,"In this paper, we present a framework that synthesizes maximally safe control policies for Jump Markov Linear Systems subject to stochastic mode switches. Our approach builds on safe and robust methods for Model Predictive Control (MPC), but in contrast to existing approaches that either optimize without regard to feasibility or utilize soft constraints that increase computational requirements, we employ a safe and robust control approach informed by the feasibility of the optimization problem. When subject to inaccurate hybrid state estimation, our feasibility-guided MPC algorithm generates a control policy that is maximally robust to uncertainty in the system's modes. Additionally, we formulate the notion of safety guarantees for multiple-model receding horizon control using Control Barrier Functions (CBF) to enforce forward invariance in safety-critical settings. We simulate our approach on a six degree-of-freedom hexacopter under several scenarios to demonstrate the utility of the framework. Results illustrate that the proposed technique of maximizing the robustness horizon, and the use of CBFs for forward-invariance, improve the overall safety and performance of Jump Markov Linear Systems."
Seg2Grasp: A Robust Modular Suction Grasping in Bin Picking,"Yoon, Hye Jung; Kim, Juno; Park, Yesol; Jun Ki, Lee; Zhang, Byoung-Tak",,
A Unified Framework of Hybrid Vision-Force Control With Nullspace Compliance for Redundant Robots,"Li, Zhiwen; Li, Weibing; Chen, Yanjie; Pan, Yongping",,
TinyLidarNet: 2D Lidar-based End-to-End Deep Learning Model for F1TENTH Autonomous Racing,"Zarrar, Mohammed Misbah; Weng, QiTao; Yerjan, Bakhbyergyen; Soyyigit, Ahmet; Yun, Heechul",,
Development of Adjustable Compliance and Sensitivity Permanent Magnetic Elastomer-based Tactile Sensor,"Abhyankar, Devesh; Wang, Yushi; Iwamoto, Yuhiro; Sugano, Shigeki; Kamezaki, Mitsuhiro",,
Active Vehicle Re-localization Based on Non-repetitive Lidar with Gimbal Motion Strategy,"Wu, Xin'ao; Yang, Chenxi; Guo, Yiyang; Zhuang, Hanyang; Wang, Chunxiang; Yang, Ming",,
Human Orientation Estimation under Partial Observation,"Zhao, Jieting; Ye, Hanjing; Zhan, Yu; Zhang, Hong",https://arxiv.org/abs/2404.14139,"Reliable Human Orientation Estimation (HOE) from a monocular image is critical for autonomous agents to understand human intention. Significant progress has been made in HOE under full observation. However, the existing methods easily make a wrong prediction under partial observation and give it an unexpectedly high confidence. To solve the above problems, this study first develops a method called Part-HOE that estimates orientation from the visible joints of a target person so that it is able to handle partial observation. Subsequently, we introduce a confidence-aware orientation estimation method, enabling more accurate orientation estimation and reasonable confidence estimation under partial observation. The effectiveness of our method is validated on both public and custom-built datasets, and it shows great accuracy and reliability improvement in partial observation scenarios. In particular, we show in real experiments that our method can benefit the robustness and consistency of the Robot Person Following (RPF) task."
Torque Ripple Reduction in Quasi-Direct Drive Motors Through Angle-Based Repetitive Learning Observer and Model Predictive Torque Controller,"Zhang, Hefei; Zhang, Xiaohu; Cheng, Jinyu; Hu, Jiangtao; Ji, Chao; Wang, Yu; Jiang, Yutong; Han, Zhen; Gao, Wei; Zhang, Shiwu",,
Data Efficient Behavior Cloning for Fine Manipulation via Continuity-based Corrective Labels,"Deshpande, Abhay; Ke, Liyiming; Pfeifer, Quinn; Gupta, Abhishek; Srinivasa, Siddhartha",https://arxiv.org/abs/2405.19307,"We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited by covariate shift due to compounding errors during execution. We investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation tasks. CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the agent back toward expert states. Through extensive experiments on peg insertion and fine grasping, we provide the first empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present in contact-rich manipulation. We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics model error enables performance gains. To effectively apply CCIL to robotic domains, we offer a practical instantiation of the framework and insights into design choices and hyperparameter selection. Our work demonstrates CCIL's practicality for alleviating compounding errors in imitation learning on physical robots."
Multi-Fidelity Reinforcement Learning for Minimum Energy Trajectory Planning,"de Castro, Luke; Ryou, Gilhyun; OHN, HYUNGSEUK; Karaman, Sertac",,
Generalized Path Impedance Control,"Montesino, Ignacio; Victores, Juan G.; Balaguer, Carlos; Jardon, Alberto",https://arxiv.org/abs/2007.07107,"We present the concept of a feedback-based topological acoustic metamaterial as a tool for realizing autonomous and active guiding of sound beams along arbitrary curved paths in free two-dimensional space. The metamaterial building blocks are acoustic transducers, embedded in a slab waveguide. The transducers generate a desired dispersion profile in closed-loop by processing real-time pressure field measurements through preprogrammed controllers. In particular, the metamaterial can be programmed to exhibit analogies of quantum topological wave phenomena, which enables unconventional and exceptionally robust sound beam guiding. As an example, we realize the quantum valley Hall effect by creating, using a collocated pressure feedback, an alternating acoustic impedance pattern across the waveguide. The pattern is traversed by artificial trajectories of different shapes, which are reconfigurable in real-time. Due to topological protection, the sound waves between the plates remain localized on the trajectories, and do not back-scatter by the sharp corners or imperfections in the design. The feedback-based design can be used to realize arbitrary physical interactions in the metamaterial, including non-local, nonlinear, time-dependent, or non-reciprocal couplings, paving the way to new unconventional acoustic wave guiding on the same reprogrammable platform. We then present a non-collocated control algorithm, which mimics another quantum effect, rendering the sound beams uni-directional."
Satellite-Model-Free Deep Learning based Pose Estimation of Non-cooperative Satellite and Tracking using Navigation Filter,"Shukla, Shubham; Srivastava, Raunak; Lima, Rolif; Bera, Titas",,
HILMA-Res: A General Hierarchical Framework via Residual RL for Combining Quadrupedal Locomotion and Manipulation,"Huang, Xiaoyu; Liao, Qiayuan; Ni, Yiming; Li, Zhongyu; Smith, Laura; Levine, Sergey; Peng, Xue Bin; Sreenath, Koushil",https://arxiv.org/abs/2407.06584,"This work presents HiLMa-Res, a hierarchical framework leveraging reinforcement learning to tackle manipulation tasks while performing continuous locomotion using quadrupedal robots. Unlike most previous efforts that focus on solving a specific task, HiLMa-Res is designed to be general for various loco-manipulation tasks that require quadrupedal robots to maintain sustained mobility. The novel design of this framework tackles the challenges of integrating continuous locomotion control and manipulation using legs. It develops an operational space locomotion controller that can track arbitrary robot end-effector (toe) trajectories while walking at different velocities. This controller is designed to be general to different downstream tasks, and therefore, can be utilized in high-level manipulation planning policy to address specific tasks. To demonstrate the versatility of this framework, we utilize HiLMa-Res to tackle several challenging loco-manipulation tasks using a quadrupedal robot in the real world. These tasks span from leveraging state-based policy to vision-based policy, from training purely from the simulation data to learning from real-world data. In these tasks, HiLMa-Res shows better performance than other methods."
Greedy Perspectives: Multi-Drone View Planning for Collaborative Perception in Cluttered Environments,"Suresh, Krishna; Rauniyar, Aditya; Corah, Micah; Scherer, Sebastian",https://arxiv.org/abs/2310.10863,"Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can enable scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a formation planner and a greedy planner that ignores inter-robot collisions. We evaluate our approach based on five test environments and complex multi-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward for filming the actors in three scenarios and comparable performance to formation planning on two others. We also observe near identical view rewards for sequential planning both with and without inter-robot collision constraints which indicates that robots are able to avoid collisions without impairing performance in the perception task. Overall, we demonstrate effective coordination of teams of aerial robots in environments cluttered with obstacles that may cause collisions or occlusions and for filming groups that may split, merge, or spread apart."
ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding,"Chen, Zhen; zhang, zongmin; GUO, WENWU; LUO, Xingjian; Bai, Long; Wu, Jinlin; Ren, Hongliang; Liu, Hongbin",https://arxiv.org/abs/2407.19435,"Surgical instrument segmentation is crucial in surgical scene understanding, thereby facilitating surgical safety. Existing algorithms directly detected all instruments of pre-defined categories in the input image, lacking the capability to segment specific instruments according to the surgeon's intention. During different stages of surgery, surgeons exhibit varying preferences and focus toward different surgical instruments. Therefore, an instrument segmentation algorithm that adheres to the surgeon's intention can minimize distractions from irrelevant instruments and assist surgeons to a great extent. The recent Segment Anything Model (SAM) reveals the capability to segment objects following prompts, but the manual annotations for prompts are impractical during the surgery. To address these limitations in operating rooms, we propose an audio-driven surgical instrument segmentation framework, named ASI-Seg, to accurately segment the required surgical instruments by parsing the audio commands of surgeons. Specifically, we propose an intention-oriented multimodal fusion to interpret the segmentation intention from audio commands and retrieve relevant instrument details to facilitate segmentation. Moreover, to guide our ASI-Seg segment of the required surgical instruments, we devise a contrastive learning prompt encoder to effectively distinguish the required instruments from the irrelevant ones. Therefore, our ASI-Seg promotes the workflow in the operating rooms, thereby providing targeted support and reducing the cognitive load on surgeons. Extensive experiments are performed to validate the ASI-Seg framework, which reveals remarkable advantages over classical state-of-the-art and medical SAMs in both semantic segmentation and intention-oriented segmentation. The source code is available at https://github.com/Zonmgin-Zhang/ASI-Seg."
Scale Disparity of Instances in Interactive Point Cloud Segmentation,"Han, Chenrui; Yu, Xuan; Xie, Yuxuan; Liu, Yili; Mao, Sitong; ZHOU, Shunbo; Xiong, Rong; Wang, Yue",https://arxiv.org/abs/2407.14009,"Interactive point cloud segmentation has become a pivotal task for understanding 3D scenes, enabling users to guide segmentation models with simple interactions such as clicks, therefore significantly reducing the effort required to tailor models to diverse scenarios and new categories. However, in the realm of interactive segmentation, the meaning of instance diverges from that in instance segmentation, because users might desire to segment instances of both thing and stuff categories that vary greatly in scale. Existing methods have focused on thing categories, neglecting the segmentation of stuff categories and the difficulties arising from scale disparity. To bridge this gap, we propose ClickFormer, an innovative interactive point cloud segmentation model that accurately segments instances of both thing and stuff categories. We propose a query augmentation module to augment click queries by a global query sampling strategy, thus maintaining consistent performance across different instance scales. Additionally, we employ global attention in the query-voxel transformer to mitigate the risk of generating false positives, along with several other network structure improvements to further enhance the model's segmentation performance. Experiments demonstrate that ClickFormer outperforms existing interactive point cloud segmentation methods across both indoor and outdoor datasets, providing more accurate segmentation results with fewer user clicks in an open-world setting."
Towards Kbps-level Vehicle Teleoperation via Persistent-Transient Environment Modelling,"Zhao, Chunyang; Zhou, Zeyu; Liu, Haoran; Kircali, Dogan; chi, guoyi; Wang, Yuanzhe; Wang, Danwei",,
OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots,"Kim, Juno; Park, Yesol; Yoon, Hye Jung; Zhang, Byoung-Tak",,
Malicious Path Manipulations via Exploitation of Representation Vulnerabilities of Vision-Language Navigation Systems,"Islam, Chashi Mahiul; Salman, Shaeke; Shams, Montasir; Liu, Xiuwen; Kumar, Piyush",https://arxiv.org/abs/2407.07392,"Building on the unprecedented capabilities of large language models for command understanding and zero-shot recognition of multi-modal vision-language transformers, visual language navigation (VLN) has emerged as an effective way to address multiple fundamental challenges toward a natural language interface to robot navigation. However, such vision-language models are inherently vulnerable due to the lack of semantic meaning of the underlying embedding space. Using a recently developed gradient based optimization procedure, we demonstrate that images can be modified imperceptibly to match the representation of totally different images and unrelated texts for a vision-language model. Building on this, we develop algorithms that can adversarially modify a minimal number of images so that the robot will follow a route of choice for commands that require a number of landmarks. We demonstrate that experimentally using a recently proposed VLN system; for a given navigation command, a robot can be made to follow drastically different routes. We also develop an efficient algorithm to detect such malicious modifications reliably based on the fact that the adversarially modified images have much higher sensitivity to added Gaussian noise than the original images."
Cury: A Backdrivable Leg Design using Linear Actuators,"Guan, Zhongtao; Chen, YiMing; Zhu, Junlei; Hu, Yu; Bai, Weibang; Chen, Jiahao",,
Deep Geometric Potential Functions for Tracking on Manifolds,"Potu Surya Prakash, Nikhil; Seo, Joohwan; Sreenath, Koushil; Choi, Jongeun; HOROWITZ, Roberto",,
Towards Foundation Models for tactile Robots: Designing a Scalable Platform for Robot Learning in the physical world,"Schneider, Samuel; Wu, Yansong; Wu, Fan; Johannsmeier, Lars; Haddadin, Sami",,
PROSPECT: Precision Robot Spectroscopy Exploration and Characterization Tool,"Hanson, Nathaniel; Lvov, Gary; Rautela, Vedant; Hibbard, Sam; Holand, Ethan; DiMarzio, Charles A; Padir, Taskin",https://arxiv.org/abs/2403.17232,"Near Infrared (NIR) spectroscopy is widely used in industrial quality control and automation to test the purity and material quality of items. In this research, we propose a novel sensorized end effector and acquisition strategy to capture spectral signatures from objects and register them with a 3D point cloud. Our methodology first takes a 3D scan of an object generated by a time-of-flight depth camera and decomposes the object into a series of planned viewpoints covering the surface. We generate motion plans for a robot manipulator and end-effector to visit these viewpoints while maintaining a fixed distance and surface normal to ensure maximal spectral signal quality enabled by the spherical motion of the end-effector. By continuously acquiring surface reflectance values as the end-effector scans the target object, the autonomous system develops a four-dimensional model of the target object: position in an R^3 coordinate frame, and a wavelength vector denoting the associated spectral signature. We demonstrate this system in building spectral-spatial object profiles of increasingly complex geometries. As a point of comparison, we show our proposed system and spectral acquisition planning yields more consistent signal signals than naive point scanning strategies for capturing spectral information over complex surface geometries. Our work represents a significant step towards high-resolution spectral-spatial sensor fusion for automated quality assessment."
From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control,"Shentu, Yide; Wu, Shiyao; Rajeswaran, Aravind; Abbeel, Pieter",https://arxiv.org/abs/2405.04798,"Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method -- Learnable Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations. \method~uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that \method~outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors."
Using Graphs of Convex Sets to Guide Nonconvex Trajectory Optimization,"von Wrangel, David; Tedrake, Russ",,
MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with Wireless Coordination,"Wang, Weiying; Cai, Victor; Gil, Stephanie",https://arxiv.org/abs/2403.13348,"This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework that leverages wireless signal-based coordination between robots and Neural Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D reconstruction, including inter-robot pose estimation, localization uncertainty quantification, and active best-next-view selection. We introduce a method for using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate relative poses between robots, as well as quantifying and incorporating the uncertainty embedded in the wireless localization of these pose estimates into the NeRF training loss to mitigate the impact of inaccurate camera poses. Furthermore, we propose an active view selection approach that accounts for robot pose uncertainty when determining the next-best views to improve the 3D reconstruction, enabling faster convergence through intelligent view selection. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our framework in theory and in practice. Leveraging wireless coordination and localization uncertainty-aware training, MULAN-WC can achieve high-quality 3d reconstruction which is close to applying the ground truth camera poses. Furthermore, the quantification of the information gain from a novel view enables consistent rendering quality improvement with incrementally captured images by commending the robot the novel view position. Our hardware experiments showcase the practicality of deploying MULAN-WC to real robotic systems."
Inferring Belief States in Partially-Observable Human-Robot Teams,"Kolb, Jack; Feigh, Karen",https://arxiv.org/abs/2403.11955,"We investigate the real-time estimation of human situation awareness using observations from a robot teammate with limited visibility. In human factors and human-autonomy teaming, it is recognized that individuals navigate their environments using an internal mental simulation, or mental model. The mental model informs cognitive processes including situation awareness, contextual reasoning, and task planning. In teaming domains, the mental model includes a team model of each teammate's beliefs and capabilities, enabling fluent teamwork without the need for explicit communication. However, little work has applied team models to human-robot teaming. We compare the performance of two current methods at estimating user situation awareness over varying visibility conditions. Our results indicate that the methods are largely resilient to low-visibility conditions in our domain, however opportunities exist to improve their overall performance."
A Hybrid Model and Learning-Based Force Estimation Framework for the Surgical Robots,"Yang, Hao; Zhou, Haoying; Fischer, Gregory Scott; Wu, Jie Ying",,
A Robot Kinematics Model Estimation Using Inertial Sensors for On-Site Building Robotics,"Sato, Hiroya; Makabe, Tasuku; Yanokura, Iori; Yamaguchi, Naoya; Okada, Kei; Inaba, Masayuki",,
Object-based SLAM using superquadrics,"Xing, Yifan; Samano, Noe; FAN, WEN; Calway, Andrew",https://arxiv.org/abs/2109.09627,"Introducing semantically meaningful objects to visual Simultaneous Localization And Mapping (SLAM) has the potential to improve both the accuracy and reliability of pose estimates, especially in challenging scenarios with significant view-point and appearance changes. However, how semantic objects should be represented for an efficient inclusion in optimization-based SLAM frameworks is still an open question. Superquadrics(SQs) are an efficient and compact object representation, able to represent most common object types to a high degree, and typically retrieved from 3D point-cloud data. However, accurate 3D point-cloud data might not be available in all applications. Recent advancements in machine learning enabled robust object recognition and semantic mask measurements from camera images under many different appearance conditions. We propose a pipeline to leverage such semantic mask measurements to fit SQ parameters to multi-view camera observations using a multi-stage initialization and optimization procedure. We demonstrate the system's ability to retrieve randomly generated SQ parameters from multi-view mask observations in preliminary simulation experiments and evaluate different initialization stages and cost functions."
Thermally-Resilient Soft Gripper for On-Orbit Operations,"Ruiz Vincueria, Fernando; Arrue, Begoña C.; Ollero, Anibal",https://arxiv.org/abs/2311.08942,"Research in soft manipulators has significantly enhanced object grasping capabilities, thanks to their adaptability to various shapes and sizes. Applying this technology to on-orbit servicing, especially during the capture and containment stages of active space debris removal missions, might offer a secure, adaptable, and cost-effective solution compared to the trend of increasing the degrees of freedom and complexity of the manipulator (e.g. ClearSpace, Astroscale). This work aims to conduct an experimental proof of concept, for which challenges such as radiation, vacuum, and microgravity are significant, but the predominant issue is ensuring effective operation in the extreme temperature swings, where flexible materials may exhibit cryogenic crystallization or drastic shifts in their elasticity. This work addresses this challenge through an initial stage of analytical modeling of the thermal dynamics inside the manipulator in orbit; which is then used for the development of a first experimental prototype tested with liquid nitrogen and heat guns. The multi-layered design for Low Earth Orbit (LEO) leverages the properties of TPU at low infill rates for lightweight inherent flexibility, silicone rubber ensuring structural integrity, PTFE (Teflon) for unparalleled thermal stability, and aerogel for insulation. The tendon-actuated servo-driven gripper is tested in the laboratory by varying the shape and size of objects during the grasping. The results, based on servomotor force metrics to assess the flexible manipulator's adaptability and object capture efficiency across temperature changes, affirm the concept's viability. Forces increase up to 220$\%$ in cryogenic conditions and decrease by no more than 50$\%$ at high temperatures."
Terrain-Attentive Learning for Efficient 6-DoF Kinodynamic Modeling on Vertically Challenging Terrain,"Datar, Aniket; Pan, Chenhui; Nazeri, Mohammad; Pokhrel, Anuj; Xiao, Xuesu",https://arxiv.org/abs/2403.16419,"Wheeled robots have recently demonstrated superior mechanical capability to traverse vertically challenging terrain (e.g., extremely rugged boulders comparable in size to the vehicles themselves). Negotiating such terrain introduces significant variations of vehicle pose in all six Degrees-of-Freedom (DoFs), leading to imbalanced contact forces, varying momentum, and chassis deformation due to non-rigid tires and suspensions. To autonomously navigate on vertically challenging terrain, all these factors need to be efficiently reasoned within limited onboard computation and strict real-time constraints. In this paper, we propose a 6-DoF kinodynamics learning approach that is attentive only to the specific underlying terrain critical to the current vehicle-terrain interaction, so that it can be efficiently queried in real-time motion planners onboard small robots. Physical experiment results show our Terrain-Attentive Learning demonstrates on average 51.1% reduction in model prediction error among all 6 DoFs compared to a state-of-the-art model for vertically challenging terrain."
BEVPose: Unveiling Scene Semantics through Self-Supervised Pose-Guided Multi-Modal BEV Alignment,"Hosseinzadeh, Mehdi; Reid, Ian",,
Design and Control of a Three-Dimensional Electromagnetic Drive System for Micro-Robots,"Fan, Qigao; Zhang, Yunrui; Liu, Yueyue",,
Learning Temporally Composable Task Segmentations with Language,"Raj, Divyanshu; Patil, Omkar; Gu, Weiwei; Baral, Chitta; Gopalan, Nakul",https://arxiv.org/abs/2201.05307,"Temporal video grounding (TVG) aims to localize a target segment in a video according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on abundant video-query paired data, which is expensive and time-consuming to collect in real-world scenarios. In this paper, we explore whether a video grounding model can be learned without any paired annotations. To the best of our knowledge, this paper is the first work trying to address TVG in an unsupervised setting. Considering there is no paired supervision, we propose a novel Deep Semantic Clustering Network (DSCNet) to leverage all semantic information from the whole query set to compose the possible activity in each video for grounding. Specifically, we first develop a language semantic mining module, which extracts implicit semantic features from the whole query set. Then, these language semantic features serve as the guidance to compose the activity in video via a video-based semantic aggregation module. Finally, we utilize a foreground attention branch to filter out the redundant background activities and refine the grounding results. To validate the effectiveness of our DSCNet, we conduct experiments on both ActivityNet Captions and Charades-STA datasets. The results demonstrate that DSCNet achieves competitive performance, and even outperforms most weakly-supervised approaches."
UW-SDF: Exploiting Hybrid Geometric Priors for Neural SDF Reconstruction from Underwater Multi-view Monocular Images,"Chen, Zeyu; Tang, Jingyi; Wang, Gu; Li, Shengquan; Li, Xinghui; Ji, Xiangyang; Li, Xiu",,
Enhancing Prosthetic Safety and Environmental Adaptability: A Visual-Inertial Prosthesis Motion Estimation Approach on Uneven Terrains,"Chen, Chuheng; Chen, Xinxing; Yin, Shucong; Wang, Yuxuan; Huang, Binxin; Leng, Yuquan; Fu, Chenglong",https://arxiv.org/abs/2404.18612,"Environment awareness is crucial for enhancing walking safety and stability of amputee wearing powered prosthesis when crossing uneven terrains such as stairs and obstacles. However, existing environmental perception systems for prosthesis only provide terrain types and corresponding parameters, which fails to prevent potential collisions when crossing uneven terrains and may lead to falls and other severe consequences. In this paper, a visual-inertial motion estimation approach is proposed for prosthesis to perceive its movement and the changes of spatial relationship between the prosthesis and uneven terrain when traversing them. To achieve this, we estimate the knee motion by utilizing a depth camera to perceive the environment and align feature points extracted from stairs and obstacles. Subsequently, an error-state Kalman filter is incorporated to fuse the inertial data into visual estimations to reduce the feature extraction error and obtain a more robust estimation. The motion of prosthetic joint and toe are derived using the prosthesis model parameters. Experiment conducted on our collected dataset and stair walking trials with a powered prosthesis shows that the proposed method can accurately tracking the motion of the human leg and prosthesis with an average root-mean-square error of toe trajectory less than 5 cm. The proposed method is expected to enable the environmental adaptive control for prosthesis, thereby enhancing amputee's safety and mobility in uneven terrains."
Hardware-Software Co-Design for Path Planning by Drones,"Dube, Ayushi; Patil, Omkar; Singh, Gian; Gopalan, Nakul; VRUDHULA, SARMA",,
Fast Explicit-Input Assistance for Teleoperation in Clutter,"Walker, Nick; Yang, Xuning; Garg, Animesh; Cakmak, Maya; Fox, Dieter; Pérez-D'Arpino, Claudia",https://arxiv.org/abs/2402.02612,"The performance of prediction-based assistance for robot teleoperation degrades in unseen or goal-rich environments due to incorrect or quickly-changing intent inferences. Poor predictions can confuse operators or cause them to change their control input to implicitly signal their goal. We present a new assistance interface for robotic manipulation where an operator can explicitly communicate a manipulation goal by pointing the end-effector. The pointing target specifies a region for local pose generation and optimization, providing interactive control over grasp and placement pose candidates. We compare the explicit pointing interface to an implicit inference-based assistance scheme in a within-subjects user study (N=20) where participants teleoperate a simulated robot to complete a multi-step singulation and stacking task in cluttered environments. We find that operators prefer the explicit interface, experience fewer pick failures and report lower cognitive workload. Our code is available at: https://github.com/NVlabs/fast-explicit-teleop"
Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural,"Korigodskii, Andrei; Kalachev, Oleg; Vasiunik, Artem; Bondar, George",,
Single Actuator Undulation Soft-bodied Robots Using A Precompressed Variable Thickness Flexible Beam,"Ta, Tung D.",,
Recovering Missed Detections in an Elevator Button Segmentation Task,"Verzic, Nicholas; Chadaga, Abhinav; Hart, Justin",,
Design of a Pneumatically Driven 3D-Printed Under-Actuated Soft Robot with Programmable Stiffness,"Mustafa, Zaid; Turkseven, Melih",,
Interactive Reinforcement Learning from Natural Language Feedback,"Tarakli, Imene; Vinanzi, Samuele; Di Nuovo, Alessandro",https://arxiv.org/abs/1705.09906,"One of the long-term goals of artificial intelligence is to build an agent that can communicate intelligently with human in natural language. Most existing work on natural language learning relies heavily on training over a pre-collected dataset with annotated labels, leading to an agent that essentially captures the statistics of the fixed external training data. As the training data is essentially a static snapshot representation of the knowledge from the annotator, the agent trained this way is limited in adaptiveness and generalization of its behavior. Moreover, this is very different from the language learning process of humans, where language is acquired during communication by taking speaking action and learning from the consequences of speaking action in an interactive manner. This paper presents an interactive setting for grounded natural language learning, where an agent learns natural language by interacting with a teacher and learning from feedback, thus learning and improving language skills while taking part in the conversation. To achieve this goal, we propose a model which incorporates both imitation and reinforcement by leveraging jointly sentence and reward feedbacks from the teacher. Experiments are conducted to validate the effectiveness of the proposed approach."
Embedded 3d printing of silicone for soft actuator with stiffness gradient and programmable workspace,"Xiao, Fei; Wei, Zhuoheng; Wang, Hao; Li, Jisen; ZHU, Jian",,
Demonstration to Adaptation: A User-Guided Framework for Sequential and Real-Time Planning,"Cai, Kuanqi; Laha, Riddhiman; Gong, Yuhe; Chen, Lingyun; Zhang, Liding; Figueredo, Luis; Haddadin, Sami",,
A Robust Visual SLAM System for Small-Scale Quadruped Robots in Dynamic Environments,"Li, Chengyang; Zhang, Yulai; Yu, Zhiqiang; Liu, Xinming; Shi, Qing",,
Continual Learning for Autonomous Robots A Prototype-based Approach,"Hajizada, Elvin; Swaminathan, Balachandran; Sandamirskaya, Yulia",https://arxiv.org/abs/2404.00418,"Humans and animals learn throughout their lives from limited amounts of sensed data, both with and without supervision. Autonomous, intelligent robots of the future are often expected to do the same. The existing continual learning (CL) methods are usually not directly applicable to robotic settings: they typically require buffering and a balanced replay of training data. A few-shot online continual learning (FS-OCL) setting has been proposed to address more realistic scenarios where robots must learn from a non-repeated sparse data stream. To enable truly autonomous life-long learning, an additional challenge of detecting novelties and learning new items without supervision needs to be addressed. We address this challenge with our new prototype-based approach called Continually Learning Prototypes (CLP). In addition to being capable of FS-OCL learning, CLP also detects novel objects and learns them without supervision. To mitigate forgetting, CLP utilizes a novel metaplasticity mechanism that adapts the learning rate individually per prototype. CLP is rehearsal-free, hence does not require a memory buffer, and is compatible with neuromorphic hardware, characterized by ultra-low power consumption, real-time processing abilities, and on-chip learning. Indeed, we have open-sourced a simple version of CLP in the neuromorphic software framework Lava, targetting Intel's neuromorphic chip Loihi 2. We evaluate CLP on a robotic vision dataset, OpenLORIS. In a low-instance FS-OCL scenario, CLP shows state-of-the-art results. In the open world, CLP detects novelties with superior precision and recall and learns features of the detected novel classes without supervision, achieving a strong baseline of 99% base class and 65%/76% (5-shot/10-shot) novel class accuracy."
LDIP: Real-time on-road object detection with depth estimation from a single image,"Xu, Chengpeng; Sun, Xiao; Xu, Yangyang; Wang, Ruolin",,
Adaptive Stochastic Nonlinear Model Predictive Control with Look-ahead Deep Reinforcement Learning for Autonomous Vehicle Motion Control,"Zarrouki, Baha; Wang, Chenyang; Betz, Johannes",https://arxiv.org/abs/2311.04303,"In this paper, we present a Deep Reinforcement Learning (RL)-driven Adaptive Stochastic Nonlinear Model Predictive Control (SNMPC) to optimize uncertainty handling, constraints robustification, feasibility, and closed-loop performance. To this end, we conceive an RL agent to proactively anticipate upcoming control tasks and to dynamically determine the most suitable combination of key SNMPC parameters - foremost the robustification factor $κ$ and the Uncertainty Propagation Horizon (UPH) $T_u$. We analyze the trained RL agent's decision-making process and highlight its ability to learn context-dependent optimal parameters. One key finding is that adapting the constraints robustification factor with the learned policy reduces conservatism and improves closed-loop performance while adapting UPH renders previously infeasible SNMPC problems feasible when faced with severe disturbances. We showcase the enhanced robustness and feasibility of our Adaptive SNMPC (aSNMPC) through the real-time motion control task of an autonomous passenger vehicle to follow an optimal race line when confronted with significant time-variant disturbances. Experimental findings demonstrate that our look-ahead RL-driven aSNMPC outperforms its Static SNMPC (sSNMPC) counterpart in minimizing the lateral deviation both with accurate and inaccurate disturbance assumptions and even when driving in previously unexplored environments."
"DriVLMe: Exploring Foundation Models as Autonomous Driving Agents That Perceive, Communicate and Navigate","Huang, Yidong; Sansom, Jacob; Ma, Ziqiao; Gervits, Felix; Chai, Joyce",https://arxiv.org/abs/2406.03008,"Recent advancements in foundation models (FMs) have unlocked new prospects in autonomous driving, yet the experimental settings of these studies are preliminary, over-simplified, and fail to capture the complexity of real-world driving scenarios in human environments. It remains under-explored whether FM agents can handle long-horizon navigation tasks with free-from dialogue and deal with unexpected situations caused by environmental dynamics or task changes. To explore the capabilities and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a video-language-model-based agent to facilitate natural and effective communication between humans and autonomous vehicles that perceive the environment and navigate. We develop DriVLMe from both embodied experiences in a simulated environment and social experiences from real human dialogue. While DriVLMe demonstrates competitive performance in both open-loop benchmarks and closed-loop human studies, we reveal several limitations and challenges, including unacceptable inference time, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, simplified language generation from robotic experiences, and difficulties in handling on-the-fly unexpected situations like environmental dynamics and task changes."
StaccaToe: A Single-Leg Robot that Mimics the Human Leg and Toe,"Perera, Kankanige Nisal Minula; yu, Shangqun; Marew, Daniel; Tang, Mack; Suzuki, Ken; McCormack, Aidan; Zhu, Shifan; Kim, Yong-Jae; Kim, Donghyun",https://arxiv.org/abs/2404.05039,"We introduce StaccaToe, a human-scale, electric motor-powered single-leg robot designed to rival the agility of human locomotion through two distinctive attributes: an actuated toe and a co-actuation configuration inspired by the human leg. Leveraging the foundational design of HyperLeg's lower leg mechanism, we develop a stand-alone robot by incorporating new link designs, custom-designed power electronics, and a refined control system. Unlike previous jumping robots that rely on either special mechanisms (e.g., springs and clutches) or hydraulic/pneumatic actuators, StaccaToe employs electric motors without energy storage mechanisms. This choice underscores our ultimate goal of developing a practical, high-performance humanoid robot capable of human-like, stable walking as well as explosive dynamic movements. In this paper, we aim to empirically evaluate the balance capability and the exertion of explosive ground reaction forces of our toe and co-actuation mechanisms. Throughout extensive hardware and controller development, StaccaToe showcases its control fidelity by demonstrating a balanced tip-toe stance and dynamic jump. This study is significant for three key reasons: 1) StaccaToe represents the first human-scale, electric motor-driven single-leg robot to execute dynamic maneuvers without relying on specialized mechanisms; 2) our research provides empirical evidence of the benefits of replicating critical human leg attributes in robotic design; and 3) we explain the design process for creating agile legged robots, the details that have been scantily covered in academic literature."
Using Hip Assisted Running Exoskeleton with Impact Isolation Mechanism to Improve Energy Efficiency,"Wang, Ziqi; Liu, Junchen; Li, Hongwu; Zhang, Qinghua; Li, Xianglong; huang, yi; Ju, Haotian; Zheng, Tianjiao; Zhao, Jie; Zhu, Yanhe",,
Design of an Array Microrobot System and Its Application in Object Delivery,"Fan, Qigao; Hou, Zhe; Liu, Yueyue",,
Understanding Robot Minds: Leveraging Machine Teaching for Transparent Human-Robot Collaboration Across Diverse Groups,"Jayaraman, Suresh Kumaar; Steinfeld, Aaron; Simmons, Reid; Admoni, Henny",https://arxiv.org/abs/2404.15472,"In this work, we aim to improve transparency and efficacy in human-robot collaboration by developing machine teaching algorithms suitable for groups with varied learning capabilities. While previous approaches focused on tailored approaches for teaching individuals, our method teaches teams with various compositions of diverse learners using team belief representations to address personalization challenges within groups. We investigate various group teaching strategies, such as focusing on individual beliefs or the group's collective beliefs, and assess their impact on learning robot policies for different team compositions. Our findings reveal that team belief strategies yield less variation in learning duration and better accommodate diverse teams compared to individual belief strategies, suggesting their suitability in mixed-proficiency settings with limited resources. Conversely, individual belief strategies provide a more uniform knowledge level, particularly effective for homogeneously inexperienced groups. Our study indicates that the teaching strategy's efficacy is significantly influenced by team composition and learner proficiency, highlighting the importance of real-time assessment of learner proficiency and adapting teaching approaches based on learner proficiency for optimal teaching outcomes."
SSL-RGB2IR: Semi-supervised RGB-to-IR Image-to-Image Translation for Enhancing Vision Task Training in Semantic Segmentation and Object Detection,"Sikdar, Aniruddh; Saadiyean, Qiranul; Anand, Prahlad; Sundaram, Suresh",,
A Wearable Mechanical Pressure-Electrophysiological Bimodal Sensing System for Rehabilitation Electromechanical Device,"Wang, Peng; Liu, Jixiao; Qi, Dianpeng; Guo, Shijie",,
Structural Optimization of Lightweight Bipedal Robot via SERL,"Cheng, Yi; Han, Chenxi; Min, Yuheng; Ye, Linqi; Liu, Houde; Hang, Liu; LIANG, bin",https://arxiv.org/abs/2408.15632,"Designing a bipedal robot is a complex and challenging task, especially when dealing with a multitude of structural parameters. Traditional design methods often rely on human intuition and experience. However, such approaches are time-consuming, labor-intensive, lack theoretical guidance and hard to obtain optimal design results within vast design spaces, thus failing to full exploit the inherent performance potential of robots. In this context, this paper introduces the SERL (Structure Evolution Reinforcement Learning) algorithm, which combines reinforcement learning for locomotion tasks with evolution algorithms. The aim is to identify the optimal parameter combinations within a given multidimensional design space. Through the SERL algorithm, we successfully designed a bipedal robot named Wow Orin, where the optimal leg length are obtained through optimization based on body structure and motor torque. We have experimentally validated the effectiveness of the SERL algorithm, which is capable of optimizing the best structure within specified design space and task conditions. Additionally, to assess the performance gap between our designed robot and the current state-of-the-art robots, we compared Wow Orin with mainstream bipedal robots Cassie and Unitree H1. A series of experimental results demonstrate the Outstanding energy efficiency and performance of Wow Orin, further validating the feasibility of applying the SERL algorithm to practical design."
Quadruped robot traversing 3D complex environments with limited perception,"Cheng, Yi; Hang, Liu; Pan, Guoping; Ye, Linqi; Liu, Houde; LIANG, bin",https://arxiv.org/abs/2404.18225,"Traversing 3-D complex environments has always been a significant challenge for legged locomotion. Existing methods typically rely on external sensors such as vision and lidar to preemptively react to obstacles by acquiring environmental information. However, in scenarios like nighttime or dense forests, external sensors often fail to function properly, necessitating robots to rely on proprioceptive sensors to perceive diverse obstacles in the environment and respond promptly. This task is undeniably challenging. Our research finds that methods based on collision detection can enhance a robot's perception of environmental obstacles. In this work, we propose an end-to-end learning-based quadruped robot motion controller that relies solely on proprioceptive sensing. This controller can accurately detect, localize, and agilely respond to collisions in unknown and complex 3D environments, thereby improving the robot's traversability in complex environments. We demonstrate in both simulation and real-world experiments that our method enables quadruped robots to successfully traverse challenging obstacles in various complex environments."
